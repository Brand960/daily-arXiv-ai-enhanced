<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.LG](#cs.LG) [Total: 104]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 提出KPM-Bench数据集和MoPE算法，提升视频字幕对细微动作的描述精度并减少幻觉


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕模型在描述精细运动细节时表现不佳，且存在严重幻觉问题，尤其在动作密集视频中缺乏对肢体动态的准确建模

Method: 构建基于运动学计算与语言解析的自动标注管道，发布KPM-Bench数据集（含细粒度视频-字幕对、运动问答对、幻觉评估集），并提出MoPE算法从文本中提取运动属性以评估和缓解幻觉

Result: KPM-Bench提供首个专注于肢体级运动理解的开源数据集，MoPE结合GRPO训练显著降低幻觉，提升运动描述可靠性

Conclusion: 通过运动学与语言融合的细粒度建模，有效解决视频字幕中的运动描述不准确与幻觉问题，为未来研究提供新基准与方法

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 提出3D-HIW数据集和CLUTCH模型，实现大规模野外手部动作文本到运动生成，显著提升动画保真度与文本对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖实验室采集数据，动作有限、难以扩展至野外场景，且文本-运动对齐与动画质量不足。

Method: 构建3D-HIW数据集（3.2万条3D手部动作+文本），提出CLUTCH系统，包含SHIFT（分模态VQ-VAE）和几何精调阶段，结合VLM与3D追踪器自动标注。

Result: 在文本到运动和运动到文本任务上达到SOTA，建立首个可扩展的野外手部动作建模基准。

Conclusion: 3D-HIW与CLUTCH为野外手部动画生成提供了高效、高保真的新范式，代码、数据与模型将公开。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: PRISM是一种自监督学习框架，通过边缘图和亮度解耦提升结肠镜图像的单目深度与位姿估计性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜导航中因无纹理表面、复杂光照、形变及缺乏真实数据标签，导致深度与位姿估计困难。

Method: PRISM引入基于学习的边缘检测（如DexiNed/HED）和亮度解耦模块，分离光照与反射成分，利用结构和阴影线索进行自监督几何学习。

Result: 在多个真实与合成数据集上达到SOTA性能，且实验证明真实数据自监督训练优于仿真数据监督训练，帧率对性能影响显著。

Conclusion: 真实场景的域真实性比标注数据更重要，视频帧采样策略是提升模型性能的关键因素。

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net，通过隐式特征幻觉替代像素级虚拟染色，实现从H&E切片高效准确预测HER2表达水平。


<details>
  <summary>Details</summary>
Motivation: 传统IHC染色成本高、耗时长且难以普及，亟需基于H&E切片的替代方法，但现有像素级虚拟染色方法计算昂贵且易产生伪影。

Method: 提出Latent-Guided Dual-Stream Network (LGD-Net)，通过跨模态特征幻觉将H&E形态特征映射至分子潜空间，并借助IHC编码器指导训练，结合核分布与膜染色强度等临床知识进行轻量级正则化。

Result: 在BCI公开数据集上达到SOTA性能，显著优于基线方法，且仅需单模态H&E输入即可高效推理。

Conclusion: LGD-Net通过隐式特征学习和临床知识引导，实现了高效、准确的HER2自动评分，为资源受限环境下乳腺癌精准诊疗提供了新方案。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一种无需训练或仅轻量LoRA微调的方法，结合CLIP与GPT-5/Qwen-VL和SAM，实现遥感图像的零样本文本引导分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖额外可训练组件，限制了泛化能力和实际应用，亟需完全无训练的解决方案。

Method: 结合对比式VLM（CLIP）选择SAM的网格提案，以及生成式VLM（GPT-5和LoRA微调Qwen-VL）生成点击提示，实现零样本分割。

Result: 在19个遥感基准上达到SOTA性能，涵盖开放词汇、指代表达和推理任务，且无需大量训练。

Conclusion: 仅依赖现有基础模型即可实现高效零样本遥感图像分割，为实际部署提供轻量、可扩展的新范式。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: 提出仅编码器的视频分割模型VidEoMT，无需追踪模块，速度提升5-10倍，达160 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有视频分割模型依赖复杂追踪模块，带来高计算开销；研究表明大容量ViT可仅用编码器实现精确分割。

Method: 引入轻量级查询传播机制复用前一帧查询，并结合时间无关的可学习查询进行融合，实现时序建模。

Result: 在保持竞争力准确率的同时，速度提升5-10倍，ViT-L骨架下可达160 FPS。

Conclusion: VidEoMT证明了仅编码器架构在视频分割中的高效性，简化了模型结构并显著提升推理速度。

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出首个视频查询性能预测（VQPP）基准，包含56K文本查询和51K视频，验证预检索预测器效果并用于LLM查询重写训练。


<details>
  <summary>Details</summary>
Motivation: 现有查询性能预测研究主要针对文本和图像检索，视频内容检索（CBVR）的QPP长期被忽视。

Method: 构建VQPP基准，包含两个文本到视频检索数据集和两个CBVR系统，评估多种预检索与后检索预测器，并将最佳预检索预测器作为奖励模型用于LLM的DPO训练。

Result: 预检索预测器表现优异，可提前预测性能；基于VQPP训练的LLM在查询重写任务上表现良好。

Conclusion: VQPP为视频领域的QPP研究提供了首个标准化基准，推动预检索预测在视频检索与LLM优化中的应用。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 论文指出Liu和Szirányi的手势识别方法因数据泄露导致评估结果虚假优异，强调需采用主体独立的划分方式。


<details>
  <summary>Details</summary>
Motivation: 现有手势识别研究中评估协议存在数据泄露问题，尤其在无人机人机交互等需泛化至未见个体的应用中，严重影响结果可靠性。

Method: 通过分析已发表的混淆矩阵、学习曲线和数据集构造，揭示其帧级随机划分导致同一主体样本泄露至训练与测试集的问题。

Result: 报告的近完美准确率实为假象，实际未衡量模型对新个体的泛化能力。

Conclusion: vision-based手势识别研究必须采用主体独立的数据划分，以确保评估的真实性和应用的可靠性。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出一种自适应视频采样与时空压缩框架，结合多模态大语言模型，有效处理长视频的冗余问题并提升理解性能。


<details>
  <summary>Details</summary>
Motivation: 长视频中帧序列冗余严重，现有模型难以在内存限制下高效处理大量帧并提取关键信息。

Method: 引入信息密度自适应视频采样器（AVS）和基于自编码器的时空视频压缩器（SVC），与多模态大语言模型（MLLM）联合训练。

Result: 在长视频理解及标准视频基准上均表现优异，实现高压缩率的同时保留关键判别信息。

Conclusion: 该框架能有效应对长视频的冗余与计算挑战，兼具高效性与通用性。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 视觉语言模型在多种视觉问答任务上表现优异，但在细粒度图像分类任务上表现不佳，研究发现视觉编码器和预训练阶段对细粒度理解至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在复杂视觉推理任务中表现突出，但在测试细粒度视觉知识的传统图像分类基准上表现欠佳，亟需理解其背后的原因。

Method: 对大量最新视觉语言模型在细粒度分类基准上进行评估，并通过消融实验分析视觉编码器、语言模型和预训练策略的影响。

Result: 更好的语言模型对所有任务性能均有均衡提升，而更好的视觉编码器显著提升细粒度分类性能；预训练阶段若解冻语言模型权重，对细粒度性能尤为关键。

Conclusion: 提升视觉语言模型的细粒度视觉理解能力需重点关注视觉编码器的优化和预训练策略的设计，尤其是语言模型是否在预训练中解冻。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [11] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 使用稀疏多模态测距数据（如雷达或LiDAR）增强扩散模型的深度估计，显著提升单图像新视图合成的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计在低纹理、恶劣天气和遮挡场景下不可靠，限制了扩散模型的新视图合成效果。

Method: 提出一种基于局部高斯过程的角域深度重建框架，利用稀疏雷达或LiDAR数据生成稠密深度图并量化不确定性，作为扩散模型的几何条件输入。

Result: 在真实驾驶场景中，使用稀疏测距数据替代单目深度后，新视图生成的几何一致性和视觉质量显著提升。

Conclusion: 可靠的几何先验对扩散模型至关重要，即使极稀疏的多模态传感数据也能显著提升渲染效果。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [12] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种无位置编码和分类令牌的紧凑型Vision Transformer，在医疗图像小样本学习中表现优异，尤其适合资源受限的临床部署。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformer依赖固定空间先验（如位置编码和[CLS]令牌），在医疗图像中因空间布局弱或不一致而限制泛化能力。

Method: 提出ZACH-ViT，移除位置编码和[CLS]令牌，采用全局平均池化实现置换不变性，结合自适应残差投影维持训练稳定与参数受限。

Result: 在7个MedMNIST数据集上小样本评估，ZACH-ViT（0.25M参数）在BloodMNIST上表现最佳，在PathMNIST上与TransMIL相当，在具强解剖先验数据集上优势减弱，推理时间低于1秒。

Conclusion: 架构先验与数据结构对齐比追求通用基准主导更重要，ZACH-ViT以极小模型实现高效临床部署潜力。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [13] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET通过残差导向的多层表示对齐，以极低计算成本显著提升2D VLA模型的3D空间理解能力


<details>
  <summary>Details</summary>
Motivation: 现有2D VLA模型缺乏3D空间理解，单层对齐无法充分利用深层信息，而多层对齐易引发梯度干扰

Method: 提出ROCKET框架，使用共享投影器对齐VLA与3D视觉基础模型的多层残差流，并引入Matryoshka稀疏激活机制平衡多层损失

Result: 仅需4%计算开销即可在LIBERO上达到98.5% SOTA成功率，且在LIBERO-Plus和RoboTwin上表现优异

Conclusion: 共享投影器设计有效缓解梯度冲突，ROCKET为低成本提升VLA模型3D能力提供了高效解决方案

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [14] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出一种记忆驱动的图像质量评估框架MQAF，减少对参考图像的依赖，支持全参考和无参考场景。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估方法依赖高质量参考图像，限制了实际应用；人类视觉系统可通过长期记忆评估质量，启发本研究。

Method: 构建记忆库存储失真模式，动态切换双模式评估策略：有参考时融合参考信息与记忆库对比，无参考时仅用记忆库推断质量。

Result: 在多个数据集上优于现有方法，同时在全参考和无参考任务中表现优异。

Conclusion: MQAF通过模拟人类记忆机制，有效降低对参考图像的依赖，实现更灵活通用的图像质量评估。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [15] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: 提出首个伪多模态水下目标跟踪数据集MUOT_3M和基于SAM的多模态到单模态跟踪器MUTrack，显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有水下目标跟踪数据集规模小且仅含RGB模态，难以应对水下颜色失真、浑浊和低能见度等挑战。

Method: 构建MUOT_3M数据集（300万帧，5模态），提出MUTrack跟踪器，结合视觉几何对齐、视觉语言融合与四级知识蒸馏，将多模态知识迁移到单模态模型。

Result: MUTrack在五个基准上超越SOTA，AUC提升8.40%，精度提升7.80%，运行速度达24 FPS。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但可实际部署的水下跟踪奠定了新基础。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [16] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 提出一种基于大语言模型的感性视觉定制任务L-AVC，通过EIC和PER模块实现高效精准的情感操控。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制方法忽视主观情感内容，缺乏通用的感性视觉定制基础模型。

Method: 提出EPEM方法，包含高效跨情感转换（EIC）模块和精确非情感内容保留（PER）模块，利用多模态LLM实现情感编辑。

Result: 在自建L-AVC数据集上，EPEM显著优于现有基线方法，验证了情感信息的重要性及方法的有效性。

Conclusion: 情感信息对视觉定制至关重要，EPEM实现了情感语义的高效转换与非情感内容的精准保留。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [17] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 提出一种新的深度安全视频理解任务DeepSVU，通过UPRM方法融合细粒度物理世界信息，显著提升威胁原因分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全视频理解研究主要聚焦威胁检测，缺乏对威胁成因的生成与评估能力。

Method: 提出UPRM方法，包含UPE块和PTR正则化器，协同建模粗粒度到细粒度的物理世界信息并自适应权衡。

Result: 在UCF-C和CUVA指令数据集上，UPRM超越多种先进视频大模型与非VLM方法。

Conclusion: 细粒度物理世界信息对威胁因果分析至关重要，UPRM有效捕捉并利用此类信息提升性能。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [18] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出一种无需训练的插件式模块UAOR，通过动作熵感知不确定性并重注入观测信息，提升VLA模型性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法依赖额外观测数据或辅助模块，成本高；希望利用FFN的键值记忆特性，在不增加训练开销下提升模型鲁棒性

Method: UAOR模块：根据动作熵衡量不确定性，当不确定性高时，通过注意力机制将关键观测信息重注入下一层FFN

Result: 在仿真与真实机器人任务中，UAOR显著提升多种VLA模型性能，无需额外观测或模块，开销极低

Conclusion: UAOR是一种高效、通用、即插即用的VLA增强方案，为低成本部署提供新思路

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [19] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出一种无需训练的双通道注意力引导方法DCAG，同时操控Key和Value空间，显著提升扩散模型图像编辑的精度与保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅操控Key空间，忽略Value空间在特征聚合中的作用，导致编辑控制不够精细。

Method: 发现DiT中Key和Value投影存在偏差-增量结构，提出DCAG同时引导Key（粗粒度控制）和Value（细粒度聚合）通道，构建二维参数空间(δ_k, δ_v)。

Result: 在PIE-Bench基准上超越仅Key方法，局部编辑任务如对象删除和添加的LPIPS分别降低4.9%和3.2%。

Conclusion: 双通道协同调控比单通道更有效，为扩散模型的无训练编辑提供新范式。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [20] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出DiST框架，利用大语言模型分解动作名称为空间与时间属性，提升小样本动作识别性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖粗粒度动作名称作为辅助上下文，缺乏对动作空间和时间细节的充分建模

Method: DiST框架包含分解与融合两阶段：分解动作名称为时空属性，使用SKC和TKC分别构建物体级和帧级原型

Result: 在五个标准FSAR数据集上达到SOTA性能

Conclusion: 通过引入大语言模型提供的时空知识，DiST能有效捕捉细粒度空间细节与多样时间模式，显著提升小样本动作识别效果

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [21] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard是一种面向隐私保护的城市场景行人重识别框架，通过拓扑感知变换器在不共享原始图像的前提下实现高精度跨摄像头身份检索。


<details>
  <summary>Details</summary>
Motivation: 城市规模的行人重识别面临外观变化大、数据隐私保护严格等挑战，现有方法难以在不共享原始图像的情况下实现鲁棒的跨视角匹配。

Method: 提出CityGuard框架，包含三部分：自适应度量学习器增强类内紧凑性，空间条件注意力引入粗粒度几何先验实现跨视图对齐，差分隐私嵌入映射结合紧凑索引保障隐私与效率。

Result: 在Market-1501等基准上显著提升检索精度和查询吞吐量，实验证明框架在隐私约束下仍具高实用性。

Conclusion: CityGuard实现了隐私与性能的平衡，为城市级隐私敏感的行人重识别提供了可行解决方案。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [22] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: 提出TCA-T2M框架，通过时空一致性建模提升文本到动作生成的连贯性与物理合理性


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法忽视同一动作在不同序列间的时序一致性，导致语义错位和不自然运动

Method: 引入时序一致性感知的时空VQ-VAE（TCaS-VQ-VAE）进行跨序列对齐，结合掩码运动变换器与运动学约束模块提升生成质量

Result: 在HumanML3D和KIT-ML基准上达到SOTA性能，验证时序一致性对生成鲁棒性的重要性

Conclusion: 显式建模跨序列时序一致性是实现高质量文本到动作生成的关键

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [23] [3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis](https://arxiv.org/abs/2602.18064)
*Ziyue Wang,Linghan Cai,Chang Han Low,Haofeng Liu,Junde Wu,Jingyu Wang,Rui Wang,Lei Song,Jiang Bian,Jingjing Fu,Yueming Jin*

Main category: cs.CV

TL;DR: 3DMedAgent让2D多模态大模型无需3D微调即可执行通用3D CT分析，通过多步推理和结构化记忆实现从感知到临床理解的逐步积累


<details>
  <summary>Details</summary>
Motivation: 现有3D分析方法要么任务孤立，要么端到端单一输出，难以系统积累感知证据；而2D MLLM虽感知强但无法处理3D容积数据

Method: 提出3DMedAgent，利用MLLM协调视觉与文本工具，将3D分析分解为从全局到局部、从3D到2D切片、从视觉到文本的渐进子任务，并构建长期结构化记忆支持证据驱动的多步推理

Result: 在新构建的DeepChestVQA基准上，3DMedAgent在40+任务中全面超越通用、医疗和3D专用MLLM模型

Conclusion: 3DMedAgent提供了一种可扩展的路径，实现无需3D微调的通用3D临床辅助分析

Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.

</details>


### [24] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出一种两阶段自监督预训练加半监督微调方法，显著减少BEV语义地图标注需求并提升性能


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头BEV方法依赖昂贵且不一致的BEV真值标注，亟需减少标注成本

Method: 自监督预训练阶段：将BEVFormer预测可微重投影至图像平面，使用Mask2Former生成的多视角伪标签训练，并引入时间一致性损失；微调阶段仅需50%标注数据

Result: 在nuScenes上比全监督基线提升最高2.5pp mIoU，标注数据用量减半，总训练时间减少三分之二

Conclusion: 可微重投影结合相机视角伪标签能学习到可迁移的BEV特征，为低标注自动驾驶感知提供可扩展路径

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [25] [Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation](https://arxiv.org/abs/2602.18083)
*Ioannis Kontogiorgakis,Athanasios Askitopoulos,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Fotios Balampanis,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 提出一种基于Sentinel-1/2和ERA-5数据的10米分辨率土壤湿度估计算法，机器学习方法表现优异，但预训练模型未显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有卫星土壤湿度产品分辨率过低（>1km），无法满足农业精细化应用需求。

Method: 结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据，使用机器学习进行建模，比较不同模态组合与时间窗参数，采用空间交叉验证确保泛化性，并评估Prithvi基础模型嵌入与传统光谱特征的效果。

Result: 最佳组合为Sentinel-2当日数据与Sentinel-1降轨数据配对，配合10天ERA-5历史窗口，R²达0.518；Prithvi嵌入较传统特征无显著提升（0.515 vs 0.514）。

Conclusion: 领域特异性光谱指数结合树集成模型是高效、实用的泛欧洲田尺度土壤湿度监测方案，传统特征工程仍具竞争力。

Abstract: Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.

</details>


### [26] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: 提出DohaScript，一个大规模多作者的手写天城文印地语数据集，填补了现有数据集规模小、多样性不足的空白。


<details>
  <summary>Details</summary>
Motivation: 现有天城文手写数据集规模小、仅限孤立字符或短词，缺乏词汇控制和作者多样性，无法反映连笔与复杂连字结构。

Method: 收集531名作者手写的6首传统印地语多哈诗，构建平行风格语料库，结合质量筛选与页面布局难度标注。

Result: 数据集具有高质量、强泛化性，基线实验验证其在手写识别、作者识别等任务上的可靠性。

Conclusion: DohaScript为低资源语言的手写天城文研究提供了标准化、可复现的基准数据集。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [27] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: 提出Diff2DGS框架，结合扩散模型与可学习变形模型，实现术中遮挡区域的高精度实时3D重建，并首次在SCARED数据集上评估深度准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡区域重建质量差，且缺乏3D真值基准，仅优化图像质量无法保证几何精度。

Method: 两阶段框架：第一阶段用扩散模型补充遮挡组织，第二阶段用可学习变形模型优化2D高斯泼溅的动态变形与几何结构。

Result: 在EndoNeRF和StereoMIS上分别达到38.02 dB和34.40 dB PSNR，显著优于SOTA，并在SCARED上验证了深度精度提升。

Conclusion: 仅优化图像质量不足以保证3D几何准确性，Diff2DGS通过联合优化外观与深度，实现更可靠的手术场景重建。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [28] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: 提出PrediT，一种无需训练的加速框架，通过线性多步预测减少DiT模型的迭代次数，实现最高5.54倍延迟降低且质量损失极小。


<details>
  <summary>Details</summary>
Motivation: DiT模型因迭代去噪过程计算成本高，现有加速方法因特征重用导致潜变量漂移和视觉退化。

Method: 将特征预测建模为线性多步问题，使用经典线性多步方法预测未来输出，结合动态校正器与自适应步长调节机制。

Result: 在多种DiT模型上实现最高5.54倍延迟降低，质量几乎无损失。

Conclusion: PrediT通过预测而非简单重用特征，有效平衡了加速与生成质量，为扩散模型加速提供了新思路。

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [29] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: 提出CapNav基准，评估视觉语言模型在考虑代理物理限制下的导航能力，发现现有模型在复杂空间推理上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 真实导航受代理运动约束影响，现有VLMs未充分考虑物理能力差异，导致导航决策不切实际。

Method: 构建CapNav基准，包含5种代理的物理特性、45个真实场景、473个导航任务和2365个问答对，评估13个主流VLMs。

Result: VLMs在严格运动约束下性能显著下降，尤其在需空间维度推理的障碍面前表现薄弱。

Conclusion: 未来VLMs需增强能力感知与空间推理能力，以实现更真实的具身导航。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [30] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: 提出OODBench基准和自动化评估指标，揭示现有视觉语言模型在处理分布外数据时性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现有VLMs假设数据独立同分布，但现实场景中分布外数据普遍存在，且缺乏评估基准，可能引发安全风险

Method: 构建OODBench（40K实例级分布外数据对），提出基于由浅入深提示问题的自动化评估指标

Result: 当前VLMs在OODBench上性能显著下降，即使图像类别常见；新指标能更全面评估不同难度问题的影响

Conclusion: OODBench为未来研究提供了可靠评估工具，推动分布外数据的获取与评估方法发展

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [31] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: ViTs在图形感知任务中表现不如人类和CNN，存在感知差距


<details>
  <summary>Details</summary>
Motivation: 探索ViTs在可视化图形感知任务中的表现，弥补其与人类感知能力的对比空白

Method: 基于Cleveland和McGill的经典研究，设计控制实验，对比ViTs、CNNs和人类在图形感知任务中的准确性

Result: ViTs在通用视觉任务中表现强，但在图形感知任务中与人类感知对齐度有限

Conclusion: ViTs应用于可视化系统时需谨慎，需解决其与人类感知的差距问题

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [32] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard通过结合思维链推理与规则政策，提升短视频广告中多模态虚假内容的检测精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 短视频平台的广告存在视觉、语音和字幕等多模态欺骗内容，传统社区安全过滤器无法满足精细化、政策驱动的审核需求。

Method: 提出BLM-Guard框架，融合ICoT数据合成、强化学习与多任务架构，利用规则驱动生成标注数据，并通过因果连贯性与政策合规性复合奖励优化模型。

Result: 在真实短视频广告数据上，BLM-Guard在准确性、一致性和泛化性上优于强基线模型。

Conclusion: BLM-Guard为商业广告审核提供了可解释、高效且政策对齐的多模态内容审计新范式。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [33] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: 提出一种后处理模块DMC，通过文本语义引导提升生成动作的物理真实性，显著改善脚部悬浮等问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成模型难以同时保证语义一致性与物理合理性，如脚部悬浮、穿模等。

Method: 设计自监督数据驱动的Distortion-aware Motion Calibrator (DMC)，输入失真动作与原始文本，学习校正为物理合理且语义一致的动作。

Result: 在T2M和T2M-GPT上FID分别降低42.74%和13.20%，R-Precision最高；对MoMask减少穿模33.0%，更贴近真实动作。

Conclusion: DMC是一种通用、有效的后处理框架，能协同提升文本到动作生成的语义与物理质量。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [34] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 首次研究离散图像分词器的对抗攻击脆弱性，并提出无监督对抗训练方法提升其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中广泛应用，但其对抗攻击脆弱性尚未被探索，影响模型安全性。

Method: 提出高效、通用的对抗攻击方法，并采用无监督对抗训练微调分词器，保持其他组件冻结。

Result: 所提方法在分类、多模态检索和图像描述任务中显著提升对抗鲁棒性，且泛化至未见任务和数据。

Conclusion: 分词器鲁棒性对下游任务至关重要，无监督对抗训练为构建安全多模态基础模型提供了重要路径。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [35] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: 提出DEIG框架，通过实例细节提取与融合模块实现细粒度可控多实例生成，显著提升语义准确性和空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂文本描述下细粒度语义理解不足，难以精确匹配局部化描述。

Method: 引入实例细节提取器（IDE）和细节融合模块（DFM），结合掩码注意力防止属性泄漏，并构建高精度标注数据集与DEIG-Bench基准。

Result: 在多个基准上超越现有方法，提升空间一致性、语义准确性和组合泛化能力，且可即插即用集成到扩散模型中。

Conclusion: DEIG实现了细粒度可控的多实例生成，为文本到图像生成提供了更精准的解决方案。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [36] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: 提出LOTS框架，结合全局草图与局部文本-草图对，提升时尚图像生成效果，并发布首个多文本-草图对的时尚数据集Sketchy。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效融合文本与草图模态，缺乏对局部语义的精准引导，同时缺少高质量的多模态时尚数据集。

Method: LOTS框架包含多级条件编码阶段和扩散模型对偶引导阶段，通过共享潜在空间编码局部特征，并在扩散去噪过程中使用注意力机制融合全局与局部引导。

Result: 在自建数据集Sketchy上显著超越SOTA，兼顾全局结构一致性与局部语义细节，且在非专业草图上表现出强鲁棒性。

Conclusion: LOTS有效实现了文本与草图的多级协同生成，Sketchy数据集为时尚生成研究提供了新基准，开源代码与数据推动领域发展。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [37] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: 提出Luminance-GS++，一种基于3D高斯泼溅的鲁棒多视图合成方法，通过全局光照校正与局部像素残差优化，在复杂光照下实现高质量重建与实时渲染。


<details>
  <summary>Details</summary>
Motivation: 真实场景中光照变化和相机ISP差异导致多视图光度不一致，破坏NeRF和3DGS等方法的光度一致性假设，降低重建质量。

Method: 结合全局视图自适应亮度调整与局部像素级残差细化，设计无监督目标函数联合优化光度校正与几何光度一致性，保持3DGS原始表达。

Result: 在低光、过曝及复杂光照场景中达到SOTA性能，提升重建保真度的同时维持实时渲染效率。

Conclusion: Luminance-GS++通过不修改底层表示的光度校正策略，有效解决多视图光照不一致问题，兼顾质量与效率。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [38] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 提出G-LoG双滤波方法，利用拓扑特征在MedMNIST上实现与深度学习模型相当的性能


<details>
  <summary>Details</summary>
Motivation: 传统单参数滤波在医学图像分析中捕捉复杂拓扑特征能力有限，需更有效的多参数方法

Method: 利用高斯拉普拉斯算子构建G-LoG双滤波，理论证明其持久模块在最大范数下稳定，并在MedMNIST上与多种深度学习基线对比

Result: G-LoG双滤波显著优于单参数滤波，简单MLP基于其特征表现媲美ResNet等复杂模型

Conclusion: G-LoG双滤波为医学图像提供高效轻量级拓扑特征提取方案，具备与深度学习竞争的潜力

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [39] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 提出一种基于退化流形的自感知目标检测框架，无需退化标签即可感知图像退化并独立于检测置信度提供风险信号。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测器在图像退化（如模糊、噪声、天气影响）下会沉默失败，缺乏对输入是否超出正常操作范围的感知能力，这在安全关键场景中不可接受。

Method: 通过多层对比学习训练轻量嵌入头，将特征空间按退化类型和严重程度组织，利用干净图像嵌入构建原型参考点，以几何偏差衡量退化程度。

Result: 在合成腐蚀基准、跨数据集零样本迁移和自然天气变化中均表现优异，实现清晰与退化图像的强分离，且对多种检测架构通用。

Conclusion: 退化感知的表示几何提供了一种实用、与检测器无关的自感知基础，显著提升模型在真实退化环境中的可靠性。

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [40] [Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges](https://arxiv.org/abs/2602.18406)
*Minh Dinh,Stéphane Deny*

Main category: cs.CV

TL;DR: 本文展示了一种从数据中学习等变算子的架构，可在噪声MNIST上实现分布外分类，超越传统与等变网络的局限。


<details>
  <summary>Details</summary>
Motivation: 深度学习在视觉任务中表现优异，但在训练中罕见的群对称变换（如异常姿态、尺度、位置）下泛化能力不足。

Method: 利用简单数据集（旋转和平移的噪声MNIST）学习隐空间中的等变算子，无需先验知识。

Result: 该架构成功实现了分布外分类，优于传统与传统等变网络。

Conclusion: 虽然方法概念上有吸引力，但扩展到更复杂数据集仍面临挑战。

Abstract: Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.

</details>


### [41] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出一种基于头戴和手部姿态控制的生成式视频世界模型，实现更自然的XR交互


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型仅能接受文本或键盘等粗粒度控制，无法满足XR中对真实运动响应的需求

Method: 构建双向视频扩散模型教师，采用改进的扩散变换器条件机制，融合3D头姿与关节级手部姿态，通过知识蒸馏生成因果交互系统

Result: 人类受试者评估显示，任务表现提升，用户对动作控制的感知显著优于基线方法

Conclusion: 该模型实现了高精度、沉浸式的egocentric虚拟环境生成，为XR交互开辟新路径

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [42] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 提出首个实时、完全因果的空间感知对话动作生成方法，可在VR头显上部署，实现用户位置与语音驱动的全身动作与自然凝视控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对用户空间位置的感知能力，无法实现自然的转向、响应和凝视交互。

Method: 采用因果Transformer VAE与交错潜在令牌实现流式推理，结合基于用户轨迹和音频的流匹配模型，并引入无分类器引导的凝视评分机制以解耦学习与控制。

Result: 在Embody 3D数据集上实现SOTA动作质量，推理速度超300 FPS（比非因果基线快3倍），并成功在真实VR系统中部署。

Conclusion: 该方法首次实现高实时性、高自然度的空间感知对话代理，为VR与数字人应用提供了可行的实时解决方案。

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [43] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 提出MemStream方法，通过扩展token预算和自适应选择策略提升视频流理解性能，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法因token数量有限导致细粒度视觉信息丢失，且相似度评分随时间偏置，偏向后期帧。

Method: 扩展token预算，引入自适应选择策略减少冗余，结合无训练的检索混合专家模型提升帧检索准确性。

Result: 在CG-Bench、LVBench和VideoMME (Long)上分别比ReKV提升8.0%、8.5%和2.4%。

Conclusion: MemStream通过更精细的时空建模有效提升了视频流问答性能，为持续视频理解提供了新方向。

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving](https://arxiv.org/abs/2602.17677)
*Sutej Kulgod,Sean Ye,Sanchit Tanwar,Christoffer Heckman*

Main category: cs.LG

TL;DR: 提出新方法减少视觉语言模型在多选题中依赖文本线索，强制其依赖视觉信息


<details>
  <summary>Details</summary>
Motivation: 现有合成多选题数据存在文本线索偏差，导致模型无需视觉输入即可高准确率作答

Method: 通过解耦正确答案与语言伪影，并采用课程学习策略，强制模型依赖视觉 grounding

Result: 将盲猜准确率从高于随机66.9%降至仅2.9%，显著消除文本捷径

Conclusion: 方法有效提升模型对视觉内容的真实理解能力，确保评估更可靠

Abstract: Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.

</details>


### [45] [Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization](https://arxiv.org/abs/2602.17679)
*Saksham Kiroriwal,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.LG

TL;DR: 提出POGPN-JPSS框架，结合专家知识与结构化概率模型，显著加速高维多阶段生物乙醇生产过程的优化


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化忽略中间观测和过程结构，难以处理高维多阶段系统；需利用专家知识提取低维特征以有效利用中间观测

Method: 结合部分可观测高斯过程网络（POGPN）与联合参数与状态空间（JPSS）建模，利用专家知识从高维时序数据中提取低维潜变量

Result: 在高维生物乙醇生产仿真中，POGPN-JPSS比现有方法快一倍达到性能阈值，且更可靠，显著节省时间和资源

Conclusion: 融合专家知识与结构化概率模型对加速制造过程优化至关重要

Abstract: Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.

</details>


### [46] [BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs](https://arxiv.org/abs/2602.17680)
*Yujia Wang,Jihong Guan,Wengen Li,Shuigeng Zhou,Xuhong Wang*

Main category: cs.LG

TL;DR: BioBridge通过持续预训练融合蛋白质领域知识与通用语言能力，实现多任务高性能。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型适应性差，通用大模型缺乏蛋白质理解能力，二者各有局限。

Method: 采用域增量持续预训练（DICP）与PLM-Projector-LLM跨模态对齐，端到端优化多任务学习。

Result: 在EC、BindingDB等蛋白质基准上性能媲美主流PLM，在MMLU、RACE等通用任务上媲美LLM。

Conclusion: BioBridge成功结合领域适应性与通用语言能力，实现蛋白质理解与通用推理的统一。

Abstract: Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.

</details>


### [47] [LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs](https://arxiv.org/abs/2602.17681)
*Ofir Gordon,Lior Dikstein,Arnon Netzer,Idan Achituve,Hai Victor Habi*

Main category: cs.LG

TL;DR: 提出LATMiX方法，通过可学习的仿射变换提升MX格式下大模型的低比特量化性能


<details>
  <summary>Details</summary>
Motivation: 现有变换方法受限于旋转或Hadamard变换，且在MX格式下性能下降，缺乏对激活分布与量化结构的联合分析

Method: 理论推导MX量化误差上界，提出LATMiX——可学习的可逆仿射变换，使用深度学习工具优化

Result: 在多种模型规模和零样本基准上，LATMiX显著优于强基线，提升低比特MX量化精度

Conclusion: 联合建模激活分布与量化结构可有效提升量化鲁棒性，可学习变换优于固定变换

Abstract: Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.

</details>


### [48] [Duality Models: An Embarrassingly Simple One-step Generation Paradigm](https://arxiv.org/abs/2602.17682)
*Peng Sun,Xinyi Shang,Tao Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出Duality Models (DuMo)，通过单输入双输出架构同时预测速度与流映射，提升一致性生成模型的稳定性和效率，在ImageNet上2步实现SOTA FID 1.79。


<details>
  <summary>Details</summary>
Motivation: 传统方法因训练预算分割导致多步目标过度占优、少步生成训练不足，限制收敛与扩展性。

Method: 采用共享骨干网络与双头结构，单输入同时预测速度$v_t$和流映射$u_t$，利用多步目标的几何约束直接优化少步估计，无需分离训练目标。

Result: 在ImageNet 256×256上，679M Diffusion Transformer配合SD-VAE仅用2步达到FID 1.79的SOTA性能。

Conclusion: DuMo通过双输出范式打破训练预算分割，显著提升生成效率与稳定性，为一致性生成模型提供新方向。

Abstract: Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional "one input, one output" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a "one input, dual output" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo

</details>


### [49] [Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates](https://arxiv.org/abs/2602.17683)
*Irene Iele,Giulia Romoli,Daniele Molino,Elena Mulero Ayllón,Filippo Ruffini,Paolo Soda,Matteo Tortora*

Main category: cs.LG

TL;DR: 提出一种基于Transformer的概率性NDVI短期预测框架，结合气象协变量与时间加权损失函数，在稀疏卫星数据下显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于云层遮挡导致卫星观测稀疏不规则，加之作物生长环境异质，现有NDVI预测方法精度受限。

Method: 采用分离历史植被动态与未来气象协变量的Transformer架构，引入时间距离加权分位数损失，并构建累积与极端天气特征以捕捉植被响应延迟效应。

Result: 在欧洲卫星数据上超越多种统计、深度学习与时序基线模型，在点预测与概率预测指标上均表现最优，消融实验验证历史信息与气象协变量的互补作用。

Conclusion: 该框架有效应对稀疏观测与不确定性，为精准农业提供可靠短期植被动态预测工具。

Abstract: Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.

</details>


### [50] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: 提出CodeScaler，一种无需执行的奖励模型，显著提升代码生成的RL训练与推理效率，超越传统基于单元测试的方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于执行反馈的强化学习受限于高质量测试用例的稀缺与不可靠，亟需可扩展的无执行奖励机制。

Method: CodeScaler通过从验证代码问题中提取偏好数据，结合语法感知的代码提取与保有效性奖励塑造，实现稳定优化。

Result: 在五个编码基准上，Qwen3-8B-Base平均提升11.72分，超越二元执行RL 1.82分；推理延迟降低10倍，并在RM-Bench多领域超越现有奖励模型。

Conclusion: CodeScaler实现了无测试用例的可扩展强化学习，同时提升推理效率与跨领域泛化能力，为代码生成RL提供新范式。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [51] [Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling](https://arxiv.org/abs/2602.17685)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.LG

TL;DR: 本文提出一种统一的共椭圆机动框架，结合霍曼转移、安全椭圆近距操作和显式加油逻辑，用于低轨多目标碎片清除，并通过强化学习方法显著提升任务效率和计算性能。


<details>
  <summary>Details</summary>
Motivation: 多目标主动碎片清除任务面临复杂轨道环境、资源约束和计算效率挑战，传统启发式方法难以兼顾效率与安全性。

Method: 提出统一的共椭圆机动框架，整合霍曼转移、安全椭圆操作与加油逻辑，并在真实轨道仿真环境中对比贪婪启发式、蒙特卡洛树搜索和掩码PPO强化学习算法。

Result: 掩码PPO在100个测试场景中表现最优，访问碎片数量是贪婪算法的两倍，且显著优于MCTS的运行时间。

Conclusion: 现代强化学习方法在主动碎片清除任务中展现出高效率、高安全性与可扩展性，为未来空间任务自动化提供了新路径。

Abstract: This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.

</details>


### [52] [Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO](https://arxiv.org/abs/2602.17686)
*Bowen Yu,Maolin Wang,Sheng Zhang,Binhao Wang,Yi Wen,Jingtong Gao,Bowen Liu,Zimo Zhao,Wanyu Wang,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 通过三阶段课程学习框架，在不牺牲可解释性的情况下，将大模型的思维链推理压缩到小模型中，显著提升准确率并减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 大模型生成的思维链过于冗长，小模型难以复现，现有方法压缩后丧失可解释性。

Method: 采用三阶段课程学习：1）掩码乱序重建以建立结构理解；2）使用GRPO优化掩码补全任务，平衡准确率与简洁性；3）针对失败案例进行定向重写并用GRPO优化。

Result: 在GSM8K上，Qwen2.5-3B-Base准确率提升11.29%，输出长度减少27.4%，超越指令微调和现有蒸馏方法。

Conclusion: 该框架有效解决师生模型能力差距，实现高效、简洁且可解释的思维链蒸馏。

Abstract: Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.

</details>


### [53] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: AnchorTree通过利用抽象语法树引导扩散模型生成结构正确的代码，提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型在生成代码时忽略编程语言的严格结构，导致生成的程序无法执行。

Method: AnchorTree利用抽象语法树（AST）作为结构先验，优先修复关键词和标识符等关键语义单元，构建生成骨架。

Result: 基于AnchorTree的AnCoder模型以参数高效的方式实现了高质量代码生成。

Conclusion: 结构化锚定的扩散方法能有效提升代码生成的正确性与效率。

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [54] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: 提出Robust-MMR框架，通过自监督预训练提升医学视觉语言模型在域偏移下的鲁棒性，显著提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态预训练方法忽视鲁棒性，导致模型在成像设备、协议和报告风格变化时性能下降。

Method: 提出Robust-MMR，结合非对称扰动感知掩码、域一致性正则化和模态弹性约束，增强域不变表征学习。

Result: 在VQA-RAD、SLAKE、VQA-2019、MELINDA和ROCO等基准上全面超越基线，交叉域准确率最高提升3.8个百分点，扰动下性能下降显著缓解。

Conclusion: 在预训练阶段显式建模鲁棒性，可显著提升医学视觉语言模型的泛化性与临床实用性。

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [55] [Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering](https://arxiv.org/abs/2602.17691)
*Craig Atkinson*

Main category: cs.LG

TL;DR: HELIX通过几何约束解耦采样温度与幻觉，实现高温度下语义多样性与逻辑一致性的平衡，显著提升量化模型的输出质量。


<details>
  <summary>Details</summary>
Motivation: 量化语言模型在高采样温度下面临重复输出与语义离散的两难困境，现有方法无法兼顾多样性与正确性。

Method: HELIX构建真值流形，通过统一真值分数（UTS）检测轨迹偏离，并施加稀疏引导向量（仅影响0.2-2.5%标记）矫正隐藏状态，实现几何锚定。

Result: 在4位量化的Granite模型上，T=3.0时GSM8K准确率保持88.84%，MMLU保持72.49%；高温度下创意重复率从70-80%降至5-20%，跨架构验证提升46.7%独特概念生成。

Conclusion: HELIX揭示了高熵创意储备空间，通过几何 tethering 实现多温度合成，突破传统温度选择的限制，为量化模型提供高效、通用的多样性控制框架。

Abstract: Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.
  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.
  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.

</details>


### [56] [Agentic Unlearning: When LLM Agent Meets Machine Unlearning](https://arxiv.org/abs/2602.17692)
*Bin Wang,Fan Wang,Pingping Wang,Jinyu Cong,Yang Yu,Yilong Yin,Zhongyi Han,Benzheng Wei*

Main category: cs.LG

TL;DR: 提出同步反流遗忘（SBU）框架，联合消除智能体参数与记忆中的敏感信息，提升遗忘效果并防止信息回流。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法仅针对模型参数，忽视记忆回流导致敏感信息重现，缺乏参数与记忆的统一遗忘策略。

Method: SBU框架通过记忆路径的依赖闭包剪枝与参数路径的随机参考对齐，结合同步双更新协议实现双向协同遗忘。

Result: 在医疗QA基准上，SBU有效清除目标私有信息，同时保持保留数据性能损失有限。

Conclusion: 同步联合遗忘显著提升遗忘完整性，为闭环智能体的隐私保护提供新范式。

Abstract: In this paper, we introduce \textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.

</details>


### [57] [A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU](https://arxiv.org/abs/2602.17693)
*Yuchen Luo,Fangyue Zhu,Ruining Zhou,Mingzhe Huang,Jian Zhu,Fanyu Fan,Wei Shao*

Main category: cs.LG

TL;DR: Ascend NPU上PTQ在推理模型上表现受平台限制，4位激活量化不稳定，8位更稳定，端到端加速受限于动态量化开销。


<details>
  <summary>Details</summary>
Motivation: 探索PTQ在Ascend NPU上的适用性，弥补与GPU架构研究的差距。

Method: 在DeepSeek-R1-Distill-Qwen和QwQ系列模型上评估AWQ、GPTQ、SmoothQuant和FlatQuant四种PTQ算法。

Result: 4位权重量化可行，但4位权值-激活量化导致层间校准不稳定与逻辑崩溃；8位量化稳定，INT8部署因动态开销受限加速效果。

Conclusion: Ascend NPU上部署量化推理模型需谨慎选择量化策略，当前动态量化开销是端到端加速的主要瓶颈。

Abstract: Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.

</details>


### [58] [AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models](https://arxiv.org/abs/2602.17694)
*Hui Ma,Shaoyu Dou,Ya Liu,Fei Xing,Li Feng,Feng Pi*

Main category: cs.LG

TL;DR: 提出AsynDBT算法，通过异步分布式双层优化提升上下文学习性能，兼顾隐私与异构环境适应性。


<details>
  <summary>Details</summary>
Motivation: 云API成本高、ICL依赖高质量数据且难共享，传统联邦学习在ICL中面临慢节点和数据异构问题。

Method: 设计异步分布式双层调优（AsynDBT）算法，联合优化上下文样本与提示片段，基于LLM反馈迭代改进。

Result: 在多个基准数据集上验证了AsynDBT的有效性与高效性，理论证明其收敛性。

Conclusion: AsynDBT为隐私保护下的联邦ICL提供了高效、可扩展的解决方案，突破了数据共享与计算异构限制。

Abstract: With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.

</details>


### [59] [EXACT: Explicit Attribute-Guided Decoding-Time Personalization](https://arxiv.org/abs/2602.17695)
*Xin Yu,Hanwen Xing,Lingzhou Xue*

Main category: cs.LG

TL;DR: EXACT是一种解码时个性化方法，通过可解释属性和语义检索实现用户偏好自适应，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有解码时个性化方法依赖隐式、不可解释的偏好表示，且使用固定不变的用户表征，无法应对偏好随提示变化的问题。

Method: EXACT在离线阶段通过偏好反馈识别用户特定属性子集，在线阶段根据提示语义检索最相关属性并注入上下文以引导生成。

Result: 理论上保证了算法近似性能，证明了相似性检索能有效缓解上下文偏好漂移，且在多个数据集上显著优于基线方法。

Conclusion: EXACT通过可解释属性与上下文感知检索，实现了高效、自适应的个性化生成，兼具性能与可解释性。

Abstract: Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.

</details>


### [60] [Can LLM Safety Be Ensured by Constraining Parameter Regions?](https://arxiv.org/abs/2602.17696)
*Zongmin Li,Jian Su,Farah Benamara,Aixin Sun*

Main category: cs.LG

TL;DR: 当前安全区域识别方法难以稳定、跨数据集地识别大模型中的安全参数子集。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中是否存在可被可靠识别的安全参数区域，以提升模型安全性。

Method: 系统评估四种不同参数粒度的安全区域识别方法，跨越四个LLM家族，使用十个安全评估数据集进行测试。

Result: 识别出的安全区域重叠度低至中等（IoU），使用效用数据集细化后重叠度显著下降。

Conclusion: 现有技术无法可靠识别稳定且与数据集无关的安全区域。

Abstract: Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.

</details>


### [61] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: 本文将大语言模型推理视为可配置系统，引入变异性建模方法，高效分析超参数配置对能耗与性能的影响，实现从少量测量中准确预测推理行为。


<details>
  <summary>Details</summary>
Motivation: LLM推理计算开销巨大，传统穷举评估因配置空间爆炸而不可行，亟需系统化方法优化能效。

Method: 基于Hugging Face Transformers构建特征化变异性模型，采样代表性配置，测量能耗、延迟、准确率，并训练预测模型。

Result: 变异性建模有效管理配置复杂性，揭示超参数交互与权衡，仅需少量测量即可高精度预测推理行为。

Conclusion: 本工作开创了软件工程与机器学习交叉的新方向，为LLM的高效可持续配置提供了系统化方法。

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [62] [ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs](https://arxiv.org/abs/2602.17698)
*Xinlin Li,Timothy Chou,Josh Fromm,Zichang Liu,Yunjie Pan,Christina Fragouli*

Main category: cs.LG

TL;DR: ScaleBITS 提出一种无需运行时开销的自动细粒度混合精度量化框架，在超低比特下显著提升 LLM 性能。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法在低于4比特时因权重敏感性非均匀且缺乏合理精度分配策略而效果受限，或引入高运行时开销。

Method: 基于新敏感性分析，提出硬件对齐的块状权重划分与双向通道重排序，将全局比特分配建模为约束优化问题，并设计可扩展贪心近似算法。

Result: 在超低比特下，ScaleBITS 相比均匀量化提升最高达36%，优于现有敏感性感知方法最高13%，且无运行时开销。

Conclusion: ScaleBITS 实现了高效、自动、细粒度的混合精度量化，为超低比特 LLM 部署提供了新范式。

Abstract: Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.

</details>


### [63] [Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure](https://arxiv.org/abs/2602.17699)
*Chandrasekhar Gokavarapu,Sudhakar Gadde,Y. Rajasekhar,S. R. Bhargava*

Main category: cs.LG

TL;DR: 本文提出一个统一框架，通过可计算的分布偏移度量为预测模型在分布偏移下的风险提供显式上界保证。


<details>
  <summary>Details</summary>
Motivation: 现有模型在分布偏移下缺乏可验证的风险保障和可解释性，依赖事后解释，难以保证安全性与可靠性。

Method: 在可验证的正则性和复杂性约束下，推导出风险上界，建立基于显式不等式的风险认证框架，并通过可识别性条件增强可解释性。

Result: 实现了对非平凡规模模型的可验证认证，明确了不可认证的失败模式，并隔离了风险边界。

Conclusion: 该框架为分布偏移下的模型安全性提供了理论保障，推动从事后解释转向基于可识别性的本质可解释性研究。

Abstract: Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.

</details>


### [64] [MIDAS: Mosaic Input-Specific Differentiable Architecture Search](https://arxiv.org/abs/2602.17700)
*Konstanty Subbotko*

Main category: cs.LG

TL;DR: MIDAS通过引入输入特定的动态架构参数和自注意力机制，显著提升了DARTS的性能与鲁棒性，在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有可微NAS方法如DARTS在实际应用中受限，因静态架构参数缺乏灵活性和鲁棒性。

Method: MIDAS用自注意力机制生成输入特定的架构参数，按激活图的空间块局部化架构选择，并采用无参数、拓扑感知的搜索空间简化边选择。

Result: 在CIFAR-10上达到97.42%准确率，NAS-Bench-201中始终找到全局最优架构，RDARTS中在两个搜索空间上设SOTA。

Conclusion: MIDAS通过patchwise注意力增强操作区分度，生成类感知且单峰的参数分布，为架构解码提供可靠指导。

Abstract: Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.

</details>


### [65] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: PaCoDi通过频域解耦实现高效长程时间序列生成，大幅降低计算成本并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统时序扩散模型因注意力机制的O(L²)复杂度和局部耦合问题，难以高效建模长程依赖。

Method: 提出频域本征架构PaCoDi，利用傅里叶变换解耦信号为独立实虚分量，结合Mean Field Theory近似与谱SDE理论，通过Hermitian对称压缩序列长度50%，并推导异方差损失函数。

Result: 在生成质量与推理速度上超越基线模型，实现理论严谨且计算高效的时序生成。

Conclusion: 频域解耦是突破时序扩散模型效率瓶颈的有效路径，PaCoDi为长程依赖建模提供了新范式。

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [66] [Provable Adversarial Robustness in In-Context Learning](https://arxiv.org/abs/2602.17743)
*Di Zhang*

Main category: cs.LG

TL;DR: 本文提出一种分布鲁棒元学习框架，分析了在Wasserstein分布偏移下上下文学习的最坏情况性能，并揭示了模型容量与对抗鲁棒性之间的平方根关系及样本复杂度的平方惩罚。


<details>
  <summary>Details</summary>
Motivation: 现有理论假设测试任务与预训练分布相似，忽视了现实中的对抗分布偏移，威胁模型可靠性。

Method: 引入基于Wasserstein距离的分布鲁棒元学习框架，聚焦线性自注意力Transformer，推导出对抗扰动强度、模型容量和上下文样本数之间的非渐近边界。

Result: 模型鲁棒性随容量平方根增长（ρ_max ∝ √m），对抗环境下样本复杂度惩罚与扰动平方成正比（N_ρ - N_0 ∝ ρ²），合成实验验证了该缩放律。

Conclusion: 模型容量是实现分布鲁棒性的根本资源，本研究为理解ICL在对抗条件下的局限性提供了理论基础。

Abstract: Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($ρ$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($ρ_{\text{max}} \propto \sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_ρ- N_0 \propto ρ^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.

</details>


### [67] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: 提出MePoly，一种基于多项式能量模型的策略参数化方法，具有显式概率密度，可实现精确熵最大化，并在多模态决策任务中超越基线。


<details>
  <summary>Details</summary>
Motivation: 传统参数化策略难以表示多模态解，扩散策略虽能捕捉多模态但缺乏显式概率密度，阻碍策略梯度优化。

Method: 提出MePoly，利用多项式能量模型构建显式且可计算的概率密度函数，基于矩问题理论保证对任意分布的通用逼近能力。

Result: MePoly能有效捕捉复杂非凸流形，在多个基准任务中性能优于现有方法。

Conclusion: MePoly为多模态策略学习提供了一种理论严谨、计算可行的新框架，兼顾熵最大化与表达能力。

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [68] [Bayesian Optimality of In-Context Learning with Selective State Spaces](https://arxiv.org/abs/2602.17744)
*Di Zhang,Jiaqi Xing*

Main category: cs.LG

TL;DR: 通过贝叶斯最优序列预测重新定义上下文学习（ICL），证明选择性SSM比梯度下降更高效，实验验证其在合成与真实任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有研究将Transformer视为隐式梯度下降，但未能解释ICL的统计效率；本文旨在从贝叶斯最优推断角度提供更合理的理论解释。

Method: 将ICL形式化为潜变量序列任务的元学习，针对线性高斯状态空间模型（LG-SSM）证明选择性SSM渐近收敛于贝叶斯最优预测器，并构建反例揭示梯度下降的统计劣势。

Result: 选择性SSM在合成与字符级马尔可夫任务中更快收敛至贝叶斯最优风险，样本效率更高，对结构化噪声下潜变量追踪更鲁棒，显著优于线性Transformer。

Conclusion: ICL的本质是贝叶斯最优推断而非隐式优化，该框架为Transformer架构设计提供了统计学基础，尤其支持选择性SSM的优越性。

Abstract: We propose Bayesian optimal sequential prediction as a new principle for understanding in-context learning (ICL). Unlike interpretations framing Transformers as performing implicit gradient descent, we formalize ICL as meta-learning over latent sequence tasks. For tasks governed by Linear Gaussian State Space Models (LG-SSMs), we prove a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor, converging to the posterior predictive mean. We further establish a statistical separation from gradient descent, constructing tasks with temporally correlated noise where the optimal Bayesian predictor strictly outperforms any empirical risk minimization (ERM) estimator. Since Transformers can be seen as performing implicit ERM, this demonstrates selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. Experiments on synthetic LG-SSM tasks and a character-level Markov benchmark confirm selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. This reframes ICL from "implicit optimization" to "optimal inference," explaining the efficiency of selective SSMs and offering a principled basis for architecture design.

</details>


### [69] [Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly](https://arxiv.org/abs/2602.17997)
*Zehao Jin,Yaoye Zhu,Chen Zhang,Yanan Sui*

Main category: cs.LG

TL;DR: 使用果蝇完整脑连接组构建神经网络控制器，实现高效运动控制，无需任务特定调参。


<details>
  <summary>Details</summary>
Motivation: 脑连接组在具身强化学习中的控制器应用尚未探索，本研究旨在利用真实生物神经架构提升控制效率。

Method: 提出FlyGM模型，将果蝇脑连接组建模为有向消息传递图，结合生物力学模型进行运动控制。

Result: FlyGM在多样运动任务中实现稳定控制，样本效率和性能优于重连图、随机图和MLP。

Conclusion: 静态脑连接组可有效转化为具身学习中的神经策略，证明生物神经结构对运动控制的优越性。

Abstract: Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.

</details>


### [70] [Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring](https://arxiv.org/abs/2602.17751)
*Nina Brolich,Simon Geis,Maximilian Kasper,Alexander Barnhill,Axel Plinge,Dominik Seuß*

Main category: cs.LG

TL;DR: 提出在低成本微控制器上部署高效AI模型进行鸟类声学监测，实现低功耗、高压缩率的边缘计算解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统鸟类监测方法成本高、效率低，现有机器学习方案需大量计算资源，难以在野外部署。

Method: 在微控制器上训练并压缩神经网络模型，评估不同物种数量对模型压缩率和性能的影响，并在多种硬件平台进行基准测试。

Result: 实现了高压缩率且性能损失极小，验证了在边缘设备上部署能量自给型鸟类监测系统的可行性。

Conclusion: 该方法为低成本、低功耗的野外生物监测提供了可行的边缘AI解决方案。

Abstract: Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.

</details>


### [71] [Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs](https://arxiv.org/abs/2602.17778)
*Zachary Coalson,Bo Fang,Sanghyun Hong*

Main category: cs.LG

TL;DR: 论文发现对话大模型存在“轮次放大”漏洞，攻击者可通过诱导澄清行为长期延长交互，且无需修改提示，攻击可跨任务和模型泛化。


<details>
  <summary>Details</summary>
Motivation: 对话大模型的多轮交互长度是运营成本的主要因素，现有攻击多依赖提示优化，缺乏对对话动态机制的系统性研究。

Method: 提出“轮次放大”新攻击模式，通过识别与澄清行为相关的无查询依赖的激活子空间，利用微调或参数扰动诱导模型持续追问。

Result: 在多个指令微调模型和基准上，该攻击显著增加轮次且保持合规性，现有防御手段几乎无效。

Conclusion: 澄清行为的机制性漏洞是可扩展攻击的新路径，需重新设计对话模型的安全与收敛机制。

Abstract: Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.

</details>


### [72] [Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors](https://arxiv.org/abs/2602.17783)
*Xiangyu Sun,Shirin Hosseinmardi,Amin Yousefpour,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出基于物理信息高斯过程的框架，高效求解多材料多物理场拓扑优化问题，生成高分辨率且物理解释性强的结构设计。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在处理多材料、多物理场拓扑优化时面临计算成本高、频谱偏差和非自伴物理模型难以处理等问题。

Method: 采用独立的高斯过程先验建模主变量、伴随变量和设计变量，其均值函数由专为PDE建模设计的神经网络参数化，通过联合优化目标函数、多物理场势能泛函和设计约束进行参数估计，并引入加速微分与积分方案。

Result: 在合规性最小化、热传导优化、柔顺机构设计及热力耦合优化等基准问题上成功生成高分辨率、尖锐界面的材料分布，结果经开源代码和COMSOL验证。

Conclusion: 所提PIGP框架能有效同时求解耦合多物理场与设计问题，突破传统ML方法的局限性，具备强物理一致性和工程实用性。

Abstract: Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.

</details>


### [73] [Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds](https://arxiv.org/abs/2602.17798)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出GrMoE，通过Grassmannian流形上的Matrix Bingham分布实现可调稀疏性路由，消除专家坍塌，提升负载均衡与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统softmax门控无法在稀疏性与专家利用率之间提供可控权衡，易导致专家坍塌。

Method: 基于Grassmannian流形的Matrix Bingham分布构建门控机制，引入浓度矩阵Λ作为连续控制变量，结合变分推断实现不确定性感知路由。

Result: 在多个规模MoE模型上实现0%专家坍塌，负载均衡提升15–30%，困惑度相当或更好，稀疏性与浓度呈单调关系，支持后训练调优。

Conclusion: GrMoE首次建立浓度控制稀疏性的理论框架，实现可解释、稳定、可调的专家路由。

Abstract: Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $Λ$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\% routing collapse across all seeds, comparable or better perplexity with 15--30\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.

</details>


### [74] [Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.17809)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出Stiefel-Bayes Adapters (SBA)，一种基于流形贝叶斯的参数高效微调方法，在不增加参数成本下显著提升模型不确定性校准与域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）缺乏可靠的不确定性估计，导致预测校准差、域外表现不可靠。

Method: 在Stiefel流形上引入Matrix Langevin先验，通过切空间Laplace近似与测地收缩进行后验推断，避免了传统投影带来的方差膨胀问题。

Result: 在多个大模型和基准上，SBA在保持与LoRA相当性能的同时，将期望校准误差降低18%-34%，域外选择性预测AUROC提升12%-25%，优于五模型集成且参数成本更低。

Conclusion: 不确定性建模的几何结构至关重要，正确嵌入流形先验比简单添加贝叶斯处理更有效。

Abstract: Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.

</details>


### [75] [Avoid What You Know: Divergent Trajectory Balance for GFlowNets](https://arxiv.org/abs/2602.17827)
*Pedro Dall'Antonia,Tiago da Silva,Daniel Csillag,Salem Lahlou,Diego Mesquita*

Main category: cs.LG

TL;DR: 提出ACE算法提升GFlowNets的探索效率，显著改善目标分布逼近与高奖励状态发现能力


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练中易浪费样本于已较好逼近的区域，探索效率受限

Method: 引入自适应互补探索（ACE），通过额外训练一个探索型GFlowNet专门寻找主模型未充分探索的高奖励区域

Result: 在逼近目标分布精度和发现多样高奖励状态方面显著优于先前方法

Conclusion: ACE通过互补探索机制有效提升GFlowNets的训练效率与性能

Abstract: Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.

</details>


### [76] [Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models](https://arxiv.org/abs/2602.17829)
*Preetom Biswas,Giulia Pedrielli,K. Selçuk Candan*

Main category: cs.LG

TL;DR: 提出ruleXplain框架，利用大语言模型从仿真系统中提取带时间延迟的因果规则，提升时序因果推断的可解释性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理具有延迟效应的复杂动态系统，且生成的解释缺乏通用性和可解释性。

Method: 构建带时间算子和延迟语义的符号规则语言，结合仿真器生成反事实输入，通过结构化提示引导LLM生成因果规则，并采用闭环优化确保规则一致性。

Result: 在PySIRTEM传染病模型和EnergyPlus能耗模型上验证，规则能有效重建输入、编码因果关系，并泛化至未见的相位动态。

Conclusion: ruleXplain显著提升时序因果系统的可解释性，为复杂动态系统的因果发现提供新范式。

Abstract: Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.

</details>


### [77] [Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning](https://arxiv.org/abs/2602.17835)
*Sirui Chen,Yunzhe Qi,Mengting Ai,Yifan Sun,Ruizhong Qiu,Jiaru Zou,Jingrui He*

Main category: cs.LG

TL;DR: Iprox通过两阶段框架从目标模型中生成影响保留的代理模型，显著降低梯度数据选择的计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有梯度数据选择方法（如TracIn）计算开销大，小模型代理又无法灵活调整或对齐目标模型，限制了其在大语言模型中的应用。

Method: Iprox采用两阶段方法：先低秩压缩保留目标模型的影响信息，再对齐梯度与logits，构建与目标模型影响一致的小型代理。

Result: 在Qwen3-4B和Llama3.2等模型上，Iprox构建的1.5B代理超越1.7B标准代理，且计算成本降低超50%，性能优于基线方法。

Conclusion: Iprox实现了可扩展、高保真的梯度影响代理，使大模型的数据选择更高效实用。

Abstract: Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.

</details>


### [78] [Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846)
*Nick Dodson,Xinyu Gao,Qingsong Wang,Yusu Wang,Zhengchao Wan*

Main category: cs.LG

TL;DR: 本文提出几何框架揭示扩散模型中记忆化风险在噪声调度中的非均匀分布，发现中等噪声区间为记忆化危险区，并提出几何感知干预方法缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽生成质量高，但存在记忆训练数据的隐私风险，目前对记忆化与泛化发生的机制、噪声尺度影响及数据几何作用尚不清晰。

Method: 构建基于高斯壳覆盖性和后验集中性的几何框架，将噪声调度划分为三个区域，分析各区域的记忆化机制。

Result: 发现中等噪声区间记忆化最显著，小噪声因覆盖不足、大噪声因后验稀疏而抵抗记忆化，提出基于几何条件的针对性干预策略。

Conclusion: 记忆化风险高度依赖噪声尺度，几何视角为理解并缓解扩散模型隐私风险提供了新途径。

Abstract: Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

</details>


### [79] [Dual Length Codes for Lossless Compression of BFloat16](https://arxiv.org/abs/2602.17849)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: 提出一种双长度编码方法，在保持较高压缩率的同时显著提升解码速度并降低硬件复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有Huffman编码解码慢、硬件复杂，而通用编码如指数哥伦布码未能利用符号频率分布，亟需平衡压缩效率与解码速度的方案。

Method: 采用双长度编码：前8个高频符号用4位编码，其余248个符号用9位编码，通过1位前缀区分长度，仅用8条目的查找表实现编解码。

Result: 压缩率为18.6%，略低于Huffman的21.3%，但解码速度大幅提升，硬件复杂度显著降低。

Conclusion: 双长度编码在实用性上优于传统方法，适合LLM训练与推理中的高效通信场景。

Abstract: Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.

</details>


### [80] [Neural Prior Estimation: Learning Class Priors from Latent Representations](https://arxiv.org/abs/2602.17853)
*Masoud Yavari,Payman Moallem*

Main category: cs.LG

TL;DR: NPE通过学习特征条件的先验估计，解决深度网络中的类别不平衡问题，显著提升少数类性能。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡导致深度神经网络产生系统性偏差，传统方法依赖显式类别计数或超参数，缺乏理论支撑。

Method: 提出神经先验估计器（NPE），通过联合训练的先验估计模块从潜在表征中学习特征条件的对数先验，并基于单向逻辑损失优化，融入对数调整形成NPE-LA。

Result: 在长尾CIFAR和不平衡语义分割数据集（STARE, ADE20K）上一致提升性能，尤其对少数类效果显著，理论证明在神经坍缩条件下可恢复真实对数先验。

Conclusion: NPE是一种轻量级、理论严谨的先验估计框架，无需显式类分布信息，有效实现偏差感知预测。

Abstract: Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.

</details>


### [81] [JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861)
*Ryan McKenna,Galen Andrew,Borja Balle,Vadym Doroshenko,Arun Ganesh,Weiwei Kong,Alex Kurakin,Brendan McMahan,Mikhail Pravilov*

Main category: cs.LG

TL;DR: JAX-Privacy 是一个简化差分隐私机器学习部署的库，注重可用性、灵活性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私机器学习工具难以兼顾研究人员的定制需求与实践者的便捷使用。

Method: 提供经过验证的模块化组件，涵盖批量选择、梯度裁剪、噪声添加、会计和审计等关键环节。

Result: 整合了近期差分隐私机器学习研究，支持深度定制与开箱即用的场景。

Conclusion: JAX-Privacy 为差分隐私机器学习提供了高效、灵活且易用的统一框架。

Abstract: JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

</details>


### [82] [Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865)
*Andrzej Podobiński,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 使用基于Transformer的GAN（TTS-GAN）生成合成金融时间序列数据，显著提升LSTM预测模型的准确性


<details>
  <summary>Details</summary>
Motivation: 金融时间序列数据稀缺且动态性强，导致深度学习模型训练不足、泛化能力差

Method: 采用TTS-GAN生成合成数据，结合LSTM进行预测，并提出结合DTW和改进DeD-iMs的时间序列质量评估指标

Result: 在比特币和标普500数据上，使用合成数据增强的模型预测精度显著优于仅用真实数据的模型

Conclusion: GAN数据增强能有效提升金融时间序列预测性能，所提评估指标可可靠监控生成数据质量

Abstract: Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

</details>


### [83] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: ADAPT是一种结合束搜索初始化和自适应梯度引导变异的新方法，用于在LLM激活空间中可视化特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征可视化方法在LLM中因文本离散性和局部极小值问题而效果不佳，亟需更适配的方法。

Method: ADAPT结合束搜索初始化与自适应梯度引导变异，专门针对LLM文本离散性与优化困境设计。

Result: 在Gemma 2 2B的稀疏自编码器潜变量上，ADAPT在各层和潜变量类型上均优于先前方法，且提出了基于数据集激活统计的评估指标。

Conclusion: LLM的特征可视化是可行的，但必须采用针对文本离散性专门设计的优化策略。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [84] [MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies](https://arxiv.org/abs/2602.17868)
*Vasilii Feofanov,Songkang Wen,Jianfeng Zhang,Lujia Pan,Ievgen Redko*

Main category: cs.LG

TL;DR: 提出Mantis+和MantisV2，显著提升时间序列的零样本特征提取性能，达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型在冻结与微调编码器间存在性能差距，亟需提升零样本特征提取能力。

Method: 引入Mantis+（基于合成数据预训练）、MantisV2（架构优化）、增强测试时方法（中间层表示与输出标记聚合）、自集成与跨模型嵌入融合。

Result: 在UCR、UEA、HAR和EEG数据集上，MantisV2和Mantis+均超越此前模型，实现零样本SOTA性能。

Conclusion: 合成数据预训练与架构优化可有效提升时间序列基础模型的零样本泛化能力。

Abstract: Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.

</details>


### [85] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 机器学习模型基于术前数据预测鼻窦手术受益，准确率超过专家医生，助力个性化诊疗。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对前瞻性临床试验数据的机器学习应用，而CRS手术决策因风险与收益不确定性而复杂，亟需辅助工具。

Method: 使用SNOT-22作为主要结局指标，基于前瞻性收集的术前数据训练多种监督学习模型，包括集成方法，预测手术受益。

Result: 最佳模型准确率达85%，在30例独立测试集上达80%，超越专家医生平均75.6%的准确率。

Conclusion: 机器学习可有效辅助CRS手术决策，提升个性化医疗水平，具有临床转化潜力。

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [86] [COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models](https://arxiv.org/abs/2602.17893)
*Jiajun Shen,Yufei Jin,Yi He,xingquan Zhu*

Main category: cs.LG

TL;DR: COMBA提出一种基于状态空间模型的大图学习方法，通过图上下文门控和跨批次聚合实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 传统状态空间模型（SSMs）擅长处理序列数据，但难以直接应用于大规模图结构，因图转序列代价高且难以有效学习。

Method: COMBA引入图上下文门控以自适应控制邻居聚合，并通过跨批次聚合在采样子图间传递信息，提升可扩展性与准确性。

Result: 理论证明跨批次聚合可降低误差，实验在基准数据集上显著优于基线方法。

Conclusion: COMBA为大规模图学习提供了一种高效、可扩展的SSM框架，具有良好的性能与实用潜力。

Abstract: State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.

</details>


### [87] [Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898)
*Jingquan Yan,Yuwei Miao,Peiran Yu,Junzhou Huang*

Main category: cs.LG

TL;DR: 本文揭示了注意力回归模型中PCC plateau现象的理论根源，并提出ECA方法突破该限制，在不牺牲MSE的前提下显著提升PCC。


<details>
  <summary>Details</summary>
Motivation: PCC在训练早期停滞，而MSE持续下降，这一现象缺乏理论解释，限制了模型在相关性任务中的性能。

Method: 理论分析揭示MSE与PCC梯度冲突及凸聚合器的PCC上限，提出 Extrapolative Correlation Attention (ECA) 以突破凸包限制。

Result: ECA在多种基准（包括同质数据）上成功打破PCC plateau，显著提升相关性，同时保持MSE性能。

Conclusion: PCC plateau源于优化冲突与模型容量限制，ECA通过理论驱动设计实现超越凸包的PCC优化。

Abstract: Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

</details>


### [88] [Distribution-Free Sequential Prediction with Abstentions](https://arxiv.org/abs/2602.17918)
*Jialin Yu,Moïse Blanchard*

Main category: cs.LG

TL;DR: 在分布未知的半对抗环境中，提出AbstainBoost算法，实现VC类的次线性误差学习，同时平衡误分类与错误弃权的多项式权衡。


<details>
  <summary>Details</summary>
Motivation: 现有工作假设已知清洁样本分布，但实际中分布未知，需在不依赖先验分布的情况下实现类似学习保障。

Method: 提出基于弱学习器提升的AbstainBoost算法，支持无分布知识下的弃权学习，适用于无知与自适应对抗者。

Result: 算法在一般VC类上实现次线性误分类误差，并对线性分类器等结构化类也有效，同时给出误分类与错误弃权的多项式下界。

Conclusion: 该工作填补了分布未知下半对抗学习的理论空白，揭示了弃权机制在对抗环境中的关键作用与误差-弃权权衡。

Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $μ$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $μ$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.

</details>


### [89] [MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance](https://arxiv.org/abs/2602.17930)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: MIRA通过记忆图整合LLM先验，减少在线LLM调用，提升稀疏奖励下RL的样本效率


<details>
  <summary>Details</summary>
Motivation: 传统RL在稀疏奖励下样本效率低，依赖LLM提供先验但成本高且不可靠

Method: 构建动态记忆图，融合高回报经验与LLM输出，通过效用信号软调整优势估计，逐步衰减LLM影响

Result: MIRA在稀疏奖励任务中表现优于基线，接近高频LLM监督方法，但大幅减少LLM调用次数

Conclusion: 记忆图机制有效 amortize LLM监督成本，实现高效且可收敛的强化学习

Abstract: Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>


### [90] [Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2602.17931)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 利用记忆图结合LLM引导与代理自身成功轨迹，提升稀疏奖励环境下RL的样本效率，减少对LLM的频繁依赖


<details>
  <summary>Details</summary>
Motivation: 稀疏或延迟奖励导致RL样本复杂度高，LLM虽能辅助子目标发现与轨迹引导，但频繁调用影响可扩展性与可靠性

Method: 构建融合LLM引导与代理成功轨迹的记忆图，推导评估轨迹相似性的效用函数，并将其融入优势函数以增强价值评估，无需修改奖励函数

Result: 在基准环境中显著提升样本效率与早期学习速度，最终回报与高频LLM交互方法相当

Conclusion: 该方法通过离线记忆图实现高效引导，有效降低对在线LLM的依赖，兼顾性能与实用性

Abstract: In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.

</details>


### [91] [Causal Neighbourhood Learning for Invariant Graph Representations](https://arxiv.org/abs/2602.17934)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong*

Main category: cs.LG

TL;DR: 提出CNL-GNN框架，通过因果干预提升图神经网络在噪声图数据上的泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖虚假相关性，难以应对分布偏移，亟需因果建模增强鲁棒性

Method: 通过反事实邻域生成和自适应边扰动，结合可学习掩码与注意力机制，实现结构级因果干预与特征解耦

Result: 在四个公开数据集（含多域变体）上超越SOTA模型，显著提升泛化与鲁棒性

Conclusion: CNL-GNN通过因果建模有效抑制虚假关联，学习不变节点表示，推动图学习从特征导向转向因果导向

Abstract: Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.

</details>


### [92] [The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning](https://arxiv.org/abs/2602.18428)
*Mojtaba Sahraee-Ardakan,Mauricio Delbracio,Peyman Milanfar*

Main category: cs.LG

TL;DR: 本文提出边际能量概念，证明无噪声感知生成模型实质是其上的黎曼梯度流，并揭示速度参数化比噪声预测更稳定。


<details>
  <summary>Details</summary>
Motivation: 解决无噪声感知生成模型在数据流形附近梯度发散的稳定性悖论，理解其优化本质。

Method: 引入边际能量 $E_{	ext{marg}}(oldsymbol{u}) = -/log p(oldsymbol{u})$，通过相对能量分解和黎曼几何分析，证明模型隐含自适应度量抵消奇异性，并分析参数化方式的稳定性。

Result: 证明无噪声模型是边际能量上的稳定梯度流；发现噪声预测存在Jensen间隙导致不稳定，而速度参数化具有有界增益，天然稳定。

Conclusion: 无噪声生成模型的稳定性依赖于参数化方式，速度建模优于噪声预测，为设计鲁棒生成模型提供理论基础。

Abstract: Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.

</details>


### [93] [Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere](https://arxiv.org/abs/2602.17940)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: 本文在超球面输入域上，为高斯过程 bandit 问题中的平方指数核给出了紧的累积和简单后悔下界，并改进了最大信息增益的上界，证明了现有最优算法的接近最优性。


<details>
  <summary>Details</summary>
Motivation: 解决高斯过程 bandit 问题中关于平方指数核的维度相关对数因子在上下界之间的开放差距问题。

Method: 在超球面域上分析算法无关的最坏情况下界，推导累积后悔和简单后悔的下界，并改进最大信息增益的上界。

Result: 证明累积后悔下界为 $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$，简单后悔下界为 $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$，并给出 $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ 的最大信息增益上界。

Conclusion: 结果在超球面域上实现了算法的最优性，仅差维度无关的对数因子，显著缩小了理论差距。

Abstract: We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$ time steps to find an $ε$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.

</details>


### [94] [Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders](https://arxiv.org/abs/2602.17941)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong,Xin Wang*

Main category: cs.LG

TL;DR: 提出CCAGNN，一种融合因果推理的图神经网络，提升在分布变化下的鲁棒性与预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖相关性，易受虚假模式和分布偏移影响，而因果学习能识别真实因果关系并处理混杂因子，提升模型鲁棒性。

Method: 提出CCAGNN框架，通过因果推理建模，结合反事实推理，主动识别并调整混杂因子。

Result: 在六个跨领域公开数据集上，CCAGNN持续超越现有SOTA模型。

Conclusion: CCAGNN有效将因果机制引入图学习，显著提升模型在真实场景中的稳定性和可靠性。

Abstract: Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.

</details>


### [95] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 本文通过偏差-方差分解分析超梯度估计误差，提出一种集成超梯度策略以降低方差，提升超参数优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的超参数优化方法忽略数据分布引起的方差误差，导致性能下降和验证集过拟合。

Method: 对超梯度估计误差进行偏差-方差分解，理论分析方差项，并提出集成超梯度策略以降低方差。

Result: 在正则化学习、数据超清洗和小样本学习任务中，所提方法显著提升超梯度估计精度，解释了经验现象。

Conclusion: 方差是超梯度估计的关键因素，本文理论与方法为理解与改进超参数优化提供了新视角。

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [96] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: 通过插入常值像素打破对称性提升准确率，但导致对抗脆弱性，揭示准确率与鲁棒性权衡的几何根源。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习中准确率与对抗鲁棒性权衡的几何机制。

Method: 使用对称性破缺维数扩展（SBDE）插入常值像素，结合测试时掩码投影分析脆弱性来源。

Result: SBDE提升CIFAR-10准确率（90.47%→95.63%），但降低鲁棒性；掩码投影可恢复鲁棒性，证明脆弱性源于辅助维度的尖锐边界。

Conclusion: 准确率提升通过优化景观加深吸引盆，却在辅助维度形成陡峭梯度，导致对非流形扰动敏感，首次给出几何解释。

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [97] [Hardware-Friendly Input Expansion for Accelerating Function Approximation](https://arxiv.org/abs/2602.17952)
*Hu Lou,Yin-Jun Gao,Dong-Xiao Zhang,Tai-Jiao Du,Jun-Jie Zhang,Jia-Rui Zhang*

Main category: cs.LG

TL;DR: 通过输入空间扩展打破参数对称性，显著提升一维函数逼近的收敛速度与精度，且无需增加模型参数。


<details>
  <summary>Details</summary>
Motivation: 神经网络在一维函数逼近中因参数空间对称性导致损失景观平坦，收敛慢、泛化差，尤其对高频成分表现不佳。

Method: 提出输入空间扩展方法：将常数（如π）与原始输入拼接形成高维输入向量，打破参数对称性，不增加网络参数量。

Result: 在10个一维函数上实验，平均减少12% LBFGS迭代次数，最优5D扩展下MSE降低66.3%，π常数表现最佳。

Conclusion: 该方法低成本、硬件友好，为算法设计提供高效解决方案。

Abstract: One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $π$) to form a higher-dimensional vector (e.g., $[π, π, x, π, π]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $π$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.

</details>


### [98] [Bayesian Online Model Selection](https://arxiv.org/abs/2602.17958)
*Aida Afshar,Yuke Zhang,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出一种新的贝叶斯在线模型选择算法，在随机多臂bandit中实现与最优基学习器相当的性能，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯bandit中在线模型选择的探索难题：如何自适应地探索多个基学习器并竞争于 hindsight 中的最佳学习器。

Method: 引入一种新的贝叶斯算法，通过共享数据缓解先验误设，并分析其对 regret 的影响。

Result: 证明了 $Oig( d^* M oot{T} + oot{MT} ig)$ 的贝叶斯遗憾上界，并在多种设定中验证了算法性能。

Conclusion: 该算法在理论和实验上均表现优异，数据共享有助于提升对先验误设的鲁棒性。

Abstract: Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.

</details>


### [99] [Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts](https://arxiv.org/abs/2602.17962)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 通过整合MMD、CORAL和DANN三种域自适应方法，在无目标标签条件下显著提升髋部骨折风险预测模型在不同队列间的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型因数据分布差异（如医院、地区、人群）在跨队列部署时性能下降，尤其在髋部骨折预测中表现突出。

Method: 使用SOF、MrOS和UKB三个大型队列的共享临床与DXA特征，评估MMD、CORAL、DANN及其组合的域自适应方法，采用无目标标签的模型选择策略。

Result: 组合方法在男性源队列AUC达0.88，女性源队列达0.95，显著优于单方法与基线，且稳定性更高。

Conclusion: 多方法融合的无监督域自适应可有效降低数据分布差异影响，提升模型在真实部署场景中的泛化能力。

Abstract: Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.

</details>


### [100] [Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation](https://arxiv.org/abs/2602.17972)
*Sebastian Felipe R. Bundoc,Paula Joy B. Martinez,Sebastian C. Ibañez,Erika Fille T. Legara*

Main category: cs.LG

TL;DR: 通过计算模型揭示补贴政策无法解决学校过度拥挤，地理邻近性比学费影响更大，容量限制是关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家学校过度拥挤影响教育公平与学习成果，现有补贴政策因数据碎片化而效果不佳，亟需数据驱动的分析方法。

Method: 构建基于负二项回归的随机引力模型，整合近3000所学校的异构政府数据，模拟学生流动与政策场景，采用双重约束空间分配机制。

Result: 地理邻近性对择校的影响是学费的四倍，学位容量而非补贴金额是主要约束；补贴政策单独无法缓解系统性拥挤。

Conclusion: 计算建模可帮助政策制定者识别结构性约束，实现有限资源下的公平、数据驱动决策，超越单纯补贴的局限性。

Abstract: School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.

</details>


### [101] [Generating adversarial inputs for a graph neural network model of AC power flow](https://arxiv.org/abs/2602.17975)
*Robert Parker*

Main category: cs.LG

TL;DR: 本文通过优化生成对抗样本，揭示神经网络在AC潮流预测中的高误差风险，并呼吁加强验证与鲁棒训练方法。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为AC潮流的代理模型可能存在严重预测误差，亟需严谨的验证与鲁棒训练方法。

Method: 构建优化问题生成对抗输入点，使神经网络预测值与真实AC潮流方程解之间误差最大化，并在最小扰动约束下检验攻击可行性。

Result: 在14节点系统上，生成的对抗点使无功功率误差达3.4 p.u.、电压幅值误差达0.08 p.u.，且仅需单节点电压扰动0.04 p.u.即可满足对抗条件。

Conclusion: 神经网络代理模型存在显著安全风险，必须发展形式化验证和鲁棒训练机制以保障其可靠性。

Abstract: This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$Δ$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

</details>


### [102] [In-Context Learning for Pure Exploration in Continuous Spaces](https://arxiv.org/abs/2602.17976)
*Alessio Russo,Yin-Ching Lee,Ryan Welch,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出C-ICPE-TS算法，通过深度神经网络元训练实现连续空间中的主动序列测试，无需参数更新即可适应新任务。


<details>
  <summary>Details</summary>
Motivation: 传统纯探索问题多针对离散假设空间，而现代应用（如连续臂bandit、函数极小值估计）需要处理连续空间，现有方法难以直接迁移。

Method: 引入C-ICPE-TS算法，使用深度神经网络元学习观察历史到查询动作与假设预测的映射，实现数据驱动的自适应序列测试。

Result: 在连续最佳臂识别、区域定位和函数极小值估计等多个基准上验证了算法有效性，无需参数更新即可在新任务上表现优异。

Conclusion: 该方法为连续空间中的主动序列测试提供了一种通用、高效且可迁移的解决方案，无需手工设计信息模型。

Abstract: In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $ε$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.

</details>


### [103] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 该论文提出基于工具变量和条件矩约束的离线强化学习算法，解决隐藏混杂因子问题，并扩展至模仿学习和时序逻辑目标学习，显著提升样本效率与理论保证。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖大量在线交互，在高风险场景中不可行；离线学习受隐藏混杂因子影响，导致错误策略。亟需无需在线交互且能处理混杂因子的鲁棒学习方法。

Method: 1) 利用工具变量解决隐藏混杂因子，构建基于双/去偏机器学习的条件矩约束（CMR）算法；2) 放宽模仿学习中混杂假设，适配CMR估计器；3) 提出基于线性时序逻辑（LTL）的最优学习算法。

Result: 所提算法在理论上有收敛与最优性保证，实验上在RL基准与合成数据中超越现有方法，显著提升样本效率与策略性能。

Conclusion: 该工作为离线强化学习提供理论严谨、实用高效的解决方案，推动其在高风险决策场景中的落地应用。

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [104] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 本文提出三种基于数学理论的机器学习方法，分别用于监督学习、迁移学习和主动学习分类，提升模型性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在函数逼近、跨域迁移和分类效率上存在理论缺陷，亟需数学基础更强的解决方案。

Method: 1. 提出新方法改进监督学习的函数逼近；2. 研究部分数据下函数在域间的迁移机制与光滑性关系；3. 将信号分离技术引入主动学习分类，提出新算法。

Result: 新方法在保持高精度的同时显著提升计算速度，理论框架统一了信号分离与分类任务。

Conclusion: 数学理论驱动的方法能有效解决机器学习中的核心问题，为未来研究提供新方向。

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [105] [Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers](https://arxiv.org/abs/2602.17993)
*Mohan Tang,Sidi Lu*

Main category: cs.LG

TL;DR: 引入TurboConn架构，通过跨层残差连接突破Transformer计算深度限制，显著提升模型在数学推理任务上的表现，且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: Transformer的推理能力受限于固定计算深度，难以处理需要多步推理的复杂问题。

Method: 提出TurboConn，将高层隐藏状态密集连接到后续令牌的低层，增强跨步信息传递。

Result: 在GSM8K、Parity等任务上准确率提升0.9%至10%以上，Qwen-3-1.7B在Parity上从53.78%提升至100%。

Conclusion: 计算路径深度是推理能力的关键因素，TurboConn提供了一种高效提升LLMs推理能力的新机制，且不影响生成延迟。

Abstract: Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms "sparse" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.

</details>


### [106] [PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting](https://arxiv.org/abs/2602.17998)
*Shubham Bhardwaj,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: PHAST是一种基于端口哈密顿框架的模型，仅使用位置观测数据，实现稳定长期预测与物理参数恢复。


<details>
  <summary>Details</summary>
Motivation: 真实物理系统具有耗散特性，仅从部分观测（如位置）学习其动力学并实现稳定预测和参数识别是科学机器学习的核心挑战。

Method: PHAST采用端口哈密顿结构，将哈密顿量分解为势能、质量与阻尼项，结合低秩PSD/SPD参数化与Strang分裂法，在KNOWN、PARTIAL、UNKNOWN三种知识 regimes下建模。

Result: 在13个跨机械、电气、分子等系统的q-only基准上，PHAST在长期预测性能上优于基线方法，并在有足够锚点时可恢复物理参数。

Conclusion: 无锚点时参数识别存在规范自由度问题，需分离评估预测稳定性与可识别性，PHAST为此提供了结构化解决方案。

Abstract: Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.

</details>


### [107] [Asynchronous Heavy-Tailed Optimization](https://arxiv.org/abs/2602.18002)
*Junfei Sun,Dixi Yao,Xuchen Gong,Tahseen Rabbani,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: 该文提出两种处理异步优化中重尾梯度噪声的通信方案，通过延迟感知学习率和延迟补偿提升性能，理论收敛性优于现有方法且更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 重尾梯度噪声在Transformer中常见，现有研究多集中于同步设置，异步优化中的交互机制未被充分探索。

Method: 提出延迟感知学习率调度和延迟补偿机制，理论分析其在重尾噪声下的收敛性。

Result: 收敛速率与同步方法相当，延迟容忍度更高，实验上在图像和语言任务中优于现有同步/异步方法。

Conclusion: 所提方法有效提升异步优化在重尾噪声下的稳定性与效率，具有更好超参数鲁棒性。

Abstract: Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.

</details>


### [108] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出NIMM评估框架和NIMMgen生成框架，提升LLM生成机制模型在真实场景下的可靠性与代码正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成机制模型的研究过于简化真实条件，缺乏在部分观测和多样任务下的可靠性评估。

Method: 引入NIMM评估框架，提出NIMMgen代理框架，通过迭代优化提升模型代码正确性与实际有效性。

Result: 在三个科学领域数据集上验证了NIMMgen的高性能，并证明其生成模型支持反事实干预模拟。

Conclusion: NIMMgen显著提升了LLM生成机制模型在真实场景中的实用性与可信度。

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [109] [Flow Actor-Critic for Offline Reinforcement Learning](https://arxiv.org/abs/2602.18015)
*Jongseong Chae,Jongeui Park,Yongjae Shin,Gyeongmin Kim,Seungyul Han,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出基于流模型的Actor-Critic方法，提升离线强化学习在多模态数据上的性能


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的数据分布复杂且多模态，传统高斯策略表达能力不足

Method: 提出Flow Actor-Critic，使用流模型作为Actor，并利用流模型构建保守型Critic正则化项，防止Q值外推爆炸

Result: 在D4RL和OGBench等基准上达到新的最优性能

Conclusion: 联合利用流模型于Actor与Critic设计，能有效处理多模态数据并提升离线RL稳定性与表现力

Abstract: The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.

</details>


### [110] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出梯度正则化（GR）替代KL惩罚，提升奖励模型准确性，减少RLHF中的奖励欺骗问题


<details>
  <summary>Details</summary>
Motivation: 现有方法通过KL惩罚限制策略更新，但无法有效避免奖励欺骗，需更优方法提升奖励模型在策略训练中的准确性

Method: 提出梯度正则化（GR），通过引导策略更新至奖励函数更准确的平坦区域，并用有限差分高效估计梯度

Result: GR在多种LM强化学习任务中优于KL惩罚，提升GPT评分胜率，避免格式过度聚焦与奖励黑客行为

Conclusion: 梯度正则化是更有效的奖励鲁棒性方法，理论与实验均表明其能提升奖励模型准确性并抑制策略欺骗

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [111] [Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework](https://arxiv.org/abs/2602.18055)
*Jingyang Qiao,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yanyun Qu,Yuan Xie*

Main category: cs.LG

TL;DR: 提出Continual-NExT框架和MAGE方法，提升双模态大模型的持续学习能力，缓解遗忘并增强跨模态知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有双模态大模型在持续学习中面临灾难性遗忘、幻觉、指令不遵循和跨模态知识迁移失败等问题，缺乏标准化持续学习框架。

Method: 提出Continual-NExT框架与MAGE方法，通过混合与聚合通用LoRA和专家LoRA实现高效知识迁移与遗忘缓解。

Result: 实验表明MAGE在持续学习任务中优于现有方法，达到最优性能。

Conclusion: Continual-NExT和MAGE为双模态大模型的持续学习提供了有效解决方案，推动其在动态场景中的适应能力。

Abstract: Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.

</details>


### [112] [Deepmechanics](https://arxiv.org/abs/2602.18060)
*Abhay Shinde,Aryan Amit Barsainyan,Jose Siguenza,Ankita Vaishnobi Bisoi,Rakshit Kr. Singh,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 物理信息深度学习模型在动力系统学习中表现优异，但在保守与耗散系统中稳定性差，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息模型缺乏对多样物理现象的系统基准测试，尤其是未检验完整轨迹的稳定性。

Method: 使用DeepChem框架，对HNN、LNN和SRNN三种模型在六个动力系统（保守与非保守）上进行轨迹误差定量与定性评估。

Result: 所有模型在混沌或非保守系统中均难以保持稳定性，预测误差较大。

Conclusion: 当前物理信息深度学习模型尚无法稳健学习经典力学系统，亟需改进以提升泛化与稳定性。

Abstract: Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.

</details>


### [113] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: 通过可控的对称性调制，在图生成模型中平衡等变性与训练效率，仅用19%的训练轮数即达到更好性能。


<details>
  <summary>Details</summary>
Motivation: 严格等变性虽保障图的置换对称性，但增加计算成本并减缓收敛，需探索其与训练效率的权衡。

Method: 基于正弦位置编码和节点置换，提出可控对称性调制方案，逐步放松等变约束。

Result: 对称性破坏加速早期训练，但易导致过拟合；合理调制可延缓过拟合并显著提升收敛速度，仅用19%训练轮数超越基线。

Conclusion: 适度放松等变性可显著提升图生成模型训练效率，无需牺牲最终性能。

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [114] [TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs](https://arxiv.org/abs/2602.18109)
*Rong Fu,Yibo Meng,Guangzhen Yao,Jiaxuan Lu,Zeyu Zhang,Zhaolu Kang,Ziming Guo,Jia Yee Tan,Xiaojing Du,Simon James Fong*

Main category: cs.LG

TL;DR: TempoNet 是一种基于强化学习的实时调度器，使用 Transformer 和深度 Q 学习，在多核系统中显著提升Deadline 满足率并实现亚毫秒级推理。


<details>
  <summary>Details</summary>
Motivation: 传统分析型调度器在复杂多核混合关键性场景中难以兼顾调度效率与稳定性，亟需一种能高效处理无序任务集、具备强泛化能力的智能调度方法。

Method: 提出 TempoNet，结合排列不变 Transformer、紧急 Tokenizer 离散化时间余量、稀疏注意力机制与可微匹配层，实现高效任务映射与 Q 值估计，并引入行为克隆预训练与 Actor-Critic 扩展。

Result: 在工业级混合关键性轨迹和大规模多核场景中，TempoNet 在截止期限满足率、优化稳定性、样本效率和推理速度上均优于传统分析方法和神经基线，支持硬件在环与微基准验证。

Conclusion: TempoNet 建立了基于 Transformer 的高吞吐实时调度实用框架，为智能调度系统提供了可扩展、高效且稳定的解决方案。

Abstract: Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.

</details>


### [115] [Non-Stationary Online Resource Allocation: Learning from a Single Sample](https://arxiv.org/abs/2602.18114)
*Yiding Feng,Jiashuo Jiang,Yige Wang*

Main category: cs.LG

TL;DR: 提出一种基于分位数的元策略，在非平稳需求下仅用每期一个历史样本即可实现多资源在线分配，对不同类型样本分别达到亚线性或对数级后悔界。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量历史数据或受限于平稳性假设，难以应对真实场景中非平稳、数据稀缺的在线资源分配问题。

Method: 设计一种类型依赖的分位数元策略，分解为奖励分布估计、流体松弛优化和动态阈值决策三模块，分别针对奖励可观测与仅类型可观测样本设计静态、部分自适应与完全自适应策略。

Result: 奖励可观测样本下实现$	ilde{O}(oot T)$后悔；仅类型样本下，在最小到达概率假设下首次获得$O((/log T)^3)$对数级后悔界。

Conclusion: 所提框架在极小历史数据、任意非平稳性、多资源约束下显著优于现有方法，为实际应用提供高效解决方案。

Abstract: We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.

</details>


### [116] [Cut Less, Fold More: Model Compression through the Lens of Projection Geometry](https://arxiv.org/abs/2602.18116)
*Olga Saukh,Dong Wang,Haris Šikić,Yun Cheng,Lothar Thiele*

Main category: cs.LG

TL;DR: 无需重训练的神经网络压缩方法中，模型折叠比剪枝在参数重构误差和功能扰动上更优，且在大规模实验中通常实现更高准确率。


<details>
  <summary>Details</summary>
Motivation: 大规模部署需无需重训练的压缩方法，现有剪枝方法存在局限，需更优几何视角的替代方案。

Method: 将结构化剪枝和模型折叠统一为正交投影算子，通过秩距离分析其重构误差与功能扰动，并在超千个模型检查点上进行大规模实验验证。

Result: 模型折叠在中高压缩率下显著优于剪枝，准确率更高；在特定训练设置下差距缩小或反转。

Conclusion: 模型折叠是一种几何感知、无需校准的压缩方法，理论更严谨、实践表现更优，可作为剪枝的有力替代。

Abstract: Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.

</details>


### [117] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出FINO方法，通过注入噪声和熵引导采样提升离线到在线强化学习的样本效率


<details>
  <summary>Details</summary>
Motivation: 生成模型在离线RL中表现良好，但在在线微调中仍沿用离线预训练方式，缺乏有效探索机制

Method: 采用流匹配策略，注入噪声以增强探索，并结合熵引导采样平衡探索与利用

Result: 在多种具有挑战性的任务中，FINO在有限在线样本预算下 consistently 超越基线方法

Conclusion: FINO通过噪声注入与熵引导有效解决了离线到在线RL的探索难题，显著提升样本效率

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [118] [Learning Long-Range Dependencies with Temporal Predictive Coding](https://arxiv.org/abs/2602.18131)
*Tom Potter,Oliver Rhodes*

Main category: cs.LG

TL;DR: 提出一种结合时序预测编码与近似RTRL的新方法，在保持PC局部并行特性的同时，逼近BPTT在长时依赖任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统BPTT能耗高且非局部，而预测编码（PC）虽节能但难以有效应用于循环神经网络（RNN）的长时依赖任务。

Method: 将时序预测编码（tPC）与近似实时递归学习（RTRL）结合，实现时空信用分配。

Result: 在合成数据和真实任务（如1500万参数机器翻译）上，性能接近BPTT，测试困惑度为7.62（BPTT为7.49）。

Conclusion: 该方法在保留PC节能、局部、并行优势的同时，有效学习复杂时序依赖，为高效学习系统提供新路径。

Abstract: Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.

</details>


### [119] [Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks](https://arxiv.org/abs/2602.18141)
*Pierre-Gabriel Berlureau,Ali Hariri,Victor Kawasaki-Borruat,Mia Zosso,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 提出一种可学习的Bakry-Emery图拉普拉斯算子，无需修改图结构即可实现自适应信息传播，构建mu-ChebNet模型，在长程任务中性能优越且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法因信息过平滑和过压缩难以处理长距离传播，而图变换或重布线方法计算开销大或需改变图结构。

Method: 引入可学习节点势能的Bakry-Emery图拉普拉斯算子，作为标准拉普拉斯的即插即用替代，并结合Chebyshev滤波器构建mu-ChebNet模型，联合学习势能与滤波器。

Result: 理论证明其能调控谱特性，实验在合成与真实数据上均表现优异，并提供信息流的可解释路由场。

Conclusion: Bakry-Emery拉普拉斯是高效、可解释、自适应谱图学习的理论基础。

Abstract: Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.

</details>


### [120] [Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks](https://arxiv.org/abs/2602.18146)
*Lionel Salesses,Larbi Arbaoui,Tariq Benamara,Arnaud Francois,Caroline Sainvitu*

Main category: cs.LG

TL;DR: 提出一种基于多尺度图神经网络的深度学习框架，实现复杂几何上长时间步温度场的稳定预测，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中对复杂几何上时空场的长期预测存在挑战，高保真模拟计算成本高，现有机器学习方法在温度和梯度长期预测上表现不佳。

Method: 采用双耦合模型的时序多尺度架构，结合潜变量递归图神经网络与变分图自编码器，在网格上直接预测温度历史，降低内存占用并提升训练稳定性。

Result: 在粉末床熔融模拟数据上实现了跨多样几何结构的高精度、长时间步稳定预测，性能优于基线方法。

Conclusion: 该框架通用性强，可扩展至三维几何与具有多尺度动态的物理系统。

Abstract: Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.

</details>


### [121] [Unifying Formal Explanations: A Complexity-Theoretic Perspective](https://arxiv.org/abs/2602.18160)
*Shahaf Bassan,Xuanxiang Huang,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出统一概率值函数框架，揭示全局解释可多项式时间计算，局部解释则为NP难问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究对充分理由和对比理由的分析分散于不同框架，缺乏统一理论，导致复杂性理解不清晰。

Method: 通过统一概率值函数建模两类解释，并分析其单调性、子模性和超模性三个组合优化性质。

Result: 全局值函数具备这三种性质，支持多项式时间算法；局部值函数则无，导致NP难。

Conclusion: 全局解释在多种模型中可高效计算，而局部解释计算困难，为可解释性方法设计提供理论依据。

Abstract: Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.

</details>


### [122] [A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction](https://arxiv.org/abs/2602.18168)
*Danning Jing,Xinhai Chen,Xifeng Pu,Jie Hu,Chao Huang,Xuguang Chen,Qinglin Wang,Jie Liu*

Main category: cs.LG

TL;DR: 提出RGD-Blast模型，实现高速度、高精度的长期爆炸波预测，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法计算成本高，现有机器学习模型在复杂城市布局和分布外场景下精度下降，且自回归预测存在误差累积问题。

Method: 引入多尺度模块捕捉全局与局部特征，结合动态-静态特征耦合机制，融合时变压力场与静态源及布局信息。

Result: 相比传统方法速度提升两个数量级，RMSE低于0.01，R2超过0.89，支持280步预测，在未见布局与爆炸参数下仍保持高泛化性。

Conclusion: RGD-Blast显著提升了爆炸波长期预测的精度与泛化能力，推动了该领域的研究进展。

Abstract: Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.

</details>


### [123] [SeedFlood: A Step Toward Scalable Decentralized Training of LLMs](https://arxiv.org/abs/2602.18181)
*Jihun Kim,Namhoon Lee*

Main category: cs.LG

TL;DR: SeedFlood通过零阶更新的种子可重构结构实现近乎零大小的消息广播，突破传统 gossip 方法的通信瓶颈，支持超大规模模型的去中心化训练。


<details>
  <summary>Details</summary>
Motivation: 传统 gossip 方法通信开销随模型规模增长且信息在多跳网络中衰减，导致全局共识效率低下，限制了大模型去中心化训练的可扩展性。

Method: SeedFlood利用零阶更新的种子可重构特性，将通信消息压缩至接近零大小，通过全网广播实现高效全局共识，通信开销与模型规模无关。

Result: 在去中心化 LLM 微调任务中，SeedFlood 在泛化性能和通信效率上均优于 gossip 基线，且在大规模设置下可媲美一阶方法。

Conclusion: SeedFlood 消除了去中心化训练的主要可扩展性瓶颈，使百亿参数模型在数百客户端上的分布式训练成为可能。

Abstract: This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.

</details>


### [124] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 提出首个AI倾向性测量框架，采用双逻辑模型替代传统IRT，揭示倾向性与能力结合能更准确预测AI行为。


<details>
  <summary>Details</summary>
Motivation: 传统IRT仅关注能力，无法刻画AI倾向性（过多或过少均有害），需新方法评估倾向性对性能与安全的影响。

Method: 引入双逻辑成功函数，定义倾向性理想区间，并用LLM生成的任务无关评分标准估计区间边界，测试六类LLM的倾向性偏移。

Result: 可量化倾向性偏移量及其任务影响，倾向性预测跨任务泛化性强，联合倾向性与能力比单独使用效果更优。

Conclusion: 该框架首次实现AI倾向性的正式测量，证明其在预测AI行为上优于纯能力评估，为安全与性能评估提供新范式。

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [125] [LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification](https://arxiv.org/abs/2602.18195)
*Hairong Chen,Yicheng Feng,Ziyu Jia,Samir Bhatt,Hengguan Huang*

Main category: cs.LG

TL;DR: 提出LERD模型，通过贝叶斯神经动力学系统从多通道EEG中无监督推断潜在神经事件及其关系，提升阿尔茨海默病的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖黑箱分类器，未建模EEG信号的潜在动力学机制，限制了临床解释性与诊断可靠性。

Method: LERD结合连续时间事件推断模块与随机事件生成过程，引入电生理学启发的动力学先验，实现端到端无监督学习。

Result: 在合成数据和两个AD真实EEG数据集上，LERD显著优于基线模型，生成符合生理学的潜在表征，揭示群体间动力学差异。

Conclusion: LERD为阿尔茨海默病提供可解释、稳定且精准的EEG动力学分析框架，具备临床筛查与监测潜力。

Abstract: Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.

</details>


### [126] [RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference](https://arxiv.org/abs/2602.18196)
*Xiuying Wei,Caglar Gulcehre*

Main category: cs.LG

TL;DR: RAT+通过密集预训练和主动递归学习，在推理时灵活切换至稀疏注意力，显著缓解了稀疏化导致的精度下降问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏化预训练注意力模型在 dilation 稀疏后精度严重下降，亟需一种无需重新训练即可保持高性能的稀疏注意力方案。

Method: RAT+ 在密集注意力基础上引入全序列递归与主动递归学习，预训练一次后可动态切换为 dilated attention、局部窗口或混合架构，仅需短时微调即可部署。

Result: 在 1.5B 参数下，RAT+ 在 D=16 时逼近密集模型精度，D=64 时仅下降 2-3 点，且优于 top-k 块注意力；2.6B 规模下趋势一致。

Conclusion: RAT+ 实现了高效稀疏推理与高精度的平衡，为大模型推理优化提供了一种可扩展、低成本的解决方案。

Abstract: Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.

</details>


### [127] [Generative Model via Quantile Assignment](https://arxiv.org/abs/2602.18216)
*Georgi Hrusanov,Oliver Y. Chén,Julien S. Bodelet*

Main category: cs.LG

TL;DR: NeuroSQL是一种无需辅助网络的新型生成模型，通过最优传输的潜变量求解实现高效稳定的图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型（如VAE、GAN）依赖编码器或判别器，导致训练不稳定、计算开销大和模式崩溃等问题。

Method: NeuroSQL通过求解线性分配问题隐式学习低维潜变量，无需编码器，仅用独立生成器进行数据生成。

Result: 在MNIST、CelebA、AFHQ、OASIS四个数据集上，NeuroSQL在图像质量、训练速度和小样本生成性能上均优于VAE、GAN和扩散模型。

Conclusion: NeuroSQL通过分位数分配机制实现了快速、稳定、低信息损失的生成，为数据稀缺场景提供了高效解决方案。

Abstract: Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.

</details>


### [128] [Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction](https://arxiv.org/abs/2602.18227)
*Redwanul Karim,Changhun Kim,Timon Conrad,Nora Gourmelon,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 提出一种基于LoRA和选择性解冻预测头的参数高效迁移方法，在高压电网中实现近似全微调的精度，同时减少85.46%可训练参数。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息图神经网络在跨电压域迁移时依赖全微调，成本高且难以平衡稳定性与可塑性。

Method: 在自注意力机制中应用LoRA低秩更新，并选择性解冻预测头，结合基尔霍夫物理损失约束模型行为。

Result: 在多个电网拓扑上，RMSE误差仅为2.6e-4，可训练参数减少85.46%，物理残差接近全微调，但源域保留率下降4.7个百分点。

Conclusion: LoRA+PHead方法在保持物理一致性前提下，实现了高效、可控的跨域AC潮流预测。

Abstract: Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.

</details>


### [129] [[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games](https://arxiv.org/abs/2602.18230)
*Jorge Carrasco Pollo,Ioannis Kapetangeorgis,Joshua Rosenthal,John Hua Yao*

Main category: cs.LG

TL;DR: 该研究复现并扩展了Abdelnabi等人的谈判基准，发现模型比较存在模糊性，评估客观性存疑，且实验设计存在信息泄露和消融研究不充分等问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM谈判评估基准缺乏鲁棒性和泛化性，需验证其可复现性与适用性。

Method: 复现原始实验于更多模型，引入新指标评估谈判质量与评估公平性，扩展基准并分析模型行为。

Result: 基准虽复杂，但模型比较结果模糊；实验存在信息泄露和消融不彻底问题，评估客观性受质疑。

Conclusion: 模型比较评估高度依赖上下文，需更严谨的基准设计与评估方法。

Abstract: Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.

</details>


### [130] [Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver](https://arxiv.org/abs/2602.18248)
*Pietro Sittoni,Emanuele Zangrando,Angelo A. Casulli,Nicola Guglielmi,Francesco Tudisco*

Main category: cs.LG

TL;DR: 提出Neural-HSS架构，基于HSS矩阵结构，显著提升PDE求解器在低数据量下的训练效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在求解PDE时依赖大量训练数据，数据生成与模型训练成本高昂，限制了实际应用。

Method: 设计Neural-HSS架构，利用HSS矩阵的结构特性，理论证明其在低数据场景下的精确性，并与傅里叶算子、卷积层建立关联。

Result: 在200万网格点的三维泊松方程上验证了卓越的低数据学习能力，超越基线方法，并成功扩展至电磁、流体、生物等多领域PDE。

Conclusion: Neural-HSS是高效、数据驱动的PDE求解新范式，为资源受限场景下的科学计算提供可行方案。

Abstract: Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.

</details>


### [131] [Variational Distributional Neuron](https://arxiv.org/abs/2602.18250)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出一种变分分布神经元，将神经元从确定性标量转变为分布，通过局部ELBO和KL正则化实现分布激活，增强不确定性建模与信息持久性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络中不确定性仅由全局参数建模，而计算单元仍为确定性标量，与不确定性应为计算内在属性的理念矛盾。

Method: 将神经元设计为VAE模块，参数化后验分布，传播重参数化样本，并通过局部ELBO的KL项正则化，实现分布激活和约束下的连续空间收缩。

Result: 实现了神经元的分布化表示，可本地调控信息量与时间持久性，并通过自回归先验扩展时序建模能力。

Conclusion: 分布化神经元解决了确定性单元与内在不确定性之间的结构张力，为概率计算提供了可解释、可控的细粒度新范式。

Abstract: We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This "contraction" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze "collapse" modes and the conditions for a "living neuron", then extend the contribution over time via autoregressive priors over the latent, per unit.

</details>


### [132] [MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data](https://arxiv.org/abs/2602.18253)
*Xabier de Zuazo,Vincenzo Verbeni,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.LG

TL;DR: 首次在MEG语音模型中实现迁移学习和跨任务解码，仅用5分钟数据即可显著提升感知与产生任务的解码性能。


<details>
  <summary>Details</summary>
Motivation: 语音脑机接口面临数据效率低的挑战，需在少量数据下实现高效解码。

Method: 基于Conformer的模型在50小时听觉数据上预训练，再在每位被试5分钟数据上微调，实现感知与产生任务的迁移与跨任务解码。

Result: 迁移学习带来1-4%的同任务提升，跨任务提升达5-6%，且生产任务模型能超越随机水平解码听觉任务。

Conclusion: 学习到的神经表征反映了感知与产生的共享机制，而非仅限于运动活动。

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

</details>


### [133] [A Probabilistic Framework for LLM-Based Model Discovery](https://arxiv.org/abs/2602.18266)
*Stefan Wahl,Raphaela Schenk,Ali Farnoud,Jakob H. Macke,Daniel Gedon*

Main category: cs.LG

TL;DR: 将模型发现视为概率推断，提出ModelSMC算法，利用LLM和序列蒙特卡洛采样自动发现可解释的机制模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的模型发现方法依赖手工启发式流程，缺乏概率形式化，限制了系统性和可解释性。

Method: 将模型发现重构为概率推断问题，提出ModelSMC算法，使用LLM生成和优化候选模型（粒子），并通过似然加权进行迭代采样。

Result: 在真实科学系统中，ModelSMC发现了具有可解释机制的模型，并提升了后验预测检查性能。

Conclusion: 该概率视角为LLM驱动的模型发现提供了统一框架，推动了更系统、可解释的自动化科学发现方法的发展。

Abstract: Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.

</details>


### [134] [PRISM: Parallel Reward Integration with Symmetry for MORL](https://arxiv.org/abs/2602.18277)
*Finn van der Knaap,Kejiang Qian,Zheng Xu,Fengxiang He*

Main category: cs.LG

TL;DR: PRISM算法通过引入对称性偏差解决异构多目标强化学习中奖励频率不均的问题，显著提升采样效率和Pareto覆盖。


<details>
  <summary>Details</summary>
Motivation: 异构多目标强化学习中，密集短期目标主导学习过程，导致稀疏长期奖励难以获得有效信用分配，从而降低样本效率。

Method: 提出PRISM算法，包含ReSymNet模型（通过残差块学习缩放机会值以加速探索）和SymReg正则化项（强制策略搜索空间具有反射对称性，降低假设复杂度）。

Result: 在MuJoCo基准上，PRISM相比稀疏奖励基线提升超100%超体积，优于全密集奖励oracle达32%，显著改善Pareto覆盖与分布平衡。

Conclusion: 对称性作为归纳偏差能有效协调多目标时间频率差异，提升强化学习的样本效率与泛化能力。

Abstract: This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.

</details>


### [135] [Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers](https://arxiv.org/abs/2602.18292)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 将解码视为受正则化约束的优化问题，统一多种解码策略并提出新方法BoK，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前解码方法多为启发式调参，缺乏理论框架，难以系统化设计与优化。

Method: 将解码建模为在概率单纯形上的正则化优化问题，统一贪婪解码、采样、Top-K、Top-P、Sparsemax等方法，并基于KL约束设计新解码器BoK。

Result: BoK在多样本任务中大幅提升性能，如Qwen2.5-Math-7B在MATH500上准确率提升18.6%。

Conclusion: 解码应被视作可推导的优化层，而非启发式工具，新框架支持无经验依赖的解码器设计。

Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.

</details>


### [136] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 通过信息论分析证明CoT监控的必要条件，并提出两种训练方法提升监控准确性，防止CoT退化与奖励黑客行为。


<details>
  <summary>Details</summary>
Motivation: 现有CoT监控方法因信息提取不足和函数近似误差而性能受限，亟需系统性改进方法。

Method: 提出两种训练目标：(a) 基于oracle直接奖励最大化监控准确率；(b) 无标签方法最大化输出与CoT的条件互信息。

Result: 在多个环境中，两种方法均显著提升监控准确率，有效防止CoT退化和奖励黑客。

Conclusion: 信息互依赖是CoT可监控性的基础，通过针对性训练可系统提升监控性能并缓解奖励黑客问题。

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [137] [On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction](https://arxiv.org/abs/2602.18301)
*Ivan Bondarenko,Egor Palkin,Fedor Tikunov*

Main category: cs.LG

TL;DR: 研究冻结LLM通过两个原型标记单步重建文本的能力，探索其语义与句法编码机制，并提出关系蒸馏方法在不损失重建质量下传递语义关系。


<details>
  <summary>Details</summary>
Motivation: 突破自回归模型逐词生成的低效瓶颈，探索利用原型标记实现单步文本重建的可行性。

Method: 通过实验解耦原型标记的语义与句法信息，分析e-token的稳定性，可视化注意力模式，并引入锚点损失和关系蒸馏正则化方法约束语义结构。

Result: m-token更擅长捕捉语义信息；锚点约束显著降低重建精度；关系蒸馏能有效传递批量语义关系且不损失重建质量。

Conclusion: 原型标记可作为非自回归seq2seq系统的中间表示，关系蒸馏为实现高效文本生成提供了可行路径。

Abstract: Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.

</details>


### [138] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: JPmHC通过流形约束的可训练线性混合器替代恒等跳跃连接，提升超连接的稳定性与效率


<details>
  <summary>Details</summary>
Motivation: 现有超连接破坏恒等映射，导致训练不稳定、可扩展性差和内存开销大

Method: 提出JPmHC，使用流形约束（如双随机、Stiefel、Grassmann）的可训练线性混合器，并结合自由概率分析、隐式微分和Cayley变换

Result: 在ARC-AGI上实现更快收敛、更高准确率和更低计算成本，优于双随机基线

Conclusion: JPmHC是谱感知、稳定且高效的深度学习框架，为拓扑架构设计和基础模型演进提供新视角

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [139] [On the "Induction Bias" in Sequence Models](https://arxiv.org/abs/2602.18333)
*M. Reza Ebrahimi,Michaël Defferrard,Sunny Panchal,Roland Memisevic*

Main category: cs.LG

TL;DR: Transformer在状态追踪任务中数据效率低，且跨序列长度无法共享权重，而RNN则具备有效权重共享能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在实践中表现优异，但其在状态追踪任务中的内在局限性，尤其是在同分布场景下的数据效率问题，尚未被充分研究。

Method: 通过大规模实验比较Transformer与RNN在多种监督机制下的数据效率，分析其在不同序列长度下的权重共享特性。

Result: Transformer所需训练数据随状态空间和序列长度增长更快，且跨长度权重共享几乎不存在；RNN则能有效共享权重，提升跨长度泛化能力。

Conclusion: 状态追踪仍是Transformer的根本挑战，即使在同分布设置下，其缺乏有效的泛化机制，而RNN更具数据效率和结构优势。

Abstract: Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.

</details>


### [140] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: 本研究通过全局与局部可解释性方法分析AutoClustering中元模型的决策依据，提升无监督学习自动化系统的透明度。


<details>
  <summary>Details</summary>
Motivation: 现有AutoClustering方法虽性能强，但因元特征对算法与超参选择的影响不透明，导致可靠性低、偏差难诊断、元特征工程效率低。

Method: 1) 系统梳理22种方法的元特征并构建分类法；2) 使用决策谓词图（DPG）进行全局特征重要性分析；3) 采用SHAP进行局部决策解释。

Result: 发现元特征相关性的一致模式，揭示当前元学习策略的结构缺陷，并为可解释AutoML设计提供可操作指导。

Conclusion: 本研究为提升无监督学习自动化中的决策透明性提供了实用基础。

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


### [141] [FedZMG: Efficient Client-Side Optimization in Federated Learning](https://arxiv.org/abs/2602.18384)
*Fotios Zantalis,Evangelos Zervas,Grigorios Koulouras*

Main category: cs.LG

TL;DR: FedZMG是一种无参数、客户端侧的优化算法，通过将梯度投影到零均值超平面上缓解非IID数据导致的客户端漂移，无需额外通信或超参调优，显著提升收敛速度和模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据非IID导致客户端漂移，现有自适应优化器虽能缓解但增加计算和通信开销，不适合资源受限的IoT环境。

Method: FedZMG通过梯度中心化技术，将本地梯度投影到零均值超平面，结构化正则化优化空间，无需额外通信或超参数。

Result: 理论证明FedZMG降低梯度方差并提供更紧的收敛界；在EMNIST、CIFAR100和Shakespeare数据集上，其收敛速度和最终精度优于FedAvg和FedAdam，尤其在强非IID场景下。

Conclusion: FedZMG是一种高效、轻量的联邦学习优化方法，有效解决非IID数据带来的客户端漂移问题，适合资源受限的边缘环境。

Abstract: Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.

</details>


### [142] [PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing](https://arxiv.org/abs/2602.18396)
*Ehsan Lari,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: PRISM-FCP 是一种抗拜占庭攻击的联邦共形预测框架，通过部分模型共享和统计边缘校准实现端到端鲁棒性，兼顾通信效率与预测区间精度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦共形预测方法仅在校准阶段应对对抗行为，训练阶段模型仍易受毒化更新影响，缺乏端到端鲁棒性。

Method: 训练阶段客户端每次仅传输 D 个参数中的 M 个，降低攻击者扰动能量；校准阶段将非一致性分数转化为特征向量，基于距离计算恶意分数并过滤可疑客户端，再估计共形分位数。

Result: 在合成数据和 UCI 超导数据集上，PRISM-FCP 在拜占庭攻击下保持名义覆盖保证，避免预测区间膨胀，且通信开销更低。

Conclusion: PRISM-FCP 提供了一种鲁棒、通信高效且可保证覆盖性能的联邦不确定性量化方法。

Abstract: We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.

</details>


### [143] [Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay](https://arxiv.org/abs/2602.18401)
*Josue Casco-Rodriguez,Nanda H. Krishna,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 通过引入隐藏状态动量和自适应机制，改进了噪声RNN中的重播机制，实现了时间压缩与探索平衡。


<details>
  <summary>Details</summary>
Motivation: 现有噪声RNN重播模型虽基于Langevin采样，但无法充分解释时间压缩和探索效率问题。

Method: 提出隐藏状态动量机制，结合负反馈自适应，连接欠阻尼Langevin采样，并在2D路径和高维海马细胞数据上验证。

Result: 动量机制显著加速重播，自适应维持探索性，二者协同解决速度与探索的权衡问题。

Conclusion: 隐藏状态动量是实现高效时间压缩重播的关键，为神经重播的计算建模提供了新框架。

Abstract: Biological neural networks (like the hippocampus) can internally generate "replay" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.

</details>


### [144] [Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study](https://arxiv.org/abs/2602.18403)
*Orfeas Bourchas,George Papalambrou*

Main category: cs.LG

TL;DR: 提出一种融合物理定律与数据驱动残差学习的混合模型，显著提升主机功率预测的泛化能力与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法无法遵守螺旋桨定律，导致外推性能差，影响船舶能效与排放合规。

Method: 构建基线模型 $P = cV^n$ 捕捉主功率-速度关系，再用XGBoost、神经网络或PINN学习环境与操作引起的残差。

Result: 混合模型在稀疏数据区域显著优于纯数据驱动模型，稠密区域性能相当，且保证物理一致性。

Conclusion: 该框架为船舶性能监控、航路优化与节能规划提供了高效实用的工具。

Abstract: Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.

</details>


### [145] [Unifying approach to uniform expressivity of graph neural networks](https://arxiv.org/abs/2602.18409)
*Huan Luo,Jonni Virtema*

Main category: cs.LG

TL;DR: 提出模板图神经网络（T-GNNs）框架，统一分析GNN表达能力，并与新逻辑GML(T)建立等价关系。


<details>
  <summary>Details</summary>
Motivation: 标准GNN受限于局部聚合与全局读出，表达能力不足，需引入子结构信息提升性能。

Method: 引入T-GNNs，通过模板嵌入聚合更新节点特征，并构建对应逻辑GML(T)及模板双模拟与推广的WL算法。

Result: 证明T-GNNs与GML(T)表达能力等价，并说明现有GNN变体均为T-GNNs的特例。

Conclusion: T-GNNs为分析和设计高表达力GNN提供了统一理论框架。

Abstract: The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.

</details>


### [146] [Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures](https://arxiv.org/abs/2602.18417)
*Joshua Nunley*

Main category: cs.LG

TL;DR: 本文提出一种用于U(d)闭子群上隐状态序列模型的直接框架，通过共享骨架推导RNN与Transformer模板，并在O(d)上验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统序列模型的状态空间设计缺乏统一的理论基础，本文旨在通过群论框架统一RNN与Transformer的结构设计。

Method: 基于最小公理体系，将子群选择作为状态空间、切线投影和更新映射的可替换组件，推导通用模板，并在O(d)子群上实现正交状态模型。

Result: 在Tiny Shakespeare和Penn Treebank上，正交状态RNN和Transformer在参数匹配下表现优异，且切线空间线性混合扩展进一步提升性能。

Conclusion: 群论框架为序列模型提供了统一且可扩展的设计范式，正交状态模型在有限参数下具有竞争力。

Abstract: This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.

</details>


### [147] [Assigning Confidence: K-partition Ensembles](https://arxiv.org/abs/2602.18435)
*Aggelos Semoglou,John Pavlopoulos*

Main category: cs.LG

TL;DR: CAKE提出了一种基于聚类集成的点级置信度评估方法，通过稳定性与几何一致性量化聚类分配的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法（如k-means）无法评估单个样本分配的可靠性，全局诊断指标不足以反映点级不稳定性，影响结果的准确性与鲁棒性。

Method: CAKE利用K分区集成计算每个点的两个统计量：分配稳定性与局部几何拟合一致性，并合成[0,1]区间内的置信得分。

Result: 理论上证明CAKE对噪声鲁棒，实验表明其能有效识别模糊点与稳定核心点，提升聚类质量。

Conclusion: CAKE为聚类分配提供了可解释的点级置信度评分，可用于过滤或优先级排序，增强聚类结果的可信度与实用性。

Abstract: Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [148] [Nested Training for Mutual Adaptation in Human-AI Teaming](https://arxiv.org/abs/2602.17737)
*Upasana Biswas,Durgesh Kalwar,Subbarao Kambhampati,Sarath Sreedharan*

Main category: cs.RO

TL;DR: 通过I-POMDP建模人类适应行为，提出嵌套训练框架，使AI代理在不产生隐式协同的情况下更好适应人类伙伴


<details>
  <summary>Details</summary>
Motivation: 现有方法使用静态训练伙伴无法捕捉人类的自适应行为，而双方同时学习又导致过度协同、泛化差

Method: 将人机协作建模为I-POMDP，采用嵌套训练机制，每一层代理训练时对抗下一层的自适应代理，避免双方同时学习

Result: 在Overcooked任务中，该方法在与未见过的自适应伙伴协作时，表现优于基线方法，任务绩效和适应性显著提升

Conclusion: 显式建模人类适应行为并分离训练过程，能有效提升AI代理在人机协作中的泛化能力与适应性

Abstract: Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.

</details>


### [149] [Reinforcement-Learning-Based Assistance Reduces Squat Effort with a Modular Hip--Knee Exoskeleton](https://arxiv.org/abs/2602.17794)
*Neethan Ratnakumar,Mariya Huzaifa Tohfafarosh,Saanya Jauhri,Xianlian Zhou*

Main category: cs.RO

TL;DR: 基于强化学习的神经网络控制器可有效降低深蹲时的生理负荷，但导致 squat 深度减小。


<details>
  <summary>Details</summary>
Motivation: 减少重复性低水平装配活动中深蹲的体力需求，提升工作效率与人体工效。

Method: 使用强化学习训练神经网络控制器，在物理仿真环境中生成个性化的髋膝关节助力扭矩，并在五名健康成人身上验证三种条件（无外骨骼、零扭矩、主动助力）下的效果。

Result: 主动助力使代谢率降低约10%，心率略有下降，但髋膝屈曲角度减小，深蹲深度降低。

Conclusion: 该控制器能有效降低深蹲生理负担，未来需优化硬件与控制策略以平衡助力效果与运动幅度。

Abstract: Squatting is one of the most demanding lower-limb movements, requiring substantial muscular effort and coordination. Reducing the physical demands of this task through intelligent and personalized assistance has significant implications, particularly in industries involving repetitive low-level assembly activities. In this study, we evaluated the effectiveness of a neural network controller for a modular Hip-Knee exoskeleton designed to assist squatting tasks. The neural network controller was trained via reinforcement learning (RL) in a physics-based, human-exoskeleton interaction simulation environment. The controller generated real-time hip and knee assistance torques based on recent joint-angle and velocity histories. Five healthy adults performed three-minute metronome-guided squats under three conditions: (1) no exoskeleton (No-Exo), (2) exoskeleton with Zero-Torque, and (3) exoskeleton with active assistance (Assistance). Physiological effort was assessed using indirect calorimetry and heart rate monitoring, alongside concurrent kinematic data collection. Results show that the RL-based controller adapts to individuals by producing torque profiles tailored to each subject's kinematics and timing. Compared with the Zero-Torque and No-Exo condition, active assistance reduced the net metabolic rate by approximately 10%, with minor reductions observed in heart rate. However, assisted trials also exhibited reduced squat depth, reflected by smaller hip and knee flexion. These preliminary findings suggest that the proposed controller can effectively lower physiological effort during repetitive squatting, motivating further improvements in both hardware design and control strategies.

</details>


### [150] [Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array](https://arxiv.org/abs/2602.17818)
*Zachary Turcotte,François Grondin*

Main category: cs.RO

TL;DR: 通过动态调整麦克风阵列几何结构提升语音增强性能，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业噪声环境下语音增强性能下降，限制语音控制技术部署，现有方法依赖数字信号处理或深度学习，缺乏物理层优化。

Method: 使用七自由度机械臂搭载16麦克风阵列，分四组，其中一组靠近末端执行器；结合声源定位、计算机视觉、逆运动学、MVDR波束形成和DNN时频掩码，动态调整阵列几何以靠近目标说话人。

Result: 在多种信噪比条件下，相比传统配置，获得更高的尺度不变信噪比和更低的词错误率。

Conclusion: 物理层阵列重构显著提升语音增强效果，为工业噪声环境提供新范式。

Abstract: Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.

</details>


### [151] [Evolution of Safety Requirements in Industrial Robotics: Comparative Analysis of ISO 10218-1/2 (2011 vs. 2025) and Integration of ISO/TS 15066](https://arxiv.org/abs/2602.17822)
*Daniel Hartmann,Kristýna Hamříková,Aleš Vysocký,Vendula Laciok,Aleš Bernatík*

Main category: cs.RO

TL;DR: ISO 10218:2025 更新了工业机器人安全标准，整合机械、功能与网络安全要求，取代2011版。


<details>
  <summary>Details</summary>
Motivation: 随着协作机器人兴起和网络化系统普及，原有安全标准无法应对网络安全与未授权访问风险，亟需更新。

Method: 对比分析ISO 10218:2011与ISO 10218:2025的结构、术语、技术要求及附录，重点考察功能安全与网络安全的演进。

Result: 新标准扩展了功能安全与网络安全要求，引入机器人与协作应用新分类，并规范整合ISO/TS 15066。

Conclusion: ISO 10218:2025构建了涵盖机械、功能与数字安全的综合框架，适用于现代机器人系统设计与运行。

Abstract: Industrial robotics has established itself as an integral component of large-scale manufacturing enterprises. Simultaneously, collaborative robotics is gaining prominence, introducing novel paradigms of human-machine interaction. These advancements have necessitated a comprehensive revision of safety standards, specifically incorporating requirements for cybersecurity and protection against unauthorized access in networked robotic systems. This article presents a comparative analysis of the ISO 10218:2011 and ISO 10218:2025 standards, examining the evolution of their structure, terminology, technical requirements, and annexes. The analysis reveals significant expansions in functional safety and cybersecurity, the introduction of new classifications for robots and collaborative applications, and the normative integration of the technical specification ISO/TS 15066. Consequently, the new edition synthesizes mechanical, functional, and digital safety requirements, establishing a comprehensive framework for the design and operation of modern robotic systems.

</details>


### [152] [WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection](https://arxiv.org/abs/2602.17908)
*Mingzhang Zhu,Alvin Zhu,Jose Victor S. H. Ramos,Beom Jun Kim,Yike Shi,Yufeng Wu,Ruochen Hou,Quanyou Wang,Eric Song,Tony Fan,Yuchen Cui,Dennis W. Hong*

Main category: cs.RO

TL;DR: 提出可穿戴手部外骨骼WHED，实现野外高保真人手演示采集，解决多指操作数据获取难题。


<details>
  <summary>Details</summary>
Motivation: 由于手部遮挡、复杂动力学和接触交互，难以收集自然的高保真人手演示数据，制约了灵巧操作的可扩展学习。

Method: 设计WHED系统，采用可穿戴优先原则与容错拇指耦合机制，结合联动指部接口、被动适配手部、本体感知传感与车载模块，构建端到端数据流水线，同步关节编码器、AR末端位姿与腕部视觉观测。

Result: 成功采集涵盖精细捏取与全手握持的代表性操作序列，演示与重放结果在行为上具有一致性。

Conclusion: WHED系统有效实现了野外自然人手演示的高保真采集，为灵巧操作学习提供了可靠数据基础。

Abstract: Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.

</details>


### [153] [Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation](https://arxiv.org/abs/2602.17921)
*Kei Ikemura,Yifei Dong,Florian T. Pokorny*

Main category: cs.RO

TL;DR: 提出首个联合优化夹爪形态与控制策略的协同设计框架，用于柔性脆弱物体操作，显著提升操作性能与真实部署能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅孤立优化夹爪设计或控制策略，难以应对柔性脆弱物体的复杂接触动力学与完整性要求。

Method: 1) 引入隐式微分同胚形状参数化以高效优化夹爪几何；2) 构建应力感知的双层协同设计管道；3) 提出特权到点云策略蒸馏实现零样本真实部署。

Result: 在果冻抓取、推动和鱼片舀取等任务中，仿真与实验证明方法显著优于现有方法。

Conclusion: 联合优化形态与控制是提升柔性物体操作性能的关键，所提框架具备强实用性和真实场景部署潜力。

Abstract: Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.

</details>


### [154] [Homotopic information gain for sparse active target tracking](https://arxiv.org/abs/2602.17926)
*Jennifer Wakulicz,Ki Myung Brian Lee,Teresa Vidal-Calleja,Robert Fitch*

Main category: cs.RO

TL;DR: 提出基于同伦类信息增益的主动目标跟踪规划方法，比传统度量信息方法更高效准确。


<details>
  <summary>Details</summary>
Motivation: 传统信息增益在多模态运动模型中定义模糊，难以有效规划传感轨迹。

Method: 引入同伦类信息增益，以最大化目标高层运动信息，替代传统度量信息增益。

Result: 在真实与仿真行人数据上验证，该方法用更少测量次数实现更高精度的轨迹估计。

Conclusion: 同伦类信息增益是度量信息增益的下界，且分布稀疏，适合高效轨迹规划。

Abstract: The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.

</details>


### [155] [Quasi-Periodic Gaussian Process Predictive Iterative Learning Control](https://arxiv.org/abs/2602.18014)
*Unnati Nigam,Radhendushka Srivastava,Faezeh Marzbanrad,Michael Burke*

Main category: cs.RO

TL;DR: 使用准周期高斯过程提升迭代学习控制的收敛速度与鲁棒性，同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 机器人重复任务中性能因环境变化和磨损而下降，传统ILC依赖历史误差，难以应对时变扰动。

Method: 引入准周期高斯过程（QPGP）构建预测型ILC框架，采用结构化方程实现O(p³)复杂度的高效推断与参数估计。

Result: 在车辆轨迹跟踪、机械臂和真实机器人实验中，方法收敛更快、抗扰更强，且计算成本更低。

Conclusion: QPGP-ILC在重复动态系统中兼具高效性与实用性，适合长期运行的机器人任务。

Abstract: Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\mathcal{O}(p^3)$ instead of $\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.

</details>


### [156] [EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots](https://arxiv.org/abs/2602.18071)
*Boyuan An,Zhexiong Wang,Yipeng Wang,Jiaqi Li,Sihang Li,Jing Zhang,Chen Feng*

Main category: cs.RO

TL;DR: EgoPush是一种仅用单目自视角摄像头实现多物体非抓取重排的强化学习框架，通过物体中心潜在空间和教师-学生蒸馏实现高效重排，成功实现零样本仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 人类能利用自视角感知在杂乱环境中重排物体而不依赖全局坐标，现有方法在动态场景中因全局状态估计失败而受限，因此需要一种无需全局姿态估计的感知驱动重排方法。

Method: 提出EgoPush框架，构建物体中心的潜在空间编码相对空间关系，使用特权RL教师从稀疏关键点学习状态与动作，并通过视觉限制教师观察模拟学生视角，再蒸馏为纯视觉学生策略；采用时间衰减的阶段奖励分解长周期任务。

Result: 在仿真中显著优于端到端RL基线，成功率达到提升；通过消融实验验证各设计有效性，并实现零样本仿真到现实的迁移。

Conclusion: EgoPush通过感知驱动与教师-学生蒸馏，实现了无需全局建模的高效长周期重排，为移动机器人在真实复杂环境中的自主重排提供了可行路径。

Abstract: Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.

</details>


### [157] [Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning](https://arxiv.org/abs/2602.18097)
*Aarati Andrea Noronha,Jean Oh*

Main category: cs.RO

TL;DR: 提出一种结合汉密尔顿-雅可比可达性分析与深度Q学习的框架，使自动驾驶车辆在保证安全的前提下高效与自行车骑行者交互。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要在确保安全的同时，实现与骑行者高效、自然的交互，现有方法难以兼顾安全保证与最优导航。

Method: 结合汉密尔顿-雅可比-贝尔曼不等式计算安全价值函数，并将其作为结构化奖励引入深度Q学习，同时建模骑行者的潜在行为响应。

Result: 通过仿真验证了该方法在安全性与效率上的优势，并优于现有最先进方法和人类驾驶行为。

Conclusion: 该框架成功实现了安全与最优性的协同优化，为自动驾驶与弱势道路使用者交互提供了新范式。

Abstract: In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.

</details>


### [158] [GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation](https://arxiv.org/abs/2602.18164)
*Jonas Frey,Turcan Tuna,Frank Fu,Katharine Patterson,Tianao Xu,Maurice Fallon,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: 发布GrandTour数据集，首个大规模多模态腿足机器人公开数据集，支持SLAM与状态估计研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法覆盖腿足机器人在复杂真实环境中的状态估计与感知需求。

Method: 使用ANYmal-D四足机器人搭载多模态传感器（LiDAR、RGB相机、本体传感器、双目深度相机），在多样户外与室内场景中采集数据，并结合RTK-GNSS和全站仪提供高精度真值轨迹。

Result: 构建了目前最大规模的开源腿足机器人数据集，涵盖多种环境、天气与光照条件，提供同步多模态数据与高精度真值。

Conclusion: GrandTour数据集显著推动腿足机器人多模态感知与融合算法的开发与评估。

Abstract: Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.

</details>


### [159] [Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark](https://arxiv.org/abs/2602.18174)
*Hyoseok Ju,Bokeon Suh,Giseop Kim*

Main category: cs.RO

TL;DR: 提出首个用于评估单目视觉SLAM尺度一致性的ScaleMaster数据集，揭示现有系统在大规模室内环境中存在严重尺度漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有单目视觉SLAM基准局限于小规模或结构简单环境，未能解决多楼层、长轨迹、重复纹理等场景下的尺度不一致问题。

Method: 构建ScaleMaster数据集，包含多楼层、长轨迹、低纹理等挑战性场景，并引入基于Chamfer距离的地图对地图评估方法，结合传统轨迹指标进行定量与定性分析。

Result: 现有先进单目SLAM系统在传统基准上表现良好，但在大规模真实环境中出现严重尺度错误，地图质量显著下降。

Conclusion: ScaleMaster数据集为未来开发尺度一致、可靠的视觉SLAM系统提供了基础，推动该领域向更鲁棒的方向发展。

Abstract: Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.

</details>


### [160] [Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator](https://arxiv.org/abs/2602.18212)
*Rui Chen,Domenico Chiaradia,Daniele Leonardis,Antonio Frisoli*

Main category: cs.RO

TL;DR: 设计了一种体积更小、响应更快的纺锤形角度执行器（SSAA），并据此开发出双自由度柔性肩外骨骼，显著降低肌肉活动。


<details>
  <summary>Details</summary>
Motivation: 现有便携式气动肩外骨骼在扭矩输出与动态响应之间存在权衡，且多执行器支持复杂肩部运动面临挑战。

Method: 提出体积优化的SSAA结构，结合曲面外展执行器（CAA）和水平内收执行器（HAA），构建390g轻量纺织外骨骼，实现多模式肩部辅助。

Result: SSAA体积减小35.7%，扭矩保持94.2%，响应快35.2%；外骨骼在 abduction 和 flexion 任务中最大降低EMG达59%和63.7%，CAA对flexion的额外增益有限。

Conclusion: 该设计为多自由度软外骨骼提供了高效执行器架构与实验依据，未来可优化针对康复人群的辅助策略。

Abstract: Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation.
  User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.

</details>


### [161] [SimVLA: A Simple VLA Baseline for Robotic Manipulation](https://arxiv.org/abs/2602.18224)
*Yuankai Luo,Woping Chen,Tong Liang,Baiqiao Wang,Zhenguo Li*

Main category: cs.RO

TL;DR: SimVLA是一个轻量级VLA基线模型，仅0.5B参数即可在仿真和真实机器人任务中达到SOTA性能，通过解耦感知与控制、标准化训练流程实现高效可复现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型架构复杂、训练方式多样，难以区分性能提升真正来源，亟需一个透明、可复现的基线模型。

Method: 采用标准视觉语言主干、轻量动作头，严格解耦感知与控制，并标准化训练动态，构建最小化但高效的VLA架构。

Result: SimVLA在无机器人预训练下，以0.5B参数超越数十亿参数模型，在仿真基准上达到SOTA，真实机器人性能媲美pi0.5。

Conclusion: SimVLA为VLA研究提供了可靠、可复现的基线，有助于未来创新的准确归因与评估。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA

</details>


### [162] [RoEL: Robust Event-based 3D Line Reconstruction](https://arxiv.org/abs/2602.18258)
*Gwangtak Bae,Jaeho Shin,Seunggu Kang,Junho Kim,Ayoung Kim,Young Min Kim*

Main category: cs.RO

TL;DR: 提出一种基于线结构的事件相机位姿优化与3D线图构建方法，显著提升事件数据映射与位姿精化性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据稀疏且噪声大，传统方法依赖额外传感器，线结构虽鲁棒但易受估计误差影响，亟需稳定提取与优化方法。

Method: 通过多时间片事件表示智能提取线轨迹，设计几何代价函数优化3D线图与相机位姿，消除投影畸变与深度歧义。

Result: 在多样数据集上显著提升事件映射与位姿精度，方法兼容点云与图像观测，适用于多模态场景。

Conclusion: 基于线结构的 formulation 具有鲁棒性与灵活性，适合事件感知模块的实际部署。

Abstract: Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/

</details>


### [163] [Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments](https://arxiv.org/abs/2602.18260)
*Magnus Norén,Marios-Nektarios Stamatopoulos,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出一种动态角色分配的领导者-跟随者编队框架，实现四足机器人在复杂环境中的灵活避障与稳定编队


<details>
  <summary>Details</summary>
Motivation: 传统方法采用固定领导者或刚性角色，难以适应动态复杂环境，缺乏灵活避障与角色切换能力

Method: 结合动态角色分配、部分目标规划、虚拟弹簧阻尼系统、自适应避障层和动态前瞻参考生成器，以FM2算法提供全局与局部路径规划

Result: 仿真与实物实验表明，系统能实现平滑协调、自适应角色切换和复杂环境中稳定的编队保持

Conclusion: 该框架显著提升四足机器人编队在非结构化环境中的灵活性与鲁棒性，具备实际应用潜力

Abstract: This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.

</details>


### [164] [Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty](https://arxiv.org/abs/2602.18312)
*Zhaoming Xie,Kevin Karol,Jessica Hodgins*

Main category: cs.RO

TL;DR: 提出线性策略网络（LPN）与动作雅可比惩罚相结合，有效生成平滑控制信号，无需任务特定调参，成功应用于仿真和物理机器人复杂动作学习。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习策略常产生非自然的高频控制信号，难以在真实机器人上实现，传统惩罚项需大量调参。

Method: 引入动作雅可比惩罚，通过自动微分直接惩罚状态变化引起的行为变化，并设计线性策略网络（LPN）降低计算开销。

Result: LPN联合雅可比惩罚在多种模仿任务中生成平滑动作，收敛更快，推理更高效，并成功迁移到物理四足机器人。

Conclusion: 该方法无需调参且计算高效，显著提升控制信号的自然性与可实现性，适用于高动态真实机器人任务。

Abstract: Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.

</details>


### [165] [Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams](https://arxiv.org/abs/2602.18330)
*Mohsen Jafarpour,Ayberk Yüksek,Shahab Eshghi,Stanislav Gorb,Edoardo Milana*

Main category: cs.RO

TL;DR: 利用螺旋状超材料梁的 snapping 行为，实现软体机器人中可调的往复与非往复运动，提升推进效率。


<details>
  <summary>Details</summary>
Motivation: 传统软体机器人驱动方式效率低，需探索基于非线性不稳定性（如 snapping）的高效运动生成方法。

Method: 设计螺旋形 tendon-driven 金属梁，使用 PLA 3D 打印，通过边界条件调控其非线性行为，并测试其在游泳机器人中的应用。

Result: 仅通过调整边界约束即可调控临界力与稳定性；实现非往复运动，推进速度达 81 mm/s（0.4 体长/秒）。

Conclusion: 几何设计驱动的 snapping 结构为软体机器人提供高效、可编程的驱动新范式。

Abstract: Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.

</details>


### [166] [Downwash-aware Configuration Optimization for Modular Aerial Systems](https://arxiv.org/abs/2602.18344)
*Mengguang Li,Heinz Koeppl*

Main category: cs.RO

TL;DR: 提出一种生成并优化同质模块化飞行器系统任务特定装配配置的框架，考虑模块间下洗效应。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦平面布局，常忽略气动干扰，缺乏对下洗效应的约束处理。

Method: 首先枚举大规模非同构连接拓扑，其次求解非线性规划以在执行器限制和下洗约束下选择最小控制输入的配置。

Result: 在物理仿真和真实实验中验证了框架的有效性。

Conclusion: 该框架能有效生成满足气动约束的最优装配配置，推动模块化飞行器系统实用化。

Abstract: This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.

</details>


### [167] [Zero-shot Interactive Perception](https://arxiv.org/abs/2602.18374)
*Venkatesh Sripada,Frank Guerin,Amir Ghalamzan*

Main category: cs.RO

TL;DR: ZS-IP通过结合多策略操作与记忆驱动的视觉语言模型，无需训练即可实现交互感知，显著提升推拉任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂部分可观测场景中的遮挡与语义模糊问题，传统被动感知方法效果有限。

Method: 提出ZS-IP框架，包含增强观测模块（引入pushlines）、记忆引导动作模块和机器人控制器，耦合推、拉、抓取策略与VLM推理。

Result: 在Franka Panda机械臂上实验表明，ZS-IP在推拉任务中优于MOKA等被动感知方法，且能保护非目标物体。

Conclusion: ZS-IP实现零样本交互感知，pushlines有效提升接触密集动作的语义理解，为复杂环境机器人操作提供新范式。

Abstract: Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.

</details>


### [168] [Ori-Sense: origami capacitive sensing for soft robotic applications](https://arxiv.org/abs/2602.18379)
*Hugo de Souza Oliveira,Xin Li,Mohsen Jafarpour,Edoardo Milana*

Main category: cs.RO

TL;DR: 提出一种基于Kresling折纸结构的柔性电容传感器Ori-Sense，可将扭转变形转化为电容变化，用于软体机器人本体感知。


<details>
  <summary>Details</summary>
Motivation: 软体机器人需要高灵敏度、低刚度的本体感知传感器，以实现精确的姿态反馈，现有传感器难以兼顾柔顺性与检测精度。

Method: 采用可溶性核心模塑法制造单片硅胶结构，嵌入导电TPU电极，形成集成软电容器，并通过有限元仿真与实验验证其力学与电学性能。

Result: 传感器在-15至15 mm轴向位移下扭矩低于0.01 N mm，30度扭转时达0.03 N mm；电容变化达30%，灵敏度最高达0.0067 pF/deg（5 mm轴向压缩时）。

Conclusion: Ori-Sense成功实现高灵敏、低刚度的扭转感知，为软体机器人提供可靠的本体反馈方案，具有良好的应用前景。

Abstract: This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.

</details>


### [169] [Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO](https://arxiv.org/abs/2602.18386)
*Mohamed Elgouhary,Amr S. El-Wakeel*

Main category: cs.RO

TL;DR: 使用强化学习动态调整纯追踪算法的前视距离和转向增益，显著提升自动驾驶赛车的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统纯追踪算法的参数（前视距离和转向增益）依赖固定或速度调度规则，难以适应不同赛道和速度曲线，导致性能不稳定。

Method: 采用PPO强化学习算法，基于速度和曲率特征在线联合优化前视距离Ld和转向增益g，训练于F1TENTH仿真环境并部署于ROS 2系统。

Result: 所提RL-PP方法在仿真与实车测试中均优于固定参数、速度调度和仅优化前视距离的变体，并超越运动学MPC轨迹跟踪器，在圈速、跟踪精度和转向平滑性上表现最佳。

Conclusion: 策略引导的参数调优可可靠提升经典几何控制方法的性能，实现无需地图重调参的泛化控制。

Abstract: Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.

</details>


### [170] [How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf](https://arxiv.org/abs/2602.18397)
*Wenqi Jiang,Jason Clemons,Karu Sankaralingam,Christos Kozyrakis*

Main category: cs.RO

TL;DR: 本文提出VLA-Perf性能模型，首次系统分析视觉-语言-动作模型在实时推理中的性能瓶颈，并给出15条设计指导原则。


<details>
  <summary>Details</summary>
Motivation: VLA模型在真实机器人部署中面临严格的实时推理约束，但其推理性能缺乏系统性研究。

Method: 提出VLA-Perf分析模型，评估模型扩展、架构选择、长视频输入、异步推理、双系统流水线及部署位置（设备/边缘/云）对延迟的影响。

Result: 通过全面评估提炼出15条关键结论，揭示模型与系统设计对实时推理性能的联合影响。

Conclusion: 未来VLA模型与推理系统的设计需协同优化模型架构与部署策略，以满足真实场景的实时性需求。

Abstract: Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.

</details>


### [171] [Snapping Actuators with Asymmetric and Sequenced Motion](https://arxiv.org/abs/2602.18421)
*Xin Li,Ye Jin,Mohsen Jafarpour,Hugo de Souza Oliveira,Edoardo Milana*

Main category: cs.RO

TL;DR: 通过几何诱导的不对称 snapping 机制，实现单压力输入驱动的四足软体机器人高效运动


<details>
  <summary>Details</summary>
Motivation: 传统软体机器人依赖复杂控制系统，亟需更简单、节能的驱动方式

Method: 设计偏心穹顶式 snapping 执行器，结合有限元仿真与实验验证，构建四执行器气动网络实现波浪式运动

Result: 单压力输入下机器人实现最大速度72.78 mm/s（7.5 Hz），运动具频率依赖性

Conclusion: 不对称 snapping 机制为无 tethered 软体机器人提供高效物理控制新范式

Abstract: Snapping instabilities in soft structures offer a powerful pathway to achieve rapid and energy-efficient actuation. In this study, an eccentric dome-shaped snapping actuator is developed to generate controllable asymmetric motion through geometry-induced instability. Finite element simulations and experiments reveal consistent asymmetric deformation and the corresponding pressure characteristics. By coupling four snapping actuators in a pneumatic network, a compact quadrupedal robot achieves coordinated wavelike locomotion using only a single pressure input. The robot exhibits frequency-dependent performance with a maximum speed of 72.78~mm/s at 7.5~Hz. These findings demonstrate the potential of asymmetric snapping mechanisms for physically controlled actuation and lay the groundwork for fully untethered and efficient soft robotic systems.

</details>
