<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.LG](#cs.LG) [Total: 125]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出PolypSeg-GradCAM模型，结合U-Net与Grad-CAM，实现高精度且可解释的息肉分割，提升结肠镜AI辅助诊断的可信度。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌早期筛查依赖息肉精准分割，但人工标注耗时且主观，现有深度学习模型缺乏可解释性，阻碍临床应用。

Method: 采用U-Net架构进行息肉分割，并集成Grad-CAM生成热力图以可视化模型决策依据，基于Kvasir-SEG数据集训练与评估。

Result: 测试集平均IoU达0.9257，Dice系数>0.96，Grad-CAM热力图证实模型聚焦临床相关区域，提升可解释性。

Conclusion: PolypSeg-GradCAM在保持高分割精度的同时增强模型透明性，推动AI辅助结肠镜向可信、临床可采纳方向发展。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE是一种基于深度学习的远程眼科应用，用于自动检测糖尿病视网膜病变，准确率达85.4%，支持实时筛查并提升偏远地区医疗可及性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是成人失明的主要原因，尤其在资源匮乏地区筛查困难，亟需高效低成本的AI解决方案。

Method: 采用ResNet-18、EfficientNet-B0和SqueezeNet等多种卷积神经网络，评估精度与计算效率的平衡，最终选定最优模型。

Result: 模型分类准确率达85.4%，支持实时筛查，集成云平台、数据安全与多用户框架。

Conclusion: PerceptronCARE展示了AI驱动的远程医疗在扩大糖尿病视网膜病变筛查覆盖面方面的巨大潜力，特别适用于偏远和资源受限地区。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出一种名为SIM的数据内在正则化框架，通过逆映射机制增强表征学习，在多个任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法依赖启发式设计，泛化性差，亟需一种更可靠、通用的正则化机制。

Method: 提出Self Identity Mapping (SIM)，利用输入-输出逆映射减少信息损失并优化梯度流动；实例化为ρSIM，通过片段采样与投影降低计算复杂度。

Result: 在图像分类、小样本提示学习、域泛化等任务中一致优于基线方法，并在语义分割、音频分类、时间序列异常检测等非视觉任务中表现优异，且与现有方法正交互补。

Conclusion: SIM是一种模型无关、任务无关的插件式正则化方法，能有效保留语义信息，提升多种任务的表征学习能力。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA是一种基于动量的自适应梯度反转方法，在单轮平均梯度场景下实现高保真多图像重建，无需辅助信息且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单轮平均梯度（SAG）场景下难以从批量梯度中分离单样本信息，导致重建效果差。

Method: MAGIA引入组合重缩放优化界和动量混合损失机制，通过随机子集探查潜藏图像信号，实现无标签推断的梯度反转。

Result: 在大批次场景下显著超越先进方法，实现高保真多图像重建，计算开销与标准求解器相当。

Conclusion: MAGIA为SAG场景下的梯度反转提供了高效、鲁棒且无需辅助信息的新框架。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer是一个专为阿拉伯语文档OCR设计的视觉语言模型，通过大规模数据和解码器微调策略，在新基准Misraj-DocOCR上取得了当前最优的WER=0.25成绩。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语手写体复杂、字体多样、带变音符号且为右至左书写，现有多模态大模型在该语言OCR上表现不佳。

Method: 基于预训练MLLM，使用合成与真实文档组成的大型数据集，采用解码器-only微调策略，在保持通用视觉特征的同时适配阿拉伯语OCR任务，并构建专家验证的Misraj-DocOCR评估基准。

Result: Baseer在Misraj-DocOCR基准上达到0.25的词错误率（WER），显著优于现有开源和商业方案，成为阿拉伯文档OCR新SOTA。

Conclusion: 针对形态丰富的语言如阿拉伯语，对通用MLLM进行领域适配可大幅提升OCR性能，为未来研究提供了坚实基线。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出一种CNN-LSTM深度学习框架，将稀疏InSAR数据转化为时空张量，实现高精度地表形变预测，超越传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 稀疏InSAR时间序列数据难以预测未来形变，传统方法缺乏对空间-时间复杂动态的建模能力。

Method: 将稀疏点观测转化为稠密时空张量，构建CNN-LSTM混合模型，联合学习空间模式与时间依赖。

Result: 在爱尔兰东部Sentinel-1数据上显著优于LightGBM和LASSO，预测更准确且空间连续，揭示传统模型依赖简单持久性模式。

Conclusion: 时空深度学习是高分辨率地表形变预测的有效新范式，具有广泛应用潜力。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: 提出Scrapbook框架，用于生成多维度数据集以评估AI模型对基础概念的理解，发现现有模型在位置和约束问题上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在复杂任务前缺乏对基础概念（如对象、位置、属性）的系统性评估方法。

Method: 构建Scrapbook框架，生成大量语言多样、覆盖基础概念的问题数据集。

Result: 模型在对象识别上表现良好，但在位置信息、几何形状和约束问题上表现差，存在答案不一致和肯定偏见。

Conclusion: Scrapbook框架为系统评估和提升AI模型的基础理解能力提供了有效工具。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 研究发现视觉-语言-视觉流程中存在严重的信息损失，99.3%的样本感知退化显著。


<details>
  <summary>Details</summary>
Motivation: 视觉内容通过文本中介时的信息损失尚未被量化，亟需评估多模态系统的局限性。

Method: 构建150组描述-生成图像对，使用LPIPS、SSIM和颜色距离度量感知、结构与色彩维度的保真度。

Result: 99.3%样本感知退化严重，91.5%出现显著结构信息损失，证实描述-生成瓶颈是系统性限制。

Conclusion: 当前多模态系统在文本中介环节存在不可忽视的信息损失，需优化跨模态表示机制。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 利用AI从卫星图像自动推断屋顶属性，提升小岛屿发展中国家的灾害风险评估能力


<details>
  <summary>Details</summary>
Motivation: 小岛屿发展中国家缺乏详细的建筑结构数据，影响灾害韧性规划与风险减缓

Method: 结合地理空间基础模型与浅层分类器，对比微调深度学习模型，并利用邻国数据增强训练

Result: 屋顶坡度和材料分类的F1分数分别达到0.88和0.83

Conclusion: 该方法可赋能SIDS利用AI与遥感数据实现更高效、基于证据的城市治理

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 提出VLA-LPAF模块提升VLA模型在多视角视觉输入下的泛化能力，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 由于不同环境中的视觉观测视角与数量差异大，导致VLA模型泛化受限。

Method: 提出轻量级模块VLA-LPAF，仅用2D数据，在潜空间融合多视角观测，仅需单视角微调即可适配多视角。

Result: 在CALVIN、LIBERO和自定义模拟基准上分别提升8%、15%和30%的任务成功率，并在真实世界任务中验证了视角自适应能力。

Conclusion: VLA-LPAF有效解决视角异构问题，显著增强VLA模型的泛化性与实用性。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 提出URNet，一种基于事件相机的不确定性感知立体深度估计网络，在DSEC数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟优势，但现有立体深度估计方法在精度和可靠性上仍有不足。

Method: 引入URNet网络，包含局部-全局细化模块和基于KL散度的不确定性建模方法，以提升深度估计的细节捕捉与预测可靠性。

Result: 在DSEC数据集上，URNet在定性和定量评估中均持续超越当前最优方法。

Conclusion: URNet通过不确定性感知与多尺度上下文建模，显著提升了事件相机立体深度估计的性能。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一种新的AI框架，可从多梯度DWI和形态MRI中自动识别周围神经，显著提升端粒症相关神经病变的成像精度。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔痛和神经受累，但传统成像技术难以精准描绘周围神经。

Method: Visionerves结合深度学习分割与符号空间推理，利用模糊空间关系编码解剖知识，无需手动ROI选择。

Result: 在10例子宫内膜异位症患者中，Dice分数提升达25%，空间误差小于5mm，优于传统追踪方法。

Conclusion: 该方法实现了自动、可重复的神经分析，为无创诊断子宫内膜异位症相关神经病变及其他神经受累疾病开辟了新路径。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: 提出首个针对巴基斯坦驾驶环境的隐私保护多模态驾驶员行为数据集V-SenseDrive，填补全球数据空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集来自发达国家，缺乏新兴经济体驾驶行为多样性，且面部录制侵犯隐私，急需本地化、隐私保护的驾驶员行为数据。

Method: 使用定制Android应用采集智能手机惯性传感器（加速度计、陀螺仪）、GPS数据与同步道路视频，覆盖城市干道、次干道和高速公路，记录正常、激进、危险三种驾驶行为。

Result: 构建了原始、处理后和语义三层结构的V-SenseDrive数据集，实现多模态时间对齐，支持驾驶行为分类与ADAS开发。

Conclusion: V-SenseDrive为巴基斯坦及类似环境的智能交通系统提供了首个真实、隐私友好的数据基础，推动情境感知交通解决方案的发展。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL是一系列参数从3B到70B的多模态大语言模型，通过领域增强技术和大规模训练实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 提升多模态模型在特定领域（如OCR、文档理解、数学推理）的性能，同时保持通用能力，并验证国产算力平台训练SOTA模型的能力。

Method: 采用多阶段渐进式训练和高精度数据合成管道，结合Baidu昆腾P800芯片进行全栈训练，增强领域特定能力。

Result: 在CCBench、SEEDBench IMG、ScienceQA、MMStar等基准上达到SOTA，OCR和文档理解任务表现优异（如DocVQA 94.75%），MathVista数学推理达78.6%，训练效率超90%。

Conclusion: Qianfan-VL证明了领域增强策略和国产算力平台在构建高性能、可部署多模态模型方面的有效性，适合企业广泛应用。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow提出一种基于ODE的去雾框架，结合MCBM生成真实雾图，实现单步推理与优异的现实场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法因缺乏真实配对数据和域差距，导致去雾效果不佳；传统ASM模型难以处理复杂真实雾况。

Method: 将大气散射模型重组为ODE，借鉴Rectified Flow学习最优轨迹映射雾图到清晰图，并引入MCBM模拟非均匀雾气生成真实训练数据。

Result: HazeFlow在多个真实世界去雾基准上达到SOTA性能，且仅需单步推理。

Conclusion: 物理引导的ODE框架结合合成真实雾数据，显著提升去雾模型在现实场景中的泛化能力与效果。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 通过通道剪枝、量化感知训练和TensorRT加速，成功压缩EcoWeedNet模型，在Jetson Orin Nano上实现高效精准农业检测。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源受限，部署深度学习模型困难，需在保持精度的同时压缩模型。

Method: 采用结构化通道剪枝、量化感知训练（QAT）和NVIDIA TensorRT加速，针对含残差连接、注意力机制等复杂架构进行优化。

Result: 模型体积减少68.5%，计算量降低3.2 GFLOPs，FP16下推理速度达184 FPS（比基线快28.7%），在CottonWeedDet12上mAP50达85.9%，超越YOLO11n和YOLO12n。

Conclusion: 所提方法在资源受限设备上实现了高效且高精度的杂草检测，适用于精准农业场景。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出一种结合增强模态丢弃和对比学习的多模态学习框架，显著提升在模态缺失和不平衡情况下的临床诊断性能。


<details>
  <summary>Details</summary>
Motivation: 医疗诊断中多模态数据广泛应用，但现有模型难以有效融合异构信息且对模态缺失鲁棒性不足。

Method: 引入可学习的模态token以增强缺失模态的融合能力，并将单模态对比目标扩展至融合的多模态表示。

Result: 在大规模临床数据集上达到SOTA性能，尤其在单模态可用场景表现优异，并成功集成到CT基础模型中。

Conclusion: 该方法高效、可扩展、泛化性强，具备显著临床应用潜力，代码已开源。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究通过490例CTPA扫描数据，系统评估了9种分割架构在肺栓塞（PE）分割中的性能，发现3D U-Net表现最佳，CNN优于ViT，预训练可能适得其反，远端栓塞仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 肺栓塞分割任务缺乏系统性架构评估，且现有模型在复杂形态和小样本下的表现不清，亟需统一评估框架以明确最优方案。

Method: 构建490例密集标注的CTPA数据集，统一测试框架下评估9种CNN与ViT模型（含预训练与随机初始化），对比Dice得分、检测准确性及泛化能力。

Result: 3D U-Net（ResNet编码器）最优，Dice=0.7131；CNN优于ViT；预训练损害分割性能；中央/大栓塞效果好，远端栓塞困难；模型间性能趋势一致。

Conclusion: 3D U-Net是当前PE分割最优架构，模型选择应优先考虑CNN与从头训练；远端栓塞分割受限于数据与任务难度，需进一步提升数据质量与算法适应性。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出一种新颖的图神经网络，显著提升手型识别准确率


<details>
  <summary>Details</summary>
Motivation: 现有计算方法很少显式建模手型，限制了识别精度和语言学分析

Method: 使用解剖学信息的图结构结合对比学习，分离时序动态与静态手型配置

Result: 在37类手型上达到46%准确率，远超基线方法的25%

Conclusion: 该方法为手型结构化识别建立了首个基准，推动手语计算建模发展

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 不同分类任务对胎儿超声中异常检测性能影响显著，任务选择需依据ID-OOD定义类型，且最佳检测不等于最优弃权决策。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注不确定性量化方法，但忽视了分类任务本身对异常检测性能的影响，尤其是在胎儿超声这种复杂临床场景中。

Method: 在四种分类任务上测试八种不确定性量化方法，评估其在不同ID-OOD定义（图像特征偏移 vs 解剖特征偏移）下的检测性能。

Result: OOD检测性能因任务而异，最佳任务取决于ID-OOD的定义类型，且高检测性能不一定对应最优弃权表现。

Conclusion: 医疗图像分析中需根据具体应用场景，协同优化分类任务与不确定性策略，而非仅依赖通用不确定性方法。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出OrthoLoC首个大规模正射地理数据辅助的无人机视觉定位数据集，并引入AdHoP算法显著提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位系统依赖高资源消耗的3D模型或大型图像库，难以在无GNSS/网络环境中部署，而轻量级正射地理数据未被充分挖掘。

Method: 构建含16,425张无人机图像的OrthoLoC数据集，支持多模态配对；提出AdHoP后处理方法，兼容任意特征匹配器以提升匹配质量。

Result: AdHoP提升匹配准确率最高达95%，降低平移误差达63%；系统性评估了域偏移、分辨率与共视性对定位的影响。

Conclusion: OrthoLoC为低资源视觉定位提供新范式，AdHoP可广泛集成，推动轻量级地理辅助定位的发展。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出一种无需外部训练数据的单图像异常检测方法SSDnet，在MVTec-AD和织物数据集上性能超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测依赖训练数据或参考样本，但在实际场景中这些数据常不可用，亟需零样本单图像异常定位方法。

Method: 基于CNN的归纳偏差，设计补丁级自重建框架，通过掩码、打乱和高斯噪声防止恒等映射，并引入基于内积相似性的感知损失。

Result: 在MVTec-AD上达到0.99 AUROC和0.60 AUPRC，在织物数据集上为0.98 AUROC和0.67 AUPRC，性能优于现有方法。

Conclusion: SSDnet无需外部数据与标签，鲁棒性强，为零样本图像异常检测提供了高效新范式。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 提出一种针对孟加拉语的视觉-语言 grounding 方法，利用合成数据和三重损失函数显著提升低资源语言的图像描述性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言中视觉-语言模型常因数据稀缺、翻译对齐断裂和英语中心预训练而生成错误对象描述。

Method: 采用冻结的 MaxViT 提取视觉特征，mBART-50 生成孟加拉语文本，通过轻量桥接模块连接模态；提出三重损失：Patch-Alignment Loss、InfoNCE 和 Sinkhorn OT，增强真实与合成数据的对齐。

Result: 在 Flickr30k-1k 和 MSCOCO-1k 上显著超越 CE 基线，BLEU-4 分别达 12.29 和 12.00，实-虚质心差距缩小 41%。

Conclusion: 该方法有效缓解低资源语言 grounding 问题，证明合成数据与结构化对齐损失的协同作用具有强大潜力。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一种仅用摄像头的轻量级BEV框架，通过知识蒸馏将大型规划模型UniAD的能力压缩到28M参数，实现全栈自动驾驶功能且速度提升5倍。


<details>
  <summary>Details</summary>
Motivation: 现有高效相机方案如VAD系列无法支持完整自动驾驶栈，而大型模型如UniAD参数量大、无法实时部署，亟需轻量且功能齐全的解决方案。

Method: 采用无模型依赖的多阶段蒸馏策略，融合特征层、输出层和自适应区域感知监督，将UniAD的多模态知识迁移到轻量BEV表示中。

Result: 在nuScenes上达到39.0 mAP检测、1.08 minADE运动预测、0.32碰撞率，运行速度11 FPS，参数减少78%，仅需摄像头输入。

Conclusion: TinyBEV证明了在资源受限条件下仍可保留完整驾驶智能，弥合了大型多模态模型与实时部署之间的鸿沟。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 提出新的模糊球标注方法，释放新数据集，并构建BlurBall模型，显著提升羽毛球检测与轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统标注将球标记在模糊条带前端，忽略运动信息且引入不对称性，影响检测效果。

Method: 采用模糊条带中心标注球位置，显式标注模糊属性，并提出BlurBall模型，结合多帧注意力机制（如SE模块）联合估计球位置与模糊特征。

Result: 新标注策略在多种模型上一致提升检测性能，BlurBall达到最优检测精度，并增强轨迹预测可靠性。

Conclusion: 利用运动模糊信息可显著提升球类检测与分析性能，为实时体育分析提供新思路。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出一种无需训练的视频目标检测方法MVP，仅在关键帧运行OWLv2，利用压缩域运动矢量传播检测结果，保持高精度且节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 在视频中对每帧运行大型开集检测器计算开销大，需找到高效替代方案以保留开集能力。

Method: 仅在固定间隔关键帧调用OWLv2，通过压缩域运动矢量（MV）的3x3网格聚合进行目标位置与尺度更新，辅以面积增长检查和可选单类切换，无需标签或微调。

Result: 在ILSVRC2015-VID验证集上，MVP达到mAP@0.5=0.609、mAP@[0.5:0.95]=0.316，在宽松IoU下接近帧级OWLv2-Large性能，且优于传统跟踪器（MOSSE/KCF/CSRT）。

Conclusion: 压缩域运动矢量传播是一种高效、无标签、开集的视频检测方法，显著降低计算开销同时保持优异零样本性能。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 通过预训练的白平衡网络改进现有HDR光照估计模型的颜色鲁棒性，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估时混淆了颜色与其他光照属性，忽略了颜色准确性对视觉真实感的关键作用。

Method: 使用新颖的HDR数据集，系统评估多种适应策略，重点测试预训练白平衡网络对输入图像的预处理效果。

Result: 预训练白平衡网络显著提升颜色准确性，优于其他策略，且在三种先进模型上均通用有效。

Conclusion: 简单预处理即可大幅提升颜色鲁棒性，是提升AR渲染真实感的高效且通用解决方案。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出一种无训练的支票字段检测框架，利用视觉语言模型实现零样本识别，降低部署门槛并支持数据集自动生成。


<details>
  <summary>Details</summary>
Motivation: 支票欺诈频发，传统检测依赖大量标注数据，但因隐私和专有性导致数据稀缺。

Method: 采用视觉语言模型（VLM）与多模态大语言模型（MLLM）实现零样本字段检测，无需训练即可识别签名、MICR线、金额、收款人等关键区域。

Result: 在110张多样格式支票上验证，表现优异，具备强泛化能力，可作为生成高质量标注数据的启动机制。

Conclusion: 该框架为金融场景提供了低门槛、高鲁棒性的支票检测解决方案，并能推动专属实时检测模型的开发。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 本文评估了主流视觉语言模型在噪声和遮挡图表上的表现，发现其存在严重幻觉问题，并提出了首个统一的图表推理基准CHART NOISe。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准过于理想化，无法反映真实场景中的噪声和遮挡问题，导致模型在实际应用中表现不可靠。

Method: 构建CHART NOISe数据集，结合图表腐蚀、遮挡与逆向不一致提示，评估GPT-4o、Claude Sonnet 4和Gemini 2.5 Pro的推理鲁棒性，并提出质量过滤与遮挡检测等缓解策略。

Result: 模型在噪声环境下性能显著下降， hallucinations（如数值伪造、趋势误读）频发，且过度自信；CHART NOISe首次统一了三类挑战场景。

Conclusion: CHART NOISe为图表理解模型的鲁棒性评估提供了新标准，推动未来研究提升模型在真实场景中的可靠性。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出一种无模板、无相位分箱的神经表示框架，通过连续变形建模呼吸运动，显著提升4D-MRI重建效率与精度


<details>
  <summary>Details</summary>
Motivation: 传统4D-MRI重建方法依赖相位分箱或独立模板，难以捕捉时间变异性、流程复杂且计算负担重

Method: 采用双网络协同架构：Spatial Anatomy Network（SAN）编码连续3D解剖结构，Temporal Motion Network（TMN）基于Transformer呼吸信号生成时序一致的变形场

Result: 在19名志愿者数据上验证，准确捕捉规则与不规则呼吸模式，保留血管/支气管连续性，处理时间从5小时降至15分钟，单帧推理<1秒

Conclusion: 该方法为4D放射治疗计划与实时自适应治疗提供了高效、高保真的新范式

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 现有基于卡尔曼滤波的跟踪方法在追踪快速小物体（如网球）时误差显著，需专用方法。


<details>
  <summary>Details</summary>
Motivation: 网球等快速小物体因运动不可预测、视觉特征小，导致传统跟踪方法性能下降，影响体育机器人感知能力。

Method: 在自建的10000帧网球数据集上评估OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT五种方法，分析推理速度与更新频率对跟踪精度的影响。

Result: DeepOCSORT误差最低（ADE 31.15像素），ByteTrack最快（26.6ms），但所有方法误差仍高达3-11cm，远超通用目标跟踪基准。

Conclusion: 当前卡尔曼滤波方法难以应对快速小物体的非线性运动，亟需开发专门的跟踪算法。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一种无需训练的运动感知自适应裁剪模块，利用H.264中的运动向量高效提升压缩域视频动作识别的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别方法在压缩域中计算开销大，难以实现实时部署，亟需一种轻量级且通用的解决方案。

Method: MoCrop通过运动向量构建运动密度子矩阵，结合去噪合并、蒙特卡洛采样和自适应裁剪，生成单裁剪窗口应用于所有I帧，无需训练、无额外参数。

Result: 在UCF101上，ResNet-50提升3.5%精度（同等计算量）或2.4%精度（减少26.5% FLOPs）；在CoViAR上达89.2%准确率，并显著降低计算开销；在MobileNet-V3、EfficientNet-B1、Swin-B上均表现一致增益。

Conclusion: MoCrop高效、通用、可即插即用，显著提升压缩域视频动作识别的精度与推理效率，适用于实时部署。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出CAFC-SE框架，通过码本量化与语义增强实现低码率下高效图像特征压缩与分析


<details>
  <summary>Details</summary>
Motivation: 现有方法在低码率下表现差，因保留冗余细节或学习过集中符号分布

Method: 采用基于码本的自适应特征压缩框架，通过向量量化将连续特征映射为离散索引，并选择性传输至云端，结合语义增强提升关键模式保留

Result: 在率失真与准确性上显著优于现有方法，尤其在低码率条件下优势明显

Conclusion: CAFC-SE能有效提升边缘-云系统中图像分析的效率与精度，适合资源受限场景

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet是一种超轻量级多核U型CNN，用于医学图像分割，在显著减少参数和计算量的同时，超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法计算复杂度高，难以部署于资源受限的点诊设备，亟需轻量且高性能的解决方案。

Method: 设计多核深度卷积块（MKDC）捕捉多分辨率空间关系，并结合通道、空间和分组门控注意力机制强化显著特征。

Result: MK-UNet仅0.316M参数和0.314G FLOPs，在6个二值医学数据集上超越TransUNet、UNeXt等SOTA模型，DICE分数提升最高达6.7%。

Conclusion: MK-UNet实现了性能与效率的突破，成为实时高保真医学诊断的理想选择，尤其适用于资源受限环境。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat通过将3D高斯分布绑定到CT网格，利用术中视频实现术前CT的 deformable 重建。


<details>
  <summary>Details</summary>
Motivation: 解决术中单目RGB视频与术前 volumetric 数据之间的对齐难题，提升手术导航精度。

Method: 将3D高斯分布参数化到CT网格三角面片上，通过光度监督联合优化高斯参数与网格变形，实现变形传播并更新CT。

Result: 在猪内脏手术和模拟人肝数据上验证了方法的有效性，可从单目RGB数据中获得合理的CT变形。

Conclusion: BridgeSplat首次实现基于高斯泼溅的术前CT动态更新，为手术导航提供了新范式。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 提出一种名为Diffusion-Guided Label Enrichment (DGLE)的伪标签优化框架，通过扩散模型从高质量初始种子标签传播生成完整高精度伪标签，提升无源域自适应语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有源自由域自适应（SFDA）方法因伪标签噪声大、整体优化困难，限制了自训练效果。

Method: 采用置信度过滤与超分辨率增强获取少量高质量初始伪标签种子，再利用扩散模型去噪并建模复杂分布，扩散生成完整高质量伪标签。

Result: 有效避免直接优化全集伪标签的困难，显著提升伪标签质量，增强目标域分割性能。

Conclusion: DGLE框架通过扩散引导的标签传播，为源自由域自适应提供了一种高效稳健的伪标签优化方案。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 本研究将超双曲空间引入Coarse-To-Fine Few-Shot Class-Incremental Learning任务，通过Poincaré球模型、超双曲对比损失和熵分布增强特征表示，显著提升粗细类别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得空间在表征层次数据时性能有限，而超双曲空间更擅长建模层级结构，因此需在C2FSCIL任务中利用其优势。

Method: 将特征提取器嵌入Poincaré球模型，引入超双曲对比损失与全连接层，并在超双曲空间中采用最大熵分布生成数据增强特征以应对小样本问题。

Result: 在C2FSCIL基准上，该方法显著提升了粗类和细类的分类准确率。

Conclusion: 超双曲空间能有效提升层次化小样本增量学习的性能，为特征表示与分类提供了新范式。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出一种几何感知的两阶段框架，通过分离几何移除与外观渲染，有效去除物体及其因果视觉伪影。


<details>
  <summary>Details</summary>
Motivation: 现有方法因忽略物体几何存在与其视觉效应间的因果关系，导致无法完整移除阴影、倒影等伪影或过度擦除其他对象。

Method: 采用两阶段框架：第一阶段基于掩码对齐的几何（如深度）移除，第二阶段依据更新后的几何渲染逼真RGB图像，结合偏好驱动目标函数引导学习。

Result: 在两个主流基准上实现最优性能，能同时移除物体及其因果视觉伪影，且避免结构误插入。

Conclusion: 几何感知的因果建模显著提升图像修复质量，为智能图像编辑提供了新范式。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 提出SEGA方法，提升对NR-IQA模型的黑盒攻击可迁移性


<details>
  <summary>Details</summary>
Motivation: 现有白盒攻击在黑盒场景下迁移性差，亟需提升对不可访问模型的有效攻击能力

Method: 通过高斯平滑源模型梯度并集成，结合自定义扰动掩膜过滤不必要扰动，构建可迁移黑盒攻击SEGA

Result: 在CLIVE数据集上验证SEGA显著优于现有方法，展现出卓越的迁移攻击效果

Conclusion: SEGA为NR-IQA模型的黑盒攻击提供了高效且实用的新范式，推动鲁棒性评估与防御研究

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet通过无参数的Hadamard乘积融合Transformer与生理D-Marker特征，在四个基准数据集上实现SOTA性能，同时减少26%参数并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习方法在微笑情感识别中因辅助任务监督和复杂损失平衡导致计算效率低下。

Method: 提出HadaSmileNet，采用参数-free的Hadamard乘法融合Transformer表示与生理学驱动的D-Marker特征，系统评估15种融合策略。

Result: 在UvA-NEMO、MMI、SPOS和BBC数据集上分别达到88.7%、99.7%、98.5%和100%准确率，参数减少26%，训练简化，特征可视化显示更强判别力。

Conclusion: HadaSmileNet高效且有效，特别适合需要实时情感计算的多媒体数据挖掘应用。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 使用事件相机和3D高斯泼溅技术，从单目视频中联合重建动态人体与静态场景，无需外部掩码，显著提升高速运动下的重建质量。


<details>
  <summary>Details</summary>
Motivation: 单目RGB视频在高速运动中因模糊导致人体与场景重建困难，事件相机具有微秒级时间分辨率，能有效缓解此问题。

Method: 提出事件引导的3D高斯泼溅框架，统一高斯集合携带语义属性，仅人体高斯变形，场景高斯静止，并引入事件引导损失匹配亮度变化。

Result: 在ZJU-MoCap-Blur和MMHPSD-Blur数据集上达到SOTA性能，PSNR/SSIM显著提升，LPIPS降低，尤其在高速主体上表现优越。

Conclusion: 该方法无需外部人体掩码，简化了高斯集合管理，验证了事件相机在动态重建中的优势。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T通过语义元组、在线去重和思维链微调LLM，实现实时威胁检测与可解释推理


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足实时性与决策可解释性需求

Method: 构建人类-物体-交互-地点语义元组，设计在线事件去重机制，微调LLM的思维链推理能力

Result: 在XD-Violence和UCF-Crime上显著超越SOTA，提升检测精度、实时性与可解释性

Conclusion: Live-E2T有效统一了实时性与可解释性，为视频威胁监控提供新范式

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 提出PhotoCritique数据集、PhotoEye模型和PhotoBench基准，提升多模态大模型对摄影美学的理解。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在美学理解上局限于常识层面，缺乏专业摄影知识，难以应对真实场景中的复杂美学分析需求。

Method: 构建大规模专业摄影批评数据集PhotoCritique，提出语言引导的多视角视觉融合模型PhotoEye，并建立专业美学评估基准PhotoBench。

Result: 在现有基准和PhotoBench上，PhotoEye显著优于现有模型，展现更强的美学理解能力。

Conclusion: 通过专业数据、新颖架构和权威基准，有效提升了MLLMs对摄影美学的深层理解。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 提出基于XMem的实时MRI引导放疗肿瘤分割框架，虽无精确数据，但初步表现满足临床实时需求。


<details>
  <summary>Details</summary>
Motivation: 提升MRI引导放疗中肿瘤跟踪的精度与安全性，应对标注数据有限的挑战。

Method: 采用记忆增强的XMem模型，实现长时 cine-MRI 序列中的肿瘤运动追踪。

Result: 缺乏详细实验数据，但初步结果显示分割性能合理且满足实时性要求。

Conclusion: 该框架为MRI引导放疗提供了可行的实时肿瘤分割方案，具有临床应用潜力。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM模型，通过空间-语义一致性约束实现多对比度MRI超分辨率，性能优越且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分建模空间-语义一致性，且忽略频域信息，导致细粒度对齐差和高频细节恢复不足。

Method: 引入动态空间扭曲模块、语义感知令牌聚合块和空间-频率融合块，联合优化空间对齐与频域细节恢复。

Result: 在公开和私有数据集上达到SOTA性能，参数更少，重建结果空间与语义一致性高。

Conclusion: SSCM有效解决了MC-MRI SR中的空间-语义一致性难题，为高效MRI成像提供了新范式。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出OraPO与FactS框架，用极低数据和算力实现放射科报告生成SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模数据和复杂模型，计算开销大，难以在资源受限场景应用

Method: 采用单阶段RL训练，通过轻量级oracle纠正失败探索，并用FactScore提取临床事实进行句级奖励引导

Result: 在CheXpert Plus上达到0.341 F1，优于SOTA，训练数据减少2-3个数量级，使用小型VLM即可

Conclusion: OraPO+FactS显著提升学习效率，为低资源场景下的放射报告生成提供高效新范式

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一种无需训练的参考式扩散模型多风格融合框架，可动态平衡多个风格输入并生成高质量融合结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅支持单风格输入，难以实现多风格混合且缺乏风格权重平衡机制。

Method: 通过语义标记分解模块编码所有风格图像与文本提示，并自适应注入冻结扩散模型的交叉注意力层，结合相似性感知重加权模块在去噪步骤中动态调整风格注意力。

Result: AMSF在定性和定量评估中均优于现有SOTA方法，支持两人以上风格无缝扩展，无需微调或外部适配器。

Conclusion: AMSF为扩散模型中的表达性多风格生成提供了实用且可扩展的解决方案。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: 提出MLF-4DRCNet，通过点级、场景级和提议级多模态融合，提升4D雷达与相机融合的3D目标检测性能，达成媲美LiDAR的效果。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达-相机融合方法沿用LiDAR-相机的BEV融合范式，忽略了雷达点云稀疏、不完整的特点，且仅进行粗粒度场景级融合，限制了性能。

Method: 提出MLF-4DRCNet两阶段框架，包含ERPE（增强雷达点编码）、HSFP（分层场景融合池化）和PLFE（提议级融合增强）三个模块，实现点-场景-提议三级融合。

Result: 在VoD和TJ4DRadSet数据集上达到SOTA性能，在VoD上性能媲美LiDAR基线模型。

Conclusion: 多级融合能有效克服雷达稀疏性问题，显著提升4D雷达在自动驾驶感知中的实用性。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: 引入PDLS框架，通过双潜流结构无训练地提升扩散模型的图像恢复质量，避免语义漂移。


<details>
  <summary>Details</summary>
Motivation: 现有方法单潜向量编码难以平衡结构保真与语义准确，导致重建模糊或属性错误。

Method: 基于校正流模型，提出提示引导的双潜流引导（PDLS），拆分为结构路径与语义路径，通过LQR最优控制动态调节生成轨迹。

Result: 在FFHQ-1K和ImageNet-1K上，PDLS在去模糊、超分、修复等任务中显著优于单潜向量基线，重建更忠实且语义更准确。

Conclusion: PDLS无需训练即可有效解决语义漂移问题，为扩散模型逆向提供高效稳定的新范式。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: Prima是一个基于22万份MRI数据训练的视觉语言模型，可在临床环境中实现高精度神经影像诊断，提升诊疗效率并减少健康不平等。


<details>
  <summary>Details</summary>
Motivation: 全球MRI需求激增导致医疗系统压力加大、诊断延迟、医生倦怠，低资源和农村地区患者受影响更严重。

Method: 利用大型学术医疗系统数据，构建首个面向临床MRI的视觉语言模型Prima，采用层次化视觉架构，训练于22万份MRI研究，并在3万份MRI上进行验证。

Result: Prima在52种神经系统疾病诊断中平均AUC达92.0，超越现有AI模型，提供可解释诊断、工作流优先排序和临床推荐，并展现算法公平性。

Conclusion: Prima证明了大规模VLM在临床影像中的变革潜力，可有效缓解医疗资源不均，推动AI驱动的公平医疗。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出Understanding-in-Generation (UiG)框架，通过图像编辑将理解融入生成过程，显著提升文本到图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法将理解与生成分离，限制了其对统一模型生成能力的引导效果。

Method: 引入图像编辑作为桥梁，在生成过程中逐步注入模型的理解能力，通过验证与指令修正迭代优化生成结果。

Result: 在TIIF基准的长提示设置下，性能提升3.92%，优于现有推理方法。

Conclusion: UiG通过理解与生成的深度融合，有效弥补了统一模型的生成缺陷，为文本到图像生成提供了新范式。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文提出了EndoSynth合成数据集并构建了内窥镜图像深度估计的基准，显著提升了模型在真实数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像的深度估计缺乏鲁棒基准和高质量数据集，限制了模型在临床场景中的泛化能力。

Method: 构建了基于先进深度估计模型的统一基准，并发布了包含真实标定深度与分割掩码的合成数据集EndoSynth，通过微调基础模型提升性能。

Result: 使用EndoSynth微调后，模型在未见过的真实内窥镜数据上的精度显著提升。

Conclusion: 该工作为内窥镜深度估计提供了重要资源，推动了合成与真实数据之间的桥梁建设。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出LEAF-Mamba模型，结合局部增强与自适应融合机制，在RGB-D显著性检测中实现性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Vision Transformer在局部感知或计算复杂度上存在局限，而State Space Model虽高效但缺乏局部语义与跨模态融合能力。

Method: 提出LEAF-Mamba，包含局部增强状态空间模块（LE-SSM）和基于SSM的自适应融合模块（AFM），以捕获多尺度局部依赖并实现跨模态互补融合。

Result: LEAF-Mamba在16种SOTA方法中持续领先，同时在RGB-T任务上表现优异，展示强泛化能力。

Conclusion: LEAF-Mamba有效解决了局部语义缺失与跨模态融合不足的问题，为RGB-D SOD提供了高效且高性能的新范式。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出一种轻量级食品图像分类算法，结合窗口多头注意力和空间注意力机制，在保持高精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 食品工业对自动化质量控制和智能生产需求上升，但现有Vision Transformer模型参数多、计算复杂度高，难以部署于资源受限环境。

Method: 融合窗口多头注意力机制（WMHAM）与空间注意力机制（SAM），通过窗口分块降低计算成本，同时自适应强化关键空间区域特征。

Result: 在Food-101和Vireo Food-172数据集上分别达到95.24%和94.33%的准确率，参数量和FLOPs显著低于基线模型。

Conclusion: 所提方法在计算效率与分类性能间取得良好平衡，适用于资源受限场景的部署。

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA是一种无需标注的三阶段框架，用于遥感图像中开放集土地覆盖的发现、分割和描述。


<details>
  <summary>Details</summary>
Motivation: 开放集土地覆盖分析需要在无类别监督下检测新对象并赋予可解释的语义标签，现有方法缺乏细粒度空间定位与多模态语义理解能力。

Method: 采用三阶段流程：(1) 使用提示微调的SAM模型进行精确分割；(2) 通过双阶段微调的多模态大语言模型进行语义标注与描述；(3) 用LLM作为评判者并结合人工评分评估结果。

Result: OSDA实现了高精度像素级分割与语义理解，无需人工标注，支持多样化遥感影像的鲁棒评估。

Conclusion: OSDA为动态土地覆盖监测提供了可扩展、可解释的解决方案，潜力在于自动化制图更新与大规模地球观测分析。

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 利用植物标本馆数据提升热带地区植物识别精度，解决数据匮乏问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习植物识别模型依赖大量野外照片，但主要覆盖北美和西欧，热带 biodiversity 丰富地区数据严重不足；标本馆长期积累的数字化标本数据提供潜在解决方案。

Method: 基于LifeCLEF 2021挑战赛，构建以圭亚那盾地为主的约1000种植物数据集，训练集包含数十万份标本图像与数千张野外照片，引入形态与功能性状信息，采用跨域分类方法学习标本与照片间的对应关系。

Result: 评估了使用标本数据提升贫数据区域植物识别性能的可行性，汇集了参赛团队的方法与结果，分析了跨域学习的有效性。

Conclusion: 植物标本馆数据可有效补充野外图像不足，为生物多样性热点地区提供可行的自动化识别路径，跨域学习是关键突破口。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 提出AGSwap方法和COF数据集，提升跨类别物体融合的文本到图像生成效果


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨类别物体融合时产生偏见、视觉混乱和语义不一致，且缺乏 comprehensive benchmark 数据集

Method: AGSwap包含组嵌入交换和自适应组更新，结合新构建的COF数据集进行训练与评估

Result: AGSwap在多种提示下优于现有SOTA方法，包括GPT-Image-1

Conclusion: AGSwap和COF显著推动了跨类别物体融合的T2I生成研究，为未来工作提供新基准和有效方法

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2019 挑战聚焦于数据匮乏的热带地区植物识别，使用1万种植物数据评估AI系统性能，并与专家比较。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习植物识别数据仅覆盖数万种，而全球植物约36.9万种，亟需拓展数据匮乏区域如圭亚那盾地和北部亚马逊雨林的研究。

Method: 构建包含1万种植物的专用数据集，评估参赛AI系统，并与热带植物专家性能对比。

Result: 评估了多种AI方法在高多样性但数据稀缺环境下的识别表现，提供了系统性能基准。

Conclusion: 该挑战推动了AI在生物多样性丰富但数据不足地区植物识别中的应用与研究。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: 提出一种无需训练的RSVG-ZeroOV框架，利用冻结的基础模型实现零样本开放词汇遥感图像视觉定位，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于封闭词汇集，且依赖昂贵数据与耗时微调，难以应对开放世界场景。

Method: 构建三阶段框架：(i) 利用视觉语言模型获取图文交叉注意力图；(ii) 借助扩散模型补充结构与形状细节；(iii) 引入注意力演化模块净化分割掩膜。

Result: 在无需训练的情况下，RSVG-ZeroOV在零样本和弱监督设置下均优于现有方法。

Conclusion: 冻结基础模型结合多模态先验可有效实现高效、可扩展的开放词汇遥感视觉定位。

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出Attribute Prompt Composition框架，结合文本语义提升ReID的判别性与泛化性，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型受限于单域或跨域场景，单域模型过拟合，跨域模型因归一化策略抑制身份判别线索。

Method: 设计Attribute Prompt Generator，包括语义属性字典SAD和提示组合模块PCM，结合Fast-Slow训练策略平衡判别性与泛化性学习。

Result: 在常规与领域泛化ReID数据集上均超越SOTA方法，显著提升判别与泛化性能。

Conclusion: APC框架有效利用文本语义和VLM先验知识，实现高判别性与强泛化性的统一。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 提出OTCCLIP框架，利用最优传输重构图文对，提升CLIP对抗数据投毒攻击的鲁棒性与性能


<details>
  <summary>Details</summary>
Motivation: 现有防御方法仅依赖全局特征匹配图文对，忽略了细粒度特征，易引入错误配对并损害预训练效果

Method: 提出基于最优传输的OTCCLIP框架，通过细粒度视觉与文本特征对齐，重新分配图文配对，并引入跨模态与模态内对齐目标函数

Result: 成功降低投毒攻击成功率，并显著提升 poisoned 数据集上CLIP的零样本和线性探测性能

Conclusion: OTCCLIP通过细粒度对齐有效缓解数据投毒攻击，优于传统方法，为CLIP安全预训练提供新思路

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出LFI框架，通过模拟VLM中的交互过程提升视觉基础模型的知识迁移能力，显著提升多任务性能与跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型忽视视觉语言模型中的跨模态交互过程，导致知识迁移效率低、泛化能力弱。

Method: 提出Learning from Interactions (LFI)框架，包含交互查询（Interaction Queries）和基于交互的监督机制，利用VLM的跨注意力机制建模视觉理解过程。

Result: 在TinyImageNet、COCO上分别取得3.3mAP和2.4AP提升，在PACS、VLCS上零样本表现提升2.4和9.3，收敛更快、参数开销小，人类评估语义一致性提高2.7倍。

Conclusion: 显式建模交互过程能更有效迁移VLM知识，LFI为视觉基础模型提供认知对齐的新范式。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 提出HyPSAM，结合动态融合与混合提示工程，提升RGB-热成像显著目标检测性能，达到SOTA且具通用性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T SOD方法受限于特征融合不充分和数据稀缺，难以精准定位边界与完整目标。

Method: 设计DFNet动态融合RGB与热成像特征生成初始显著图作为视觉提示，结合P2RNet利用文本、掩码和边界框提示引导SAM进行精细化优化。

Result: 在三个公开数据集上达到SOTA性能，且能无缝集成至其他RGB-T SOD方法，显著提升其效果。

Conclusion: 混合提示工程在RGB-T SOD中具有强大潜力，为多模态显著性检测提供了新范式。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: 提出TriFusion-AE多模态自编码器，融合文本、深度图和激光雷达点云，提升在噪声和对抗攻击下的重建鲁棒性


<details>
  <summary>Details</summary>
Motivation: 原始激光雷达点云易受噪声、遮挡和对抗攻击影响，传统自编码器在真实复杂场景中性能下降

Method: 设计TriFusion-AE，通过跨注意力机制融合文本语义、单目深度图几何特征与激光雷达空间结构

Result: 在强对抗攻击和高噪声下显著优于CNN自编码器，在nuScenes-mini数据集上验证有效，且可无缝集成至现有CNN架构

Conclusion: 多模态融合提升感知鲁棒性，为低数据场景下的自动驾驶提供有效解决方案

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 提出COLT方法，使开源视频LLM能在持续变化的工具流中学习工具使用而不遗忘旧工具，并构建视频工具基准数据集VideoToolBench，实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM工具使用方法依赖固定工具库，难以适应现实世界中持续演变的工具数据，易产生灾难性遗忘。

Method: 提出COLT，通过可学习的工具码本动态选择工具，结合视频-centric的指令微调数据集VideoToolBench实现持续学习。

Result: 在原有视频LLM基准和VideoToolBench上均达到SOTA性能。

Conclusion: COLT有效解决了视频LLM在动态工具环境中的持续学习问题，为真实场景应用提供了新路径。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一种无需训练的方法，利用扩散模型提升稀疏视图下3D高斯溅射的重建质量，显著减少伪影并增强多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图下3D重建因视觉信息不足产生伪影，现有生成先验方法难以保证多视角一致性，导致结构模糊和不真实细节。

Method: 提出FixingGS，通过蒸馏技术获取更准确且跨视角一致的扩散先验，并结合自适应渐进增强策略优化欠约束区域。

Result: FixingGS在视觉质量和重建性能上超越现有最先进方法，有效去除伪影并恢复合理细节。

Conclusion: FixingGS为稀疏视图3D重建提供了一种高效、无需训练的增强框架，具有实用潜力，代码将开源。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: 提出Bi-VLM，通过高斯分位数非均匀量化实现≤2-bit的视觉语言模型压缩，在VQA任务上显著超越SOTA，并发现图像令牌存在90%-99%冗余可进一步剪枝。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型计算开销大、内存需求高，难以部署于硬件受限环境。

Method: 基于高斯分位数将权重分为离群（显著）和多个内点（非显著）子集，设计显着性感知的混合量化算法，对缩放因子和二值矩阵施加不同约束。

Result: 在语言模型部分提升3%-47%，整体VLM提升4%-45%；发现图像令牌冗余达90%-99%，支持进一步剪枝。

Conclusion: Bi-VLM在极低比特下实现高效VLM压缩，兼具精度优势与推理效率提升潜力。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT提出一种离散自监督框架，通过多尺度向量量化的瓶颈约束，提升医学图像表示的可迁移性和标签效率。


<details>
  <summary>Details</summary>
Motivation: 现有自监督方法依赖复杂架构或特定解剖先验，易陷入Shortcut Learning，尤其在胸片等解剖相似性强、病理细微的模态中泛化性差。

Method: 引入DiSSECT框架，将多尺度向量量化集成到自监督流程中，构建离散表示瓶颈，迫使模型学习可重复、结构感知的特征，抑制视图特异或低效模式。

Result: DiSSECT在分类和分割任务上表现优异，仅需极少或无需微调，在低标签场景下标签效率高，跨多个公共数据集验证优于现有SOTA方法。

Conclusion: DiSSECT通过离散化表示瓶颈有效提升医学图像自监督学习的泛化性与迁移能力，为低标注数据场景提供高效解决方案。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 本文提出一种结合热成像、深度学习和车用通信的实时鹿-车碰撞预警系统，显著提升检测精度与响应速度。


<details>
  <summary>Details</summary>
Motivation: 美国每年发生约210万起鹿-车碰撞事故，导致440人死亡、5.9万人受伤及100亿美元经济损失，同时威胁鹿群数量，亟需有效解决方案。

Method: 使用12,000张热成像鹿图像训练深度学习模型，结合热成像与CV2X通信，实现实时检测与车辆间预警广播。

Result: 系统平均精度达98.84%，精确率95.44%，召回率95.96%；热成像在恶劣天气下保持88–92%准确率，远超可见光相机（<60%），端到端延迟低于100毫秒。

Conclusion: 该系统在真实环境中验证有效，为通过热成像与车联网技术降低鹿-车碰撞提供可行路径。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [70] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: 提出SAADi框架，通过对齐扩散模型与下游任务偏好，提升手术图像合成质量，显著改善分类与分割性能。


<details>
  <summary>Details</summary>
Motivation: 手术标注数据稀缺导致深度学习模型性能受限，现有扩散模型存在记忆数据、样本不多样等问题，可能损害下游任务表现。

Method: 构建优选与非优选合成图像对，轻量微调扩散模型，显式对齐生成过程与下游任务目标。

Result: 在三个手术数据集上，分类性能提升7–9%，分割提升2–10%，对少数类效果更显著，迭代优化进一步提升4–10%。

Conclusion: SAADi克服样本退化，确立任务感知对齐为缓解数据稀缺的关键原则，推动手术视觉应用发展。

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [71] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出一种基于模型的神经网络KMDS-Net，用于动态PET图像去噪，显著提升时空分辨率


<details>
  <summary>Details</summary>
Motivation: 动态PET短时帧因统计受限导致图像质量差，传统方法效果有限，深度学习在医学图像去噪中展现潜力

Method: 构建基于核空间的多维稀疏（KMDS）模型，利用帧间空间相关性和帧内结构一致性，并以神经网络替代参数估计，形成端到端的KMDS-Net

Result: 在仿真和真实数据上验证，KMDS-Net去噪性能优于基线方法，有效提升动态PET的时空分辨率

Conclusion: KMDS-Net为动态PET提供了高效去噪方案，具有临床应用潜力，源代码已开源

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [72] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 提出一种基于光流插值的多任务学习框架，解决手术场景中时空标注不平衡问题，提升机器人辅助手术的视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单任务学习，难以应对手术中复杂的时空动态与器械交互；高成本标注导致像素级标注数据稀缺，且长短期标注时空分布不平衡。

Method: 利用光流从已标注关键帧插值标签至未标注帧，增强稀疏空间监督，实现多任务学习的时空平衡。

Result: 提升了手术场景理解的准确性与效率，优化了机器人辅助手术的视觉感知性能。

Conclusion: 该框架有效缓解标注成本与时空不平衡问题，为机器人辅助手术提供更鲁棒的智能视觉支持。

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [73] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: 提出Hyper-Bagel框架，通过分治策略加速多模态理解与生成，实现最高22倍速度提升且保持高质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散去噪和自回归解码在处理多模态交错标记时计算开销过大，亟需高效加速方法。

Method: 采用推测解码进行下一个标记预测，并结合多阶段蒸馏优化扩散去噪过程，进一步引入对抗蒸馏与人类反馈学习。

Result: 多模态理解提速超2倍，文本生成提速16.67倍，图像编辑提速22倍；1-NFE模型支持近实时交互。

Conclusion: Hyper-Bagel显著提升多模态系统效率与响应速度，实现低成本、高响应的无缝交互体验。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [74] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 多模态大模型在基督教圣像图单标签分类中表现优于ResNet50，但性能受数据集质量影响，提示工程提升零样本表现，小样本学习效果有限。


<details>
  <summary>Details</summary>
Motivation: 评估通用多模态大模型（如GPT-4o、Gemini 2.5）在文化 heritage 领域基督教圣像分类任务中的能力，探索其作为元数据标注工具的潜力。

Method: 在ArtDL、ICONCLASS和Wikidata三个数据集上测试CLIP、SigLIP、GPT-4o、Gemini 2.5等模型，对比三种输入条件：纯类别标签、Iconclass描述、五样本示例，并与微调ResNet50基准比较。

Result: Gemini-2.5 Pro和GPT-4o超越ResNet50；Wikidata上性能骤降，SigLIP表现最佳；提示加入Iconclass描述提升零样本准确率，小样本学习增益微弱。

Conclusion: 通用多模态LLM可胜任复杂文化图像分类，具备应用于数字人文学科元数据自动标注的潜力，未来需优化提示设计并扩展模型与任务范围。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [75] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 提出一种可学习的重参数化图构建方法LRGC，用于视觉图神经网络，在不依赖超参数的情况下提升图像表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViG模型依赖非参数化、不可学习的统计方法构建图结构，无法自适应选择最优邻域，且需手动调参。

Method: 提出LRGC方法，通过节点间的键-查询注意力机制计算相似度，并使用可微分的软阈值重参数化进行边选择，所有阈值通过训练学习。

Result: 在ImageNet-1k数据集上，ViG-LRGC超越了现有同规模ViG模型的性能。

Conclusion: LRGC实现了端到端可学习、无超参数的图构建，显著提升视觉图神经网络的表达能力与泛化性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [76] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 通过结构化反思提升大语言模型在多轮工具调用中的错误恢复能力，显著提高成功率并减少冗余调用。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式提示或单向推理，无法学习错误诊断与修复，导致多轮交互中重复犯错。

Method: 提出结构化反思机制，将错误到修复的过程转化为显式、可控制、可训练的动作，结合DAPO与GSPO目标及面向工具使用的奖励机制，优化Reflect-Call-Final三步策略。

Result: 在BFCL v3和Tool-Reflection-Bench上显著提升多轮工具调用成功率，减少冗余调用，验证了结构化反思的有效性。

Conclusion: 显式化并直接优化反思过程，可增强工具交互的可靠性，为智能体从失败中学习提供可复现路径。

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [77] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出Point Prompt Defender，通过对抗强化学习自动优化点提示，提升SAM的分割鲁棒性与泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工或启发式提示，难以扩展和泛化，限制了SAM的性能

Method: 构建双空间图环境，采用对抗强化学习框架，攻击者优化破坏性提示，防御者学习抑制干扰并恢复精度，使用DQN训练，仅部署防御者进行推理

Result: 在多种任务上显著提升SAM的分割性能，实现灵活、可解释、即插即用的提示优化框架

Conclusion: Point Prompt Defender为基于提示的分割提供了高效、通用的自动化优化方案，无需重训即可增强SAM泛化能力

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [78] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: 发布首个多模态野生动物监测数据集SmartWilds，整合无人机、红外相机和声学记录，支持生态保护与AI研究。


<details>
  <summary>Details</summary>
Motivation: 应对濒危物种研究、保护生态学和栖息地管理中对全面环境监测的迫切需求。

Method: 在俄亥俄州The Wilds保护区同步采集无人机影像、相机陷阱图像/视频和生物声学记录，覆盖220英亩牧场及多种动物。

Result: 展示了多模态传感器在土地利用、物种检测、行为分析和栖息地监测中的互补优势，建立了可复用的监测协议。

Conclusion: SmartWilds为保护计算机视觉研究提供了开放数据集，未来将扩展GPS追踪和跨季节数据。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [79] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 提出首个用于遥感图像3D理解的基准RS3DBench，包含5.5万对对齐的图像与深度图，并基于稳定扩散构建高性能深度估计模型。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏完整的深度信息或图像与深度图的精确对齐，制约了3D视觉模型的发展。

Method: 构建RS3DBench基准数据集，融合遥感图像、像素级对齐深度图和文本描述；提出基于稳定扩散的深度估计模型，利用多模态融合提升性能。

Result: 在RS3DBench上实现了遥感深度估计的最先进性能，为3D视觉感知与地理AI提供新工具。

Conclusion: RS3DBench推动遥感领域3D视觉模型与地理人工智能的发展，数据集、模型与代码已开源。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [80] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出首个无SfM的事件相机去模糊3D高斯泼溅方法DeblurSplat，高效生成高质量新视角。


<details>
  <summary>Details</summary>
Motivation: 传统SfM方法因相机位姿累积误差影响初始点云精度，导致去模糊3D重建效果不佳。

Method: 利用DUSt3R直接从模糊图像获取初始点云，避免SfM；引入事件流提供细粒度监督，联合优化重建。

Result: DeblurSplat在保真度和渲染效率上均超越现有去模糊3D-GS方法。

Conclusion: 事件相机与SfM-free策略能显著提升动态场景的3D高斯泼溅去模糊性能。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [81] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: Moir'eNet是一种高效参数的U-Net架构，通过融合频域与空域特征实现莫尔条纹去除，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 莫尔条纹由显示像素网格与相机传感器网格的频谱混叠引起，具有各向异性和多尺度特性，传统去莫尔方法效果不佳。

Method: 提出Moir'eNet，包含方向性频-空编码器（DFSE）和频-空自适应选择器（FSAS），联合建模频域与空域特征以精准抑制莫尔条纹。

Result: 在公开数据集上达到SOTA性能，参数量仅5.513M，较ESDNet-L减少48%，同时保持优越的修复质量。

Conclusion: Moir'eNet在去莫尔效果与参数效率上取得平衡，适用于智能手机、工业成像和增强现实等资源受限场景。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [82] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出一种频率感知的音视频分割框架FAVS，通过频域分解与跨模态一致性建模提升分割性能


<details>
  <summary>Details</summary>
Motivation: 现有AVS方法忽视音频高频噪声与视觉高频细节之间的频率域矛盾，导致性能受限

Method: 引入FDED模块进行残差迭代频域分解，结合SCMC模块通过专家混合架构实现跨模态语义一致性与特征保留

Result: 在三个基准数据集上达到SOTA性能，定性结果验证了模块有效性

Conclusion: 频域视角下的分解与重组为AVS提供了新思路，F得出框架显著提升多模态融合效果

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [83] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 本文综述了视觉感知任务中四种代表性可解释AI方法：显著性图、概念瓶颈模型、基于原型的方法及混合方法，分析其机制、优劣与评估指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在图像分析中表现优异但缺乏可解释性，影响其在关键应用中的可靠性，因此需要可解释AI（xAI）方法来揭示模型决策过程。

Method: 系统综述四种xAI方法：显著性图、概念瓶颈模型（CBM）、基于原型的方法和混合方法，分析其机制、优缺点及评估指标。

Result: 提供了四种xAI方法的全面对比与评估框架，为未来研究和应用场景提供指导。

Conclusion: 可解释AI对提升深度学习模型的可信度至关重要，四种方法各有优势，未来应注重方法融合与标准化评估。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [84] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 使用增强的DDPM生成高质量合成LiDAR点云，提升自动驾驶感知性能


<details>
  <summary>Details</summary>
Motivation: 真实LiDAR数据采集耗时且受噪声和稀疏性影响，亟需高质量合成数据增强

Method: 提出改进的去噪扩散概率模型（DDPM），引入新型噪声调度与时间步嵌入技术，生成更真实的点云数据

Result: 在IAMCV和KITTI-360数据集上，性能超越多数SOTA方法，显著改善噪声与稀疏数据下的点云质量

Conclusion: 该方法能有效生成具有丰富空间关系和结构细节的多样化点云，提升自动驾驶环境感知能力

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [85] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: 提出一种基于异常先验的自监督预训练方法AGSSP，显著提升金属表面缺陷检测性能，mAP提升达11.4%。


<details>
  <summary>Details</summary>
Motivation: 传统预训练方法在工业缺陷检测中面临域差距大和自监督学习难以区分缺陷与噪声的困境。

Method: AGSSP采用两阶段框架：先通过异常图知识蒸馏学习缺陷显著特征，再利用伪缺陷框预训练检测器，辅以高质量异常图生成方法和大规模工业数据集。

Result: 在多个设定下均显著优于ImageNet预训练模型，mAP@0.5提升10%，mAP@0.5:0.95提升11.4%。

Conclusion: AGSSP有效解决工业缺陷检测中的预训练难题，为小样本场景提供了高效解决方案，并公开了数据集与代码。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [86] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出首个音频驱动的通用真实感光子形象合成方法，通过UHAP编码几何与外观变化，实现高精度唇同步和表情细节生成


<details>
  <summary>Details</summary>
Motivation: 现有方法仅映射音频到几何变形，忽略音频相关的外观变化，导致真实感不足

Method: 引入Universal Head Avatar Prior (UHAP)，以中性扫描数据为监督，直接将原始音频映射到包含几何与外观的潜在表达空间，并结合单目编码器高效个性化新主体

Result: 生成高真实感Avatar，精确还原唇动、眉部运动、眼神变化和口腔内部细节，显著优于仅几何建模方法

Conclusion: 该方法是首个可泛化的音频驱动Avatar模型，同时建模几何与外观，在唇同步、图像质量和感知真实感上全面领先

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [87] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的模块化管道，用于自动检测、追踪和提取双光子显微镜下树突棘的3D+时间序列特征，并发布了首个相关标注数据集。


<details>
  <summary>Details</summary>
Motivation: 树突棘的结构动态对研究学习与记忆的神经基础至关重要，但大规模3D+时间显微数据的分析仍具挑战性且耗时。

Method: 结合基于Transformer的检测模块、整合空间特征的深度追踪组件、利用空间一致性的时间追踪模块，以及量化生物相关属性的特征提取单元。

Result: 在开源标注数据及两个新发布标注数据集上验证了方法有效性，首次提供了用于检测、深度追踪和时间追踪的专用数据集。

Conclusion: 该方法实现了树突棘动态的可扩展端到端分析，并通过公开数据、代码和预训练权重为未来研究建立了基准。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [88] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出一种无需标注数据的零样本图像分类框架，结合视觉语言模型和预训练视觉模型，通过自学习循环动态提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大量标注数据，实践中数据稀缺，亟需无监督或少监督的解决方案。

Method: 利用视觉语言模型进行置信度伪标注，预训练视觉模型增强特征，迭代训练轻量分类器，避免微调大模型。

Result: 在十个多样数据集上超越基线零样本方法，无需微调VLM或使用大语言模型。

Conclusion: 该方法通过视觉与语义互补线索，实现高效、低依赖的零样本分类，具有实用价值。

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [89] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 提出MirrorScene3D数据集和ReflectiveGS方法，利用镜面反射提供互补视角，显著提升含镜环境的3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和3DGS在含镜场景中表现不佳，因镜面反射带来视点依赖失真，而传统方案仅依赖对称映射，忽视了反射提供的额外信息。

Method: 构建MirrorScene3D数据集（含1256张图像与镜面掩码），并提出ReflectiveGS，将镜面反射作为互补视角增强几何重建与细节恢复。

Result: ReflectiveGS在MirrorScene3D上超越现有方法，在SSIM、PSNR、LPIPS指标和训练速度上均表现更优。

Conclusion: 镜面反射是提升含镜环境3D重建的关键信息源，ReflectiveGS为该领域设定了新基准。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [90] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 使用深度学习和GAN生成数据，提升腹腔镜胆囊切除术中胆道的可视化以减少损伤风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽恢复快、美观，但胆管损伤风险高，影响生活质量与生存率，需改善术中胆道可视化。

Method: 基于白光图像，采用YOLO检测算法，结合传统数据增强与GAN生成合成数据构建训练数据库。

Result: 实验验证了方法的有效性，并讨论了伦理问题。

Conclusion: 深度学习结合GAN生成数据可显著提升胆道定位精度，有助于降低手术风险。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [91] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出Prompt-DAS，一种基于提示的多任务框架，实现高效无监督和弱监督域自适应分割


<details>
  <summary>Details</summary>
Motivation: 现有方法如SAM需为每个实例提供提示，标注成本高；为实现annotation-efficient学习，需更灵活的域自适应分割方法

Method: 提出Prompt-DAS框架，集成中心点检测任务与提示引导对比学习，支持点提示的灵活使用（全点、稀疏点或无点）

Result: 在多个挑战性基准上表现优于现有UDA、WDA及SAM方法

Conclusion: Prompt-DAS通过灵活提示机制与对比学习，显著提升域自适应分割效率与精度，适用于多种标注场景

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [92] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 提出VIR-Bench基准，评估多模态大模型在长距离时空旅行视频中的理解能力，发现现有模型表现差，但能提升实际旅行规划代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准多局限于室内或短时户外场景，缺乏对长距离时空轨迹理解的评估，而此类能力对嵌入式AI导航至关重要。

Method: 构建VIR-Bench基准，包含200个旅行视频，将行程重建定义为评估任务，以测试模型的时空智能，并开发原型旅行规划代理验证其应用价值。

Result: 当前领先MLLMs在VIR-Bench上表现不佳，证明长时空间理解仍是挑战；基于该基准的原型代理显著改善行程推荐效果。

Conclusion: VIR-Bench有效评估并推动了MLLMs在长距离时空理解上的发展，且其评估体系可直接转化为实际应用性能提升。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [93] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 提出步级推理框架CoS，提升视觉语言模型的细粒度推理能力与评估效果，实现强化学习与推理时扩展的突破。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的思维链推理过于粗粒度，难以评估中间步骤质量，制约了强化学习与推理优化。

Method: 引入步级推理数据、过程奖励模型（PRM）和强化学习训练框架，实现对每个推理步骤的精细评估与优化。

Result: 在多个视觉语言基准上取得显著提升，并揭示了推理时扩展的有效性与各组件贡献。

Conclusion: 该框架为视觉语言推理建立了可靠基线，提供了可解释、可扩展的多模态推理新范式。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [94] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 提出一种基于SAM和ViT的弱监督食物图像语义分割方法，无需像素级标注，通过CAM生成提示，结合预处理与多掩码策略提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有食物图像分割依赖昂贵的像素级标注，亟需减少人工标注成本的弱监督方法。

Method: 使用Swin Transformer生成CAM作为SAM的提示输入，结合图像预处理和单/多掩码策略优化分割结果，仅用图像级标签训练ViT。

Result: 在FoodSeg103数据集上，平均生成2.4个非背景掩码，多掩码策略实现mIoU 0.54。

Conclusion: 该方法可显著降低标注成本，适用于食品标注与营养追踪系统的集成应用。

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [95] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 提出DyL-UNet模型，提升超声心动图序列分割的时间稳定性，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 超声心动图受变形和斑点噪声影响，单帧分割虽准但时序不稳定，影响临床功能评估。

Method: 提出DyL-UNet，构建Echo-Dynamics Graph（EDG）提取动态信息，结合Swin-Transformer编码器-解码器分支与Cardiac Phase-Dynamics Attention（CPDA）增强时序一致性。

Result: 在CAMUS和EchoNet-Dynamic数据集上，DyL-UNet在保持分割精度的同时显著提升时间稳定性。

Conclusion: DyL-UNet为自动化超声心动图分析提供了稳定可靠的分割解决方案。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [96] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: 提出ColorBlindnessEval基准，评估视觉语言模型在类色盲测试的视觉对抗场景中的表现，揭示模型存在严重幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂视觉环境中的鲁棒性不足，尤其在类似色盲测试的对抗性图像中表现不佳，亟需系统性评估工具。

Method: 构建包含500张类Ishihara图像的基准数据集，涵盖0-99数字，使用Yes/No和开放提示评估9个VLMs，并与人类表现对比。

Result: 模型在对抗性颜色模式中准确率显著低于人类，普遍存在幻觉，难以正确识别嵌入数字。

Conclusion: ColorBlindnessEval有效揭示VLMs在视觉对抗场景中的局限性，为提升其真实应用场景的可靠性提供重要评估工具。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [97] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian通过将扩散模型引入小波域，在稀疏视图下高效重建3D高斯对象，显著减少训练时间并保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点绘（3DGS）在稀疏视图下性能急剧下降，现有方法依赖扩散模型修复渲染结果，但计算开销大。

Method: 将扩散模型应用于小波域的低频LL子带，高频子带由轻量网络优化，并提出高效的在线随机掩码策略替代传统的留一法训练。

Result: 在Mip-NeRF 360和OmniObject3D数据集上，WaveletGaussian在保持竞争性渲染质量的同时大幅减少训练时间。

Conclusion: 小波域扩散是一种高效稀疏视图3D重建方法，兼顾质量与效率。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [98] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: 提出Sa2VA-i改进版，修复训练与推理不一致问题，在多个视频分割基准上显著提升性能，甚至媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 原始Sa2VA在视频指代分割任务中未发挥全部潜力，主要因训练与推理过程存在不一致。

Method: 提出Sa2VA-i，修正训练与推理间的不一致问题，保持原模型结构并优化实现细节。

Result: 在MeViS、Ref-YT-VOS、Ref-DAVIS和ReVOS上分别提升最高+11.6、+1.4、+3.3和+4.1 J&F，Sa2VA-i-1B性能媲美Sa2VA-26B。

Conclusion: 实现细节对模型性能至关重要，该工作为视频指代分割领域提供重要启示，并开源代码与模型。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [99] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，将多光谱图像作为零样本输入注入通用多模态模型（如Gemini2.5），显著提升遥感任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型需专门训练多光谱数据，成本高且无法与通用多模态模型兼容；后者虽强大却无法理解多光谱信号。

Method: 通过视觉空间适配和指令注入，将多光谱数据转换为通用模型可理解的格式，无需重新训练。

Result: 在遥感基准测试中，Gemini2.5实现显著的零样本性能提升，验证方法有效性与易用性。

Conclusion: 该方法使地理信息从业者能便捷利用通用多模态模型处理专业遥感数据，加速分析并发挥其推理能力。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [100] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V 是一个统一的多模态医学基础模型，结合图像分析与文本推理，实现病灶定位、报告生成和诊断推理。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像模型过于专一，需多个网络，泛化能力差；临床应用需精准视觉定位、多模态融合与链式推理。

Method: 提出 Citrus-V 模型，整合检测、分割与多模态链式推理，并设计新型多模态训练方法，发布开源数据集。

Result: Citrus-V 在多个基准上超越现有开源模型和专家系统，支持精准量化、自动报告和可靠会诊。

Conclusion: Citrus-V 实现了从视觉定位到临床推理的统一管道，显著提升医学影像分析的智能性与实用性。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 使用多模态大模型（MLLMs）在无标注数据下通过视觉分析增强提示，实现交通事故的零样本检测，Pixtral表现最优。


<details>
  <summary>Details</summary>
Motivation: 传统事故检测依赖大量标注数据，而基础设施视觉传感器需高效、可扩展的实时检测方案，零样本学习可减少对标注数据的依赖。

Method: 在CARLA模拟的DeepAccident数据集上评估Gemini、Gemma 3和Pixtral等MLLMs，结合YOLO、Deep SORT和SAM进行视觉增强提示，比较其检测与描述能力。

Result: Pixtral F1值0.71、召回率83%最优；Gemini 1.5提示增强后精度达90%但F1和召回下降；Gemma 3表现最稳定。

Conclusion: MLLMs联合先进视觉分析技术可显著提升交通事故检测的准确性与可解释性，具备实际部署潜力。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: 提出Track-On2，一种基于Transformer的在线长时点跟踪模型，仅用合成数据训练即可超越现有线上和离线方法。


<details>
  <summary>Details</summary>
Motivation: 解决视频中点在显著外观变化、运动和遮挡下的长期在线跟踪问题，满足实时应用需求。

Method: 基于Transformer架构，采用因果处理与记忆机制，结合改进的合成训练策略，实现帧级因果推理与时空一致性维持。

Result: 在五个合成与真实数据集上达到SOTA性能，超越包括利用双向上下文的离线方法。

Conclusion: 因果记忆架构结合合成数据训练是高效、可扩展的点跟踪解决方案，无需依赖未来帧或迭代优化。

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是一个用于多相机、多光谱同步和实时检测海豹与北极熊的系统，显著缩短数据处理时间并开源所有工具。


<details>
  <summary>Details</summary>
Motivation: 传统方法在航空调查中处理冰上海豹数据耗时长，亟需高效自动化解决方案。

Method: 通过精密校准与硬件同步实现多光谱数据融合，结合元数据标注和世界坐标映射，提升检测精度与效率。

Result: 数据处理时间减少高达80%，所有图像与检测结果可精确绘制在世界平面上，便于评估。

Conclusion: KAMERA为科学界提供了可复用、开源的多光谱监测系统，有望推动类似应用场景的发展。

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX通过时空冗余优化，实现边缘-云协同推理，在大幅降低能耗和延迟的同时保持高精度


<details>
  <summary>Details</summary>
Motivation: 边缘部署SNN面临高时间步开销、延迟和能量约束，现有边缘-云协同方案受制于传输成本和延迟

Method: 提出NeuCODEX架构，包含学习型脉冲压缩模块和动态提前退出机制，联合优化时空冗余

Result: 数据传输减少达2048倍，边缘能耗降低90%以上，端到端延迟降低3倍，精度损失小于2%

Conclusion: NeuCODEX实现了资源受限环境下高效实用的SNN部署

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 提出一种鲁棒的自监督立体匹配方法，通过视觉基础模型和场景对应先验提升恶劣天气下的性能


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法在夜间、雨雾等恶劣天气下性能显著下降，主要因CNN特征提取失效和光度一致性假设失效

Method: 引入视觉基础模型的鲁棒先验增强特征提取，并构建场景对应先验替代光度一致性，结合合成的含真实天气退化的数据集，提出鲁棒自监督场景对应学习与恶劣天气蒸馏两阶段训练框架

Result: 在合成与真实恶劣天气数据上显著超越现有自监督方法，具有优越性与通用性

Conclusion: 通过融合视觉基础模型与场景对应先验，可有效提升自监督立体匹配在恶劣天气下的鲁棒性与泛化能力

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: 提出YOLO-LAN模型，基于YOLO和M2IoU损失，显著提升结肠镜下息肉检测精度，适用于临床AI辅助筛查。


<details>
  <summary>Details</summary>
Motivation: 传统结肠镜手动检测存在不一致和漏检问题，亟需更准确、实时的自动检测方法。

Method: 采用YOLO-LAN框架，结合M2IoU损失函数、多样化数据增强和负样本训练，提升模型在真实临床场景中的泛化能力。

Result: 在Kvasir-seg和BKAI-IGH NeoPolyp数据集上，YOLOv12和YOLOv8分别达到mAP$_{50}$ 0.9619/0.9540和mAP$_{50:95}$ 0.8599/0.8487，显著优于现有方法，尤其在小息肉定位上表现优异。

Conclusion: YOLO-LAN具有高精度与临床实用性，可有效提升结直肠癌早筛的自动化水平和诊断可靠性。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 通过改进SAM-2框架，利用长期记忆和概念感知记忆，在LSVOS的MOSEv2赛道中取得第一，JF得分为39.89%


<details>
  <summary>Details</summary>
Motivation: 应对复杂半监督视频目标分割中的长期遮挡、重出现与干扰物问题

Method: 分析并增强SeC（SAM-2的改进版），利用长期记忆保持时序连续性，概念感知记忆提供语义先验以抑制干扰

Result: 在MOSEv2测试集上获得39.89%的JF分数，位列第一

Conclusion: 长期记忆与概念感知记忆协同作用，有效解决MOSEv2核心挑战，显著提升分割性能

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 通过模拟人类视觉的双流机制，分解视觉语言模型的视觉处理为对象识别与空间感知，并提出高效的令牌压缩与RoPE缩放技术以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型以序列化方式处理图像，与人类并行视觉机制不符，且内部机制不透明，限制了理解与创新。

Method: 基于人类视觉双流假说，将视觉处理分解为对象识别（图像转文本标记图）与空间感知（推导位置表示的几何结构），并提出插件式视觉解码器与RoPE缩放技术。

Result: 实验证实了两阶段感知过程与几何结构，所提方法显著提升解码效率与空间推理能力。

Conclusion: 该工作提供了对VLM内部机制的深度理解，并为未来架构设计提供了清晰原则。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 提出一种无视觉编码器的文本到文本检索方法，仅用文本描述替代图像，实现优于传统多模态模型的零样本检索性能，且参数更小、更隐私友好。


<details>
  <summary>Details</summary>
Motivation: 传统对比训练的视觉语言模型存在语言理解浅层、模态鸿沟大、依赖海量数据及隐私问题。

Method: 放弃视觉编码器，利用VLLM生成结构化图像描述，构建文本到文本检索管道，并在subFlickr和subCOCO基准上评估。

Result: 在多个基准上达到SOTA零样本性能，仅用几小时在双GPU上微调，参数低至0.3B，性能超越传统多模态模型。

Conclusion: 视觉编码器并非检索必需，文本驱动的单一编码器框架更高效、隐私友好，为VLMs提供新范式。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 复合性与长描述理解在视觉语言模型中相互促进，高质量数据和模型设计是关键。


<details>
  <summary>Details</summary>
Motivation: 现有对比视觉语言模型在理解长而密集的描述时表现不佳，研究者假设复合性（对象-属性绑定与对象间关系推理）是解决该问题的关键。

Method: 训练并评估多种针对复合性或长描述理解的模型，分析二者之间的相互影响。

Result: 复合性训练提升长描述检索性能，长描述训练促进复合性理解，但效果受数据质量和参数更新策略显著影响；冻结位置嵌入等策略无效。

Conclusion: 复合性与长描述理解是相互关联的能力，可通过高质量、接地的密集描述联合训练实现共同提升，为模型泛化提供实用指导。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 利用合成RGB图像、少量真实标注和GAN跨模态对齐，显著提升热成像中植物与杂草的分割性能。


<details>
  <summary>Details</summary>
Motivation: 热成像中植物与杂草对比度低、遮挡频繁，导致高通量田间表型分析中的分割困难。

Method: 使用1128张合成RGB图像生成分割掩码，结合最多5张真实标注图像，并通过CycleGAN-turbo实现RGB到热成像的跨模态对齐。

Result: 相比纯真实数据基线，杂草类相对提升22%，植物类提升17%。

Conclusion: 合成数据+少量真实标注+生成式跨域对齐可大幅提升复杂田间环境下多模态图像的分割效果。

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: 提出一个公开的儿童脑积水数据集HyKid，包含高分辨率MRI和专家标注的脑组织分割，发现脉络丛体积可作为脑积水评估的生物标志物。


<details>
  <summary>Details</summary>
Motivation: 现有儿童脑积水研究缺乏公开的专家标注数据集，尤其缺少脉络丛分割数据。

Method: 收集48例患儿的3D MRI，通过切片到体素算法重建高分辨率图像，并由神经科医生手动校正脑组织分割，同时使用检索增强生成框架提取临床报告结构化数据。

Result: 脉络丛体积与总脑脊液体积高度相关，预测模型AUC达0.87，验证了其作为生物标志物的潜力。

Conclusion: HyKid数据集为神经影像算法开发提供了高质量基准，并揭示了脉络丛在脑积水评估中的关键作用。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出MsFIN网络，通过多尺度特征融合与上下文交互建模，显著提升基于行车记录仪视频的事故提前预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理行车记录仪视角下的目标遮挡问题及事故前复杂异步时序行为的建模。

Method: 设计Multi-scale Feature Interaction Network (MsFIN)，包含多尺度特征聚合、因果约束时序处理和多尺度特征后融合三个模块，结合Transformer实现跨尺度交互。

Result: 在DAD和DADA数据集上，MsFIN在预测准确率和提前性上均显著优于现有单尺度模型，消融实验验证了各模块有效性。

Conclusion: 多尺度特征融合与上下文交互建模是提升行车视角事故预测性能的关键。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 提出一种基于发展性专家混合架构的持续学习方法，用于应对不断演变的深度伪造人脸检测问题，通过正交梯度防止遗忘，有效提升模型适应新伪造类型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有检测模型难以跟上快速演变的伪造技术，且通常在学习新类型时遗忘旧类型，亟需一种能持续适应新伪造而不遗忘的检测框架。

Method: 采用发展性专家混合（MoE）架构，引入Real-LoRA学习真实人脸特征，多个Fake-LoRA分别学习不同伪造类型，并通过正交梯度约束避免知识干扰与灾难性遗忘。

Result: 在多种数据集和增量伪造类型协议下，该方法显著优于现有方法，能有效持续学习新伪造类型而不遗忘旧类型。

Conclusion: 将人脸伪造检测建模为持续学习问题，并结合正交专家架构，是应对动态伪造演化的有效途径，具备良好的可扩展性与实用性。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一种统一的多模态掩码扩散模型，能同时实现图像理解与高分辨率生成，并通过规划与自反思提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散模型仅支持简单图像理解与低分辨率生成，缺乏高效统一的框架来结合理解与生成能力。

Method: 引入弹性混合Transformer架构、通用文本条件化和分层采样技术，实现理解驱动的生成与编辑。

Result: 在RefCOCO、GenEval和ImgEdit等基准上超越Qwen2.5-VL和FluxKontext-dev，实现SOTA性能并加快推理速度。

Conclusion: Lavida-O是首个通过理解能力提升生成效果的统一MDM，为多模态模型提供了新范式。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 提出ConViS任务和ConViS-Bench基准，通过语义概念实现可解释的视频相似度评估，提升模型对视频相似性的细粒度理解


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖全局相似分数，难以像人类一样从多维度（如动作、场景）理解视频相似性，亟需细粒度、可解释的评估方法

Method: 引入Concept-based Video Similarity (ConViS) 任务，结合ConViS-Bench基准，使用预定义语义概念计算相似分数并标注对比描述

Result: 基准包含多领域标注视频对，评测现有模型发现其在不同概念上表现差异显著，部分概念难以建模

Conclusion: ConViS-Bench为语言驱动的视频理解研究提供了新工具，推动细粒度、可解释的视频相似性评估发展

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出一种对抗优化的VQ-GAN框架，通过密集运动分词压缩时空热图，显著提升人体运动表示的效率与质量。


<details>
  <summary>Details</summary>
Motivation: 人体运动数据维度高且冗余大，传统方法存在运动模糊和时序错位等问题，亟需更高效且精确的压缩表示方法。

Method: 结合密集运动分词与对抗精炼机制，通过量化令牌压缩时空热图，优化重建质量并减少伪影。

Result: 在CMU Panoptic数据集上，SSIM提升9.31%，时序不稳定性降低37.1%；2D运动仅需128令牌，3D运动需1024令牌实现高保真重建。

Conclusion: 该方法有效实现人体运动的高效压缩与精细表示，具备在多种应用中部署的可行性。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 提出Graph-Radiomic Learning (GrRAiL)方法，通过图论量化MRI病灶内异质性，显著提升肿瘤复发与放射性损伤等混淆病理的鉴别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统放射组学方法聚合ROI特征，忽略复杂空间关系，难以可靠区分恶性肿瘤与混淆病理。

Method: GrRAiL首先基于体素放射组学测量聚类亚区域，再计算图论指标构建加权图，编码病灶内高阶空间关系。

Result: 在947例患者中，GrRAiL在胶质母细胞瘤、脑转移瘤和胰腺IPMN分类任务中均显著优于GNN、纹理放射组学等基线方法，测试准确率提升10%-13%。

Conclusion: GrRAiL能有效捕捉病灶内异质性，具有临床可行性，为肿瘤鉴别诊断提供新工具。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: 提出CLOPS，首个仅依赖自我中心视觉感知来生成类人动作的虚拟角色，通过解耦低层动作与高层控制实现训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定的感知，违背人类感知-动作的内在关联，需用类人感知（如自我中心视觉）生成真实行为。

Method: 先在大规模动作捕捉数据上训练动作先验模型，再用Q-learning将自我中心视觉输入映射为高层控制指令，驱动动作模型。

Result: CLOPS能通过视觉避开障碍物，生成具类人特征的运动行为，验证了视觉感知驱动动作的有效性。

Conclusion: 配备类人传感器（如自我中心视觉）有望训练出行为更接近人类的虚拟角色。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 提出OverLayScore指标和OverLayBench基准，解决布局图像生成中重叠边界框的挑战，并提出CreatiLayout-AM模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理边界框高度重叠的布局时生成质量下降，而现有基准偏简单，无法有效评估复杂场景。

Method: 引入OverLayScore量化重叠复杂度，构建OverLayBench基准，并基于无模态掩码数据集微调CreatiLayout-AM模型。

Result: 揭示了现有基准的偏差，新基准覆盖多样重叠级别，CreatiLayout-AM在复杂重叠场景中表现更优。

Conclusion: 为真实复杂场景下的布局生成提供了评估标准与改进方向。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出一种自蒸馏框架，将视频扩散模型的隐式3D知识转化为显式3D高斯点表示，无需多视图数据即可生成静态和动态3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法依赖真实世界多视图数据，而视频扩散模型虽具想象力但仅限2D，难以用于机器人导航等3D交互场景。

Method: 在RGB解码器基础上增加3D高斯点撒布（3DGS）解码器，通过视频扩散模型生成的合成数据监督训练，实现无多视图数据的3D场景生成。

Result: 在静态和动态3D场景生成任务中达到当前最优性能，支持文本或单图输入实时渲染，且可从单目视频生成动态3D场景。

Conclusion: 该框架突破了对真实多视图数据的依赖，为虚拟环境生成提供了高效且灵活的解决方案。

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出一种基于体素对齐高斯的前馈3D高斯溅射方法，提升多视角一致性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有像素对齐的3D高斯溅射方法依赖输入视图数量，易受遮挡和低纹理影响，导致密度偏差和对齐误差。

Method: 通过预测3D体素网格直接生成体素对齐的高斯点云，替代传统像素对齐方式，实现更鲁棒的多视角重建。

Result: 在RealEstate10K和ScanNet上取得SOTA性能，重建更真实、视角一致，且具备更高可扩展性。

Conclusion: VolSplat为前馈3D重建提供了更高效、鲁棒的框架，推动了3D表示与渲染的进一步研究。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: 提出CAR-Flow，通过条件感知重参数化简化流匹配模型的学习负担，显著提升生成性能且参数增加极少。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流方法需同时学习质量传输与条件注入，模型负担重。

Method: CAR-Flow引入轻量级条件感知偏移，调整源分布、目标分布或二者，缩短概率路径。

Result: 在ImageNet-256上，SiT-XL/2的FID从2.07降至1.68，参数仅增加<0.6%。

Conclusion: CAR-Flow有效提升流匹配模型效率与性能，适用于高维图像生成任务。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [124] [PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies](https://arxiv.org/abs/2509.18282)
*Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li*

Main category: cs.RO

TL;DR: PEEK利用视觉语言模型提取关键点表示，使机器人策略仅需专注执行，显著提升零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人操控策略因需同时学习关注位置、动作选择与执行而难以泛化，需分离语义推理与执行控制。

Method: PEEK微调VLM预测统一的点状中间表示：末端执行器路径与任务相关掩码，通过自动化管道在20+数据集上生成标注。

Result: 在真实世界中，PEEK使仅在仿真中训练的3D策略性能提升41.4倍，大/小策略均有2-3.5倍增益。

Conclusion: 将语义复杂性交由VLM处理，PEEK为操控策略提供最小必要线索（where, what, how），实现跨架构泛化。

Abstract: Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.

</details>


### [125] [Fine-Tuning Robot Policies While Maintaining User Privacy](https://arxiv.org/abs/2509.18311)
*Benjamin A. Christie,Sagar Parekh,Dylan P. Losey*

Main category: cs.RO

TL;DR: 提出PRoP框架，通过用户专属密钥实现机器人个性化行为的同时保护隐私，避免外部代理窃取用户偏好数据。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器人策略在个性化微调过程中会泄露用户偏好、习惯等私密数据，外部代理可通过推理策略获取这些信息，亟需隐私保护的个性化方法。

Method: PRoP为每位用户分配唯一密钥，用于数学变换机器人网络权重；正确密钥激活个性化行为，错误密钥则恢复基线行为，不改变原模型架构。

Result: PRoP在模仿学习、强化学习和分类任务中具有通用性，实验表现优于现有编码器方法，且保持原策略行为与结构。

Conclusion: PRoP提供了一种模型无关、实用且高效的隐私保护个性化机器人方案，为未来人机交互中的数据安全提供了新路径。

Abstract: Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.

</details>


### [126] [Haptic Communication in Human-Human and Human-Robot Co-Manipulation](https://arxiv.org/abs/2509.18327)
*Katherine H. Allen,Chris Rogers,Elaine S. Short*

Main category: cs.RO

TL;DR: 通过IMU跟踪人-人和人-机器人协同操作物体的运动，发现人-人协作更流畅，激励机器人未来具备类人触觉通信能力。


<details>
  <summary>Details</summary>
Motivation: 人类在协同操作物体时通过触觉通信传递意图，但机器人缺乏此类能力，导致协作不流畅。

Method: 使用低成本IMU跟踪人类双人协作及人类-机器人协作时共享物体的运动，并结合问卷评估主观体验。

Result: 人-人协作在主观流畅性和客观运动轨迹上均显著优于人-机器人协作。

Conclusion: 机器人应学习接收与发送类人触觉信号，以提升物理协作任务中的表现。

Abstract: When a human dyad jointly manipulates an object, they must communicate about
their intended motion plans. Some of that collaboration is achieved through the
motion of the manipulated object itself, which we call "haptic communication."
In this work, we captured the motion of human-human dyads moving an object
together with one participant leading a motion plan about which the follower is
uninformed. We then captured the same human participants manipulating the same
object with a robot collaborator. By tracking the motion of the shared object
using a low-cost IMU, we can directly compare human-human shared manipulation
to the motion of those same participants interacting with the robot.
Intra-study and post-study questionnaires provided participant feedback on the
collaborations, indicating that the human-human collaborations are
significantly more fluent, and analysis of the IMU data indicates that it
captures objective differences in the motion profiles of the conditions. The
differences in objective and subjective measures of accuracy and fluency
between the human-human and human-robot trials motivate future research into
improving robot assistants for physical tasks by enabling them to send and
receive anthropomorphic haptic signals.

</details>


### [127] [The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020](https://arxiv.org/abs/2509.18330)
*Marsette Vona*

Main category: cs.RO

TL;DR: 融合火星车与轨道器数据生成交互式3D地形可视化


<details>
  <summary>Details</summary>
Motivation: 提升火星探测任务中科学目标规划的精度与效率

Method: 自动构建上下文网格，融合2D/3D图像与轨道高程/色彩数据

Result: 实现交互式3D地形可视化，用于任务规划与公众展示

Conclusion: 该方法有效支持科学决策并促进公众参与火星探索

Abstract: The Landform contextual mesh fuses 2D and 3D data from up to thousands of
Mars 2020 rover images, along with orbital elevation and color maps from Mars
Reconnaissance Orbiter, into an interactive 3D terrain visualization.
Contextual meshes are built automatically for each rover location during
mission ground data system processing, and are made available to mission
scientists for tactical and strategic planning in the Advanced Science
Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also
deployed to the "Explore with Perseverance" public access website.

</details>


### [128] [Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation](https://arxiv.org/abs/2509.18342)
*Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara*

Main category: cs.RO

TL;DR: 提出一种融合语义墙与GPS先验的语义粒子滤波器，提升激光雷达在葡萄园中的定位精度


<details>
  <summary>Details</summary>
Motivation: 传统激光雷达方法因葡萄园行距重复和感知别名效应导致定位失败

Method: 利用葡萄藤主干和支撑杆的语义检测，投影至鸟瞰图并融合激光雷达数据，构建语义墙作为伪结构约束，并在头域引入自适应噪声GPS先验

Result: 在真实葡萄园中实现行内稳定定位，克服AMCL失效情况，优于RTAB-Map等视觉SLAM方法

Conclusion: 语义约束与多源信息融合显著提升复杂农业环境中的机器人定位鲁棒性

Abstract: Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.

</details>


### [129] [AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback](https://arxiv.org/abs/2509.18384)
*Yunhao Yang,Junyuan Hong,Gabriel Jacob Perin,Zhiwen Fan,Li Yin,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: LAD-VF是一种无需微调的框架，利用形式验证反馈自动优化提示词，显著提升大语言模型在机器人任务中的安全合规性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的规划系统常因幻觉或对齐不足而违反安全约束，传统对齐方法依赖昂贵的人工标注或耗时的微调。

Method: 提出LAD-VF框架，结合形式验证反馈与LLM-AutoDiff，通过迭代优化提示词而非模型参数实现安全对齐。

Result: 在机器人导航与操作任务中，成功率从60%提升至90%以上，且无需微调、兼容模块化架构、提示可审计。

Conclusion: LAD-VF为可扩展、可解释、形式验证的可信LLM控制提供了新路径。

Abstract: Large language models (LLMs) can translate natural language instructions into
executable action plans for robotics, autonomous driving, and other domains.
Yet, deploying LLM-driven planning in the physical world demands strict
adherence to safety and regulatory constraints, which current models often
violate due to hallucination or weak alignment. Traditional data-driven
alignment methods, such as Direct Preference Optimization (DPO), require costly
human labeling, while recent formal-feedback approaches still depend on
resource-intensive fine-tuning. In this paper, we propose LAD-VF, a
fine-tuning-free framework that leverages formal verification feedback for
automated prompt engineering. By introducing a formal-verification-informed
text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts
rather than model parameters. This yields three key benefits: (i) scalable
adaptation without fine-tuning; (ii) compatibility with modular LLM
architectures; and (iii) interpretable refinement via auditable prompts.
Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF
substantially enhances specification compliance, improving success rates from
60% to over 90%. Our method thus presents a scalable and interpretable pathway
toward trustworthy, formally-verified LLM-driven control systems.

</details>


### [130] [Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections](https://arxiv.org/abs/2509.18407)
*Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston*

Main category: cs.RO

TL;DR: 提出一种基于POMDP的驾驶辅助框架，提升无信号交叉口的通行安全性，概率规划优于规则基方法。


<details>
  <summary>Details</summary>
Motivation: 无信号交叉口因路权模糊、遮挡和驾驶员行为不可预测导致高事故率，现有系统缺乏对人工驾驶车辆的辅助支持。

Method: 将问题建模为POMDP，使用自定义仿真环境评估四种决策方法：FSM、QMDP、POMCP和DESPOT。

Result: 概率规划方法优于FSM，POMCP最安全（97.5%无碰撞），DESPOT在效率与实时性间平衡。

Conclusion: 不确定性感知规划对驾驶辅助至关重要，未来需融合传感器与环境感知模块以实现实时部署。

Abstract: Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.

</details>


### [131] [Latent Action Pretraining Through World Modeling](https://arxiv.org/abs/2509.18428)
*Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid*

Main category: cs.RO

TL;DR: 提出LAWM框架，通过世界建模自监督预训练潜在动作表示，提升机器人模仿学习在真实场景中的效率与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型依赖大量标注数据且模型庞大，难以在真实场景部署；需更高效、可迁移的预训练方法。

Method: LAWM框架：利用无标签视频数据（机器人或人类操作），通过世界建模学习潜在动作表示，实现自监督预训练，模型无关。

Result: 在LIBERO基准和真实场景中，LAWM超越使用真实动作训练的模型及其他预训练方法，同时显著更高效。

Conclusion: LAWM为机器人模仿学习提供了高效、可迁移的自监督预训练新范式，适合实际部署。

Abstract: Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.

</details>


### [132] [PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction](https://arxiv.org/abs/2509.18447)
*Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: PrioriTouch 是一种用于多接触物理人机交互的优先级排序框架，能够根据个体偏好动态调整力控制，提升安全性与舒适度。


<details>
  <summary>Details</summary>
Motivation: 在护理任务中，多点同时接触会引发力要求冲突，单一控制策略无法满足所有偏好，亟需优先级排序机制。

Method: 结合学习排序与分层操作空间控制，通过模拟滚动进行高效安全探索，并基于用户研究个性化舒适阈值。

Result: 在仿真与真实实验中验证了 PrioriTouch 能有效适应用户偏好，保持任务性能，提升安全与舒适性。

Conclusion: PrioriTouch 为多接触 pHRI 提供了可泛化、自适应的优先级控制框架，适用于护理及其他复杂交互场景。

Abstract: Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.

</details>


### [133] [Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands](https://arxiv.org/abs/2509.18455)
*Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: 提出GD2P方法，利用灵巧手通过几何感知的推拉实现非抓取操作，实验验证其有效性并开源数据与模型。


<details>
  <summary>Details</summary>
Motivation: 传统非抓取操作依赖并联夹爪或工具，难以应对复杂几何对象；灵巧手提供更丰富的接触模式与稳定支持，弥补动力学建模困难。

Method: 将问题建模为合成与学习接触前手部姿态，通过接触引导采样生成姿态，物理仿真过滤，训练基于物体几何的扩散模型预测可行姿态，结合运动规划执行推拉动作。

Result: 在Allegro Hand上完成840次真实实验，性能优于基线；在LEAP Hand上验证跨形态适用性；开源130万姿态、2300物体数据集。

Conclusion: GD2P为灵巧手非抓取操作提供了可扩展的训练路径，具有通用性与实际应用潜力。

Abstract: Nonprehensile manipulation, such as pushing and pulling, enables robots to
move, align, or reposition objects that may be difficult to grasp due to their
geometry, size, or relationship to the robot or the environment. Much of the
existing work in nonprehensile manipulation relies on parallel-jaw grippers or
tools such as rods and spatulas. In contrast, multi-fingered dexterous hands
offer richer contact modes and versatility for handling diverse objects to
provide stable support over the objects, which compensates for the difficulty
of modeling the dynamics of nonprehensile manipulation. Therefore, we propose
Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile
manipulation with dexterous robotic hands. We study pushing and pulling by
framing the problem as synthesizing and learning pre-contact dexterous hand
poses that lead to effective manipulation. We generate diverse hand poses via
contact-guided sampling, filter them using physics simulation, and train a
diffusion model conditioned on object geometry to predict viable poses. At test
time, we sample hand poses and use standard motion planners to select and
execute pushing and pulling actions. We perform 840 real-world experiments with
an Allegro Hand, comparing our method to baselines. The results indicate that
GD2P offers a scalable route for training dexterous nonprehensile manipulation
policies. We further demonstrate GD2P on a LEAP Hand, highlighting its
applicability to different hand morphologies. Our pre-trained models and
dataset, including 1.3 million hand poses across 2.3k objects, will be
open-source to facilitate further research. Our project website is available
at: geodex2p.github.io.

</details>


### [134] [A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems](https://arxiv.org/abs/2509.18460)
*Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh*

Main category: cs.RO

TL;DR: 提出一种基于反事实推理的感知系统故障检测与隔离框架，无需物理冗余，通过因果推理和主动控制提升故障识别效率。


<details>
  <summary>Details</summary>
Motivation: 感知系统故障具有环境相关性且多阶段级联传播，传统冗余方法成本高，需更高效诊断方法。

Method: 采用反事实推理构建感知可靠性测试，结合被动信念更新与主动因果-bandit方法（MCTS+UCB），最大化有效信息（EI）以优化控制输入。

Result: 在空间机器人视觉导航场景中，主动调整姿态可有效隔离传感器损伤、动态场景和感知退化等故障。

Conclusion: 反事实推理驱动的主动FDI框架在无冗余条件下显著提升故障诊断精度与效率。

Abstract: Perception systems provide a rich understanding of the environment for
autonomous systems, shaping decisions in all downstream modules. Hence,
accurate detection and isolation of faults in perception systems is important.
Faults in perception systems pose particular challenges: faults are often tied
to the perceptual context of the environment, and errors in their multi-stage
pipelines can propagate across modules. To address this, we adopt a
counterfactual reasoning approach to propose a framework for fault detection
and isolation (FDI) in perception systems. As opposed to relying on physical
redundancy (i.e., having extra sensors), our approach utilizes analytical
redundancy with counterfactual reasoning to construct perception reliability
tests as causal outcomes influenced by system states and fault scenarios.
Counterfactual reasoning generates reliability test results under hypothesized
faults to update the belief over fault hypotheses. We derive both passive and
active FDI methods. While the passive FDI can be achieved by belief updates,
the active FDI approach is defined as a causal bandit problem, where we utilize
Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find
control inputs that maximize a detection and isolation metric, designated as
Effective Information (EI). The mentioned metric quantifies the informativeness
of control inputs for FDI. We demonstrate the approach in a robot exploration
scenario, where a space robot performing vision-based navigation actively
adjusts its attitude to increase EI and correctly isolate faults caused by
sensor damage, dynamic scenes, and perceptual degradation.

</details>


### [135] [Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task](https://arxiv.org/abs/2509.18463)
*Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel*

Main category: cs.RO

TL;DR: 通过在强化学习奖励函数中引入高斯噪声变异，使机器人在倒液任务中学会多样化技能，包括意外有用的如清洁杯沿、混合液体等新技能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习奖励函数固定，限制了技能多样性；受人类运动控制中的成本-收益权衡启发，希望探索奖励函数变异如何激发更丰富的行为。

Method: 基于NVIDIA Isaac Sim仿真环境，使用Franka Panda机械臂进行倒液任务，采用PPO算法，并对奖励函数中准确性、时间、努力三项的权重添加高斯噪声进行系统性变异。

Result: 变异后的策略展现出广泛行为，不仅包括原任务的多样执行方式，还衍生出清洁杯沿、混合液体、浇灌等新颖技能。

Conclusion: 奖励函数的有意变异为机器人提供了一种无需重新设计目标即可自动学习多样化技能的途径，并可能为未来未知任务生成有意义的迁移技能。

Abstract: This paper explores how deliberate mutations of reward function in
reinforcement learning can produce diversified skill variations in robotic
manipulation tasks, examined with a liquid pouring use case. To this end, we
developed a new reward function mutation framework that is based on applying
Gaussian noise to the weights of the different terms in the reward function.
Inspired by the cost-benefit tradeoff model from human motor control, we
designed the reward function with the following key terms: accuracy, time, and
effort. The study was performed in a simulation environment created in NVIDIA
Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a
glass with a liquid that needed to be poured into a container. The
reinforcement learning algorithm was based on Proximal Policy Optimization. We
systematically explored how different configurations of mutated weights in the
rewards function would affect the learned policy. The resulting policies
exhibit a wide range of behaviours: from variations in execution of the
originally intended pouring task to novel skills useful for unexpected tasks,
such as container rim cleaning, liquid mixing, and watering. This approach
offers promising directions for robotic systems to perform diversified learning
of specific tasks, while also potentially deriving meaningful skills for future
tasks.

</details>


### [136] [RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2509.18466)
*Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao*

Main category: cs.RO

TL;DR: 提出一种强化学习增强的模型预测控制框架，用于双足机器人在复杂地形上的鲁棒行走，显著优于传统MPC和RL方法。


<details>
  <summary>Details</summary>
Motivation: 传统MPC难以建模复杂地形交互，而RL缺乏约束满足保障且依赖大量奖励设计，现有MPC与RL结合方法多限于平地或四足机器人。

Method: 参数化基于单刚体动力学的MPC的三个关键组件：系统动力学、摆动腿控制器和步频，利用RL进行优化。

Result: 在NVIDIA IsaacLab的多种地形（楼梯、踏石、低摩擦表面）仿真中，所提方法比基线MPC和RL表现出更强的适应性和鲁棒性。

Conclusion: RL-Augmented MPC为双足机器人在复杂地形中实现高效、安全运动提供了有效方案，兼顾MPC的约束可靠性和RL的环境适应性。

Abstract: Model predictive control (MPC) has demonstrated effectiveness for humanoid
bipedal locomotion; however, its applicability in challenging environments,
such as rough and slippery terrain, is limited by the difficulty of modeling
terrain interactions. In contrast, reinforcement learning (RL) has achieved
notable success in training robust locomotion policies over diverse terrain,
yet it lacks guarantees of constraint satisfaction and often requires
substantial reward shaping. Recent efforts in combining MPC and RL have shown
promise of taking the best of both worlds, but they are primarily restricted to
flat terrain or quadrupedal robots. In this work, we propose an RL-augmented
MPC framework tailored for bipedal locomotion over rough and slippery terrain.
Our method parametrizes three key components of
single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,
and gait frequency. We validate our approach through bipedal robot simulations
in NVIDIA IsaacLab across various terrains, including stairs, stepping stones,
and low-friction surfaces. Experimental results demonstrate that our
RL-augmented MPC framework produces significantly more adaptive and robust
behaviors compared to baseline MPC and RL.

</details>


### [137] [Spatial Envelope MPC: High Performance Driving without a Reference](https://arxiv.org/abs/2509.18506)
*Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal*

Main category: cs.RO

TL;DR: 提出一种无需预定义轨迹的基于包络的模型预测控制框架，实现自动驾驶车辆在高动态场景下的高性能操控。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考轨迹的规划与控制方法在车辆接近动力学极限时表现不佳，难以应对无预定义轨迹的复杂场景。

Method: 构建高效车辆动力学模型与连续可微包络数学描述，结合强化学习与优化技术，直接在安全可行驶包络内进行规划与控制。

Result: 通过仿真与实车实验验证，该框架在竞速、紧急避障和越野导航等任务中均表现出高性能与强鲁棒性。

Conclusion: 该框架实现了无参考轨迹的高动态自动驾驶，具有广阔的应用前景与可扩展性。

Abstract: This paper presents a novel envelope based model predictive control (MPC)
framework designed to enable autonomous vehicles to handle high performance
driving across a wide range of scenarios without a predefined reference. In
high performance autonomous driving, safe operation at the vehicle's dynamic
limits requires a real time planning and control framework capable of
accounting for key vehicle dynamics and environmental constraints when
following a predefined reference trajectory is suboptimal or even infeasible.
State of the art planning and control frameworks, however, are predominantly
reference based, which limits their performance in such situations. To address
this gap, this work first introduces a computationally efficient vehicle
dynamics model tailored for optimization based control and a continuously
differentiable mathematical formulation that accurately captures the entire
drivable envelope. This novel model and formulation allow for the direct
integration of dynamic feasibility and safety constraints into a unified
planning and control framework, thereby removing the necessity for predefined
references. The challenge of envelope planning, which refers to maximally
approximating the safe drivable area, is tackled by combining reinforcement
learning with optimization techniques. The framework is validated through both
simulations and real world experiments, demonstrating its high performance
across a variety of tasks, including racing, emergency collision avoidance and
off road navigation. These results highlight the framework's scalability and
broad applicability across a diverse set of scenarios.

</details>


### [138] [LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA](https://arxiv.org/abs/2509.18576)
*Zeyi Kang,Liang He,Yanxin Zhang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 提出轻量级LCMF框架，实现高效多模态融合，在VQA和EQA任务中表现优异且计算资源需求低。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在资源受限环境下多模态数据融合效率低和计算开销大的问题。

Method: 在Mamba模块中引入多层次跨模态参数共享机制，结合交叉注意力与选择性参数共享状态空间模型（SSMs）。

Result: VQA准确率74.29%，EQA性能达大语言模型代理中游水平，FLOPs减少4.35倍，参数量为166.51M（图像-文本）和219M（视频-文本）。

Conclusion: LCMF为资源受限的人机交互场景提供了高效、强泛化能力的多模态决策解决方案。

Abstract: Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.

</details>


### [139] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: VLN-Zero通过视觉-语言模型和符号推理实现零样本导航，显著提升在未知环境中的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖耗尽式探索或僵化策略，难以泛化至未知环境，缺乏高效与可扩展的导航能力。

Method: 提出双阶段框架：探索阶段用结构化提示生成符号场景图，部署阶段用神经符号规划器结合缓存机制执行导航任务。

Result: 相比最优零样本模型，成功率提升2倍，导航时间减半，VLM调用减少55%，并超越多数微调基线。

Conclusion: VLN-Zero通过融合快速探索、符号推理与缓存执行，实现了在未知环境中的鲁棒、高效、可扩展的视觉-语言导航。

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [140] [Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](https://arxiv.org/abs/2509.18597)
*Yuan Meng,Zhenguo Sun,Max Fest,Xukun Li,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 提出一种人机协作框架，通过外部记忆和检索增强生成将修正编码为可复用技能，大幅提升长任务成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码生成方法存在噪音大、原语固定、上下文受限、长期任务表现差及修正知识格式不佳导致遗忘的问题。

Method: 引入人机协作框架，将人类修正编码为可复用技能，结合外部记忆与带提示机制的检索增强生成（RAG）实现动态复用。

Result: 在Ravens、Franka Kitchen、MetaWorld及真实场景中，成功率达0.93（提升27%），修正轮次效率提升42%，可完成超20步的复杂长任务如“建房子”。

Conclusion: 该框架有效解决LLM在机器人长任务中的推理瓶颈，实现可泛化、可持续学习的技能复用。

Abstract: Large language models (LLMs)-based code generation for robotic manipulation
has recently shown promise by directly translating human instructions into
executable code, but existing methods remain noisy, constrained by fixed
primitives and limited context windows, and struggle with long-horizon tasks.
While closed-loop feedback has been explored, corrected knowledge is often
stored in improper formats, restricting generalization and causing catastrophic
forgetting, which highlights the need for learning reusable skills. Moreover,
approaches that rely solely on LLM guidance frequently fail in extremely
long-horizon scenarios due to LLMs' limited reasoning capability in the robotic
domain, where such issues are often straightforward for humans to identify. To
address these challenges, we propose a human-in-the-loop framework that encodes
corrections into reusable skills, supported by external memory and
Retrieval-Augmented Generation with a hint mechanism for dynamic reuse.
Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world
settings, show that our framework achieves a 0.93 success rate (up to 27%
higher than baselines) and a 42% efficiency improvement in correction rounds.
It can robustly solve extremely long-horizon tasks such as "build a house",
which requires planning over 20 primitives.

</details>


### [141] [End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2509.18608)
*Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出一种基于深度强化学习的端到端导航系统，直接从LiDAR数据生成控制命令，在仿真中实现100%直线行种植成功率


<details>
  <summary>Details</summary>
Motivation: 温室或林下农业环境因GNSS不可靠、植被杂乱和光照变化导致导航困难

Method: 使用基于体素的降采样策略将LiDAR数据减少95.83%，结合仿真训练的深度强化学习策略直接映射原始点云到控制指令

Result: 仿真中直线行的成功率达100%，随着行弯曲度增加性能逐步下降，已在不同正弦频率和幅值下验证

Conclusion: 该方法无需标注数据或手写控制接口，可高效实现复杂农业环境下的自主导航

Abstract: Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.

</details>


### [142] [PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving](https://arxiv.org/abs/2509.18609)
*Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr*

Main category: cs.RO

TL;DR: PIE框架通过融合感知、推理与意图建模，在无需集成和数据增强的情况下，实现了卓越的端到端运动规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶规划面临场景理解不足和预测效果不佳的挑战，阻碍其大规模部署。

Method: PIE提出双向Mamba融合、推理增强解码器（Mamba+MoE）和动作-运动交互模块，优化多模态感知与轨迹推理。

Result: 在NAVSIM基准上，PIE无需集成和数据增强即达到88.9 PDM和85.6 EPDM分数，超越现有最优方法。

Conclusion: PIE能稳定生成符合场景的高质量轨迹，是高效端到端规划的有效解决方案。

Abstract: End-to-end motion planning is promising for simplifying complex autonomous
driving pipelines. However, challenges such as scene understanding and
effective prediction for decision-making continue to present substantial
obstacles to its large-scale deployment. In this paper, we present PIE, a
pioneering framework that integrates advanced perception, reasoning, and
intention modeling to dynamically capture interactions between the ego vehicle
and surrounding agents. It incorporates a bidirectional Mamba fusion that
addresses data compression losses in multimodal fusion of camera and LiDAR
inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and
Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize
adaptive trajectory inference. PIE adopts an action-motion interaction module
to effectively utilize state predictions of surrounding agents to refine ego
planning. The proposed framework is thoroughly validated on the NAVSIM
benchmark. PIE, without using any ensemble and data augmentation techniques,
achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of
prior state-of-the-art methods. Comprehensive quantitative and qualitative
analyses demonstrate that PIE is capable of reliably generating feasible and
high-quality ego trajectories.

</details>


### [143] [SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones](https://arxiv.org/abs/2509.18610)
*Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager*

Main category: cs.RO

TL;DR: SINGER实现仅靠机载感知的开放词汇无人机语言导航，通过仿真生成数据与轻量级视觉运动策略，在零样本迁移中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开放词汇无人机导航面临演示数据稀缺、实时控制要求高和外部位姿估计不可靠的挑战。

Method: 使用基于高斯点云的仿真器生成语言嵌入飞行数据，结合RRT启发的多轨迹专家生成导航演示，训练轻量端到端视觉运动策略。

Result: 在70万至百万级数据训练下，SINGER比速度控制基线平均达到目标高23.33%，保持目标在视野内高16.67%，碰撞率降低10%。

Conclusion: SINGER在无外部定位条件下实现了高效、可靠的语言引导无人机自主导航，具备优秀的零样本泛化能力。

Abstract: Large vision-language models have driven remarkable progress in
open-vocabulary robot policies, e.g., generalist robot manipulation policies,
that enable robots to complete complex tasks specified in natural language.
Despite these successes, open-vocabulary autonomous drone navigation remains an
unsolved challenge due to the scarcity of large-scale demonstrations, real-time
control demands of drones for stabilization, and lack of reliable external pose
estimation modules. In this work, we present SINGER for language-guided
autonomous drone navigation in the open world using only onboard sensing and
compute. To train robust, open-vocabulary navigation policies, SINGER leverages
three central components: (i) a photorealistic language-embedded flight
simulator with minimal sim-to-real gap using Gaussian Splatting for efficient
data generation, (ii) an RRT-inspired multi-trajectory generation expert for
collision-free navigation demonstrations, and these are used to train (iii) a
lightweight end-to-end visuomotor policy for real-time closed-loop control.
Through extensive hardware flight experiments, we demonstrate superior
zero-shot sim-to-real transfer of our policy to unseen environments and unseen
language-conditioned goal objects. When trained on ~700k-1M observation action
pairs of language conditioned visuomotor data and deployed on hardware, SINGER
outperforms a velocity-controlled semantic guidance baseline by reaching the
query 23.33% more on average, and maintains the query in the field of view
16.67% more on average, with 10% fewer collisions.

</details>


### [144] [The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving](https://arxiv.org/abs/2509.18626)
*Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone*

Main category: cs.RO

TL;DR: 通过将车祸叙述标准化并统一场景-动作表征，提升自动驾驶系统在安全边界处的决策校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统主要基于无事故数据训练，缺乏对高风险场景的指导；真实车祸报告包含重要对比证据但难以利用。

Method: 将车祸叙述转化为以自我为中心的语言，统一日志与车祸为场景-动作表示，并通过检索先例及反事实推理辅助决策。

Result: 在nuScenes基准上，先例检索使上下文优选动作的召回率从24%提升至53%，反事实变体进一步增强风险区域决策精度。

Conclusion: 利用真实车祸数据进行检索与反事实推理，可显著提升自动驾驶系统在安全边界处的决策可靠性与校准性。

Abstract: Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.

</details>


### [145] [Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training](https://arxiv.org/abs/2509.18631)
*Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu*

Main category: cs.RO

TL;DR: 提出一种仿真与真实数据协同训练框架，利用少量真实演示大幅提升机器人操作策略的泛化能力，成功率达30%提升。


<details>
  <summary>Details</summary>
Motivation: 真实演示获取成本高，仿真数据虽可扩展但存在域差距，导致策略难以迁移到现实。

Method: 引入基于最优传输（OT）的损失函数，对齐观测与动作的联合分布，扩展为非平衡OT处理仿真与真实数据量不平衡问题。

Result: 在复杂操作任务中，仅需少量真实演示，即实现最高30%的真实世界成功率提升，并能泛化至仅在仿真中见过的场景。

Conclusion: 联合分布对齐比单独对齐观测更有效，该框架显著提升仿真到现实的迁移性能，为低成本高鲁棒性机器人学习提供新路径。

Abstract: Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.

</details>


### [146] [Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments](https://arxiv.org/abs/2509.18636)
*Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao*

Main category: cs.RO

TL;DR: 该论文提出一种基于可变形虚拟结构（DVS）的编队规划方法，支持在狭窄环境中动态增减无人机数量并快速恢复编队形状。


<details>
  <summary>Details</summary>
Motivation: 在狭窄环境中，无人机编队数量变化导致传统规划方法难以收敛到期望构型。

Method: 采用PAAS算法（Lloyd分区+Hungarian分配）维持编队完整性，结合基元搜索与非线性优化规划DVS时空轨迹，并通过仿射变换适应环境，各无人机分布式执行轨迹规划并保障避碰与动力学可行性。

Result: 仿真中支持最多15%无人机增减并快速恢复编队，实验验证了方法在真实环境中的有效性与鲁棒性，优于现有前沿方法。

Conclusion: 所提DVS引导的编队规划方法在动态环境和数量变化下具有优异的适应性、收敛速度与实用性。

Abstract: Formation maintenance with varying number of drones in narrow environments
hinders the convergence of planning to the desired configurations. To address
this challenge, this paper proposes a formation planning method guided by
Deformable Virtual Structures (DVS) with continuous spatiotemporal
transformation. Firstly, to satisfy swarm safety distance and preserve
formation shape filling integrity for irregular formation geometries, we employ
Lloyd algorithm for uniform $\underline{PA}$rtitioning and Hungarian algorithm
for $\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal
trajectory involving DVS is planned using primitive-based path search and
nonlinear trajectory optimization. The DVS trajectory achieves adaptive
transitions with respect to a varying number of drones while ensuring
adaptability to narrow environments through affine transformation. Finally,
each agent conducts distributed trajectory planning guided by desired
spatiotemporal positions within the DVS, while incorporating collision
avoidance and dynamic feasibility requirements. Our method enables up to 15\%
of swarm numbers to join or leave in cluttered environments while rapidly
restoring the desired formation shape in simulation. Compared to cutting-edge
formation planning method, we demonstrate rapid formation recovery capacity and
environmental adaptability. Real-world experiments validate the effectiveness
and resilience of our formation planning method.

</details>


### [147] [Do You Need Proprioceptive States in Visuomotor Policies?](https://arxiv.org/abs/2509.18644)
*Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: 无需本体感觉输入，仅靠视觉观察的无状态策略显著提升机器人操作的空间泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统视觉+本体感觉策略过度依赖本体输入，导致过拟合与弱泛化

Method: 提出无状态策略，仅用双广角腕部摄像头视觉输入，在相对末端执行器动作空间中预测动作

Result: 在拾放、叠 shirt、全身操作等任务中，高度泛化成功率从0%提升至85%，水平泛化从6%提升至64%，同时提升数据效率与跨机器人适配性

Conclusion: 无状态视觉策略是实现强泛化与实用部署的有效途径，优于传统依赖本体感觉的方法

Abstract: Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.

</details>


### [148] [SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](https://arxiv.org/abs/2509.18648)
*Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause*

Main category: cs.RO

TL;DR: SPiDR通过悲观域随机化实现安全的仿真到现实迁移，兼具可扩展性与理论保障。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法难以兼容可扩展训练管道，而域随机化虽简单常用却常导致现实中的不安全行为。

Method: SPiDR将仿真-现实差距的不确定性纳入安全约束，采用悲观域随机化策略，在保持兼容性的同时提供理论安全保障。

Result: 在仿真-仿真基准和两个真实机器人平台上的实验表明，SPiDR能有效确保安全性，同时保持高性能。

Conclusion: SPiDR是实现安全、可扩展仿真实现迁移的有效方法，为现实部署提供了可行路径。

Abstract: Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.

</details>


### [149] [Distributionally Robust Safe Motion Planning with Contextual Information](https://arxiv.org/abs/2509.18666)
*Kaizer Rahaman,Simran Kumari,Ashish R. Hota*

Main category: cs.RO

TL;DR: 提出一种融合上下文信息的分布鲁棒碰撞避免方法，通过RKHS中的条件核均值嵌入构建模糊集，提升运动规划的安全性


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑障碍物轨迹的上下文依赖性与分布不确定性，导致在复杂场景中碰撞避免效果不足

Method: 利用条件核均值嵌入将障碍物未来轨迹的条件分布嵌入RKHS，构建基于经验估计的模糊集，并将其纳入滚动时域运动规划约束中

Result: 在多个挑战性场景中，所提方法相比不考虑上下文或分布鲁棒性的方法具有更高的碰撞避免成功率

Conclusion: 引入上下文信息与分布鲁棒性可显著提升自主系统在不确定环境中的安全规划能力

Abstract: We present a distributionally robust approach for collision avoidance by
incorporating contextual information. Specifically, we embed the conditional
distribution of future trajectory of the obstacle conditioned on the motion of
the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional
kernel mean embedding operator. Then, we define an ambiguity set containing all
distributions whose embedding in the RKHS is within a certain distance from the
empirical estimate of conditional mean embedding learnt from past data.
Consequently, a distributionally robust collision avoidance constraint is
formulated, and included in the receding horizon based motion planning
formulation of the ego agent. Simulation results show that the proposed
approach is more successful in avoiding collision compared to approaches that
do not include contextual information and/or distributional robustness in their
formulation in several challenging scenarios.

</details>


### [150] [N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout](https://arxiv.org/abs/2509.18671)
*Kaixin Chai,Hyunjun Lee,Joseph J. Lim*

Main category: cs.RO

TL;DR: N2M是一种引导机器人在到达任务区域后调整至理想初始位姿的过渡模块，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 移动操作中操作策略对初始位姿有强烈偏好，但导航模块仅关注到达任务区域，未考虑下游操作所需的最佳位姿，导致两者不匹配。

Method: 提出N2M模块，仅依赖自中心观测，实现实时、鲁棒、数据高效的初始位姿调整，无需全局或历史信息。

Result: 在PnPCounterToCab任务中，成功率从3%提升至54%；在Toybox Handover任务中，仅用15个样本即可在未见环境中稳定预测。

Conclusion: N2M在多种任务、机器人硬件和环境中展现出强通用性、高数据效率和优异泛化能力，有效解决导航与操作间的位姿不匹配问题。

Abstract: In mobile manipulation, the manipulation policy has strong preferences for
initial poses where it is executed. However, the navigation module focuses
solely on reaching the task area, without considering which initial pose is
preferable for downstream manipulation. To address this misalignment, we
introduce N2M, a transition module that guides the robot to a preferable
initial pose after reaching the task area, thereby substantially improving task
success rates. N2M features five key advantages: (1) reliance solely on
ego-centric observation without requiring global or historical information; (2)
real-time adaptation to environmental changes; (3) reliable prediction with
high viewpoint robustness; (4) broad applicability across diverse tasks,
manipulation policies, and robot hardware; and (5) remarkable data efficiency
and generalizability. We demonstrate the effectiveness of N2M through extensive
simulation and real-world experiments. In the PnPCounterToCab task, N2M
improves the averaged success rate from 3% with the reachability-based baseline
to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable
predictions even in unseen environments with only 15 data samples, showing
remarkable data efficiency and generalizability.

</details>


### [151] [3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space](https://arxiv.org/abs/2509.18676)
*Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee*

Main category: cs.RO

TL;DR: 3D FDP 使用三维光流作为中间表示，提升机器人在复杂接触任务中的泛化能力，在仿真和真实机器人上均达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视局部运动线索，难以应对多样化的物体和交互动力学，导致泛化性差。

Method: 提出3D Flow Diffusion Policy，利用场景级3D光流作为结构化中间表示，通过扩散架构联合预测查询点轨迹并条件化动作生成。

Result: 在MetaWorld的50个任务中取得SOTA，尤其在中等和困难任务上表现突出；在8个真实机器人任务中也显著优于基线方法。

Conclusion: 3D光流是学习可泛化视觉运动策略的强大结构先验，有助于提升机器人操作的鲁棒性与灵活性。

Abstract: Learning robust visuomotor policies that generalize across diverse objects
and interaction dynamics remains a central challenge in robotic manipulation.
Most existing approaches rely on direct observation-to-action mappings or
compress perceptual inputs into global or object-centric features, which often
overlook localized motion cues critical for precise and contact-rich
manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework
that leverages scene-level 3D flow as a structured intermediate representation
to capture fine-grained local motion cues. Our approach predicts the temporal
trajectories of sampled query points and conditions action generation on these
interaction-aware flows, implemented jointly within a unified diffusion
architecture. This design grounds manipulation in localized dynamics while
enabling the policy to reason about broader scene-level consequences of
actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP
achieves state-of-the-art performance across 50 tasks, particularly excelling
on medium and hard settings. Beyond simulation, we validate our method on eight
real-robot tasks, where it consistently outperforms prior baselines in
contact-rich and non-prehensile scenarios. These results highlight 3D flow as a
powerful structural prior for learning generalizable visuomotor policies,
supporting the development of more robust and versatile robotic manipulation.
Robot demonstrations, additional results, and code can be found at
https://sites.google.com/view/3dfdp/home.

</details>


### [152] [Query-Centric Diffusion Policy for Generalizable Robotic Assembly](https://arxiv.org/abs/2509.18686)
*Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao*

Main category: cs.RO

TL;DR: 提出Query-centric Diffusion Policy (QDP)，通过基于对象、接触点和技能的查询桥接高层规划与底层控制，显著提升机器人装配任务的精度和成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人装配任务因部件交互复杂且对接触噪声敏感，传统分层策略存在高层指令与底层执行不匹配的问题。

Method: QDP采用查询中心机制，利用点云观测识别任务相关组件，将对象、接触点和技能信息作为查询引导底层策略，结合扩散模型实现高精度控制。

Result: 在FurnitureBench仿真与真实环境中，QDP在插装和拧螺丝任务中比无结构查询的基线方法技能成功率提升超50%，长序列任务成功率显著提高。

Conclusion: QDP通过结构化查询有效整合高层规划与底层控制，为复杂装配任务提供了鲁棒且高效的解决方案。

Abstract: The robotic assembly task poses a key challenge in building generalist robots
due to the intrinsic complexity of part interactions and the sensitivity to
noise perturbations in contact-rich settings. The assembly agent is typically
designed in a hierarchical manner: high-level multi-part reasoning and
low-level precise control. However, implementing such a hierarchical policy is
challenging in practice due to the mismatch between high-level skill queries
and low-level execution. To address this, we propose the Query-centric
Diffusion Policy (QDP), a hierarchical framework that bridges high-level
planning and low-level control by utilizing queries comprising objects, contact
points, and skill information. QDP introduces a query-centric mechanism that
identifies task-relevant components and uses them to guide low-level policies,
leveraging point cloud observations to improve the policy's robustness. We
conduct comprehensive experiments on the FurnitureBench in both simulation and
real-world settings, demonstrating improved performance in skill precision and
long-horizon success rate. In the challenging insertion and screwing tasks, QDP
improves the skill-wise success rate by over 50% compared to baselines without
structured queries.

</details>


### [153] [Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation](https://arxiv.org/abs/2509.18734)
*Nishant Doshi,Amey Sutvani,Sanket Gujar*

Main category: cs.RO

TL;DR: 提出使用强化学习训练虚拟四旋翼无人机，借助深度摄像头在模拟城市环境中实现避障导航。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GPS精度下降、空间狭窄和动态障碍物使航路规划复杂，需依赖机载深度传感器实现可靠避障。

Method: 采用强化学习方法训练虚拟四旋翼代理，配备深度摄像头，在模拟城市环境中学习导航与避障策略。

Result: 未提供具体结果，仅说明方法框架。

Conclusion: 该方法有望提升无人机在复杂城市环境中的自主导航能力。

Abstract: One of the challenges faced by Autonomous Aerial Vehicles is reliable
navigation through urban environments. Factors like reduction in precision of
Global Positioning System (GPS), narrow spaces and dynamically moving obstacles
make the path planning of an aerial robot a complicated task. One of the skills
required for the agent to effectively navigate through such an environment is
to develop an ability to avoid collisions using information from onboard depth
sensors. In this paper, we propose Reinforcement Learning of a virtual
quadcopter robot agent equipped with a Depth Camera to navigate through a
simulated urban environment.

</details>


### [154] [MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning](https://arxiv.org/abs/2509.18757)
*Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka*

Main category: cs.RO

TL;DR: MV-UMI通过融合第三人称视角提升手持抓取器的数据收集效果，显著提高机器人模仿学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有手持抓取器依赖第一人称视角，缺乏场景上下文，限制了复杂操作任务的学习；传统机器人数据收集成本高且缺乏泛化性。

Method: 提出MV-UMI框架，结合第三人称视角与_egocentric_腕部摄像头，缓解人-机域偏移，保留跨本体优势。

Result: 在3个任务中，对需要全局理解的子任务性能提升约47%，验证了方法有效性。

Conclusion: MV-UMI在不牺牲手持设备泛化性前提下，扩展了可学习的操纵任务范围。

Abstract: Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.

</details>


### [155] [VGGT-DP: Generalizable Robot Control via Vision Foundation Models](https://arxiv.org/abs/2509.18778)
*Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang*

Main category: cs.RO

TL;DR: 提出VGGT-DP，结合3D视觉先验与本体感知，提升机器人视觉模仿学习的时空理解与泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有视觉编码器忽视结构与容量，导致空间理解不足和泛化受限，受生物视觉系统启发，需融合视觉与本体感知信息

Method: 采用VGGT作为视觉编码器，引入本体感知引导的视觉学习策略，结合帧级令牌重用与随机令牌剪枝以提升效率与鲁棒性

Result: 在MetaWorld挑战任务中显著超越DP和DP3基线，尤其在高精度和长任务场景表现优异

Conclusion: 融合几何先验与本体反馈的视觉-运动策略可有效提升机器人模仿学习的空间 grounding 和闭环控制性能

Abstract: Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.

</details>


### [156] [Human-Interpretable Uncertainty Explanations for Point Cloud Registration](https://arxiv.org/abs/2509.18786)
*Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel*

Main category: cs.RO

TL;DR: 提出GP-CA方法，量化并解释点云配准中的不确定性，通过主动学习发现新误差源，在多数据集和真实机器人实验中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统方法如ICP在传感器噪声、位姿估计误差和部分重叠等不确定性下表现不佳，亟需能量化并解释不确定性的新方法。

Method: 提出高斯过程概念归因（GP-CA），结合主动学习，量化配准不确定性并将其归因于已知误差源，同时发现新误差源。

Result: 在三个公开数据集和真实机器人实验中，GP-CA在运行效率、样本效率和准确性上超越现有方法，并实现有效的故障恢复。

Conclusion: GP-CA不仅提升了点云配准的鲁棒性和准确性，还通过可解释性增强机器人感知系统的可靠性。

Abstract: In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.

</details>


### [157] [Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations](https://arxiv.org/abs/2509.18793)
*Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein*

Main category: cs.RO

TL;DR: 本文提出一种基于Kubernetes和ROS 2的按需应用管理框架，用于动态协调车载微服务，提升C-ITS资源利用率。


<details>
  <summary>Details</summary>
Motivation: C-ITS环境动态性强，传统资源管理难以高效响应多实体需求，导致计算与网络资源浪费。

Method: 利用Kubernetes与ROS 2构建按需驱动的应用管理框架，实现微服务的自动部署、重配置、更新与扩展。

Result: 在集体环境感知用例中验证了框架有效性，显著降低资源消耗与网络流量。

Conclusion: 该框架为大规模C-ITS提供了可扩展、动态响应的云原生应用管理解决方案。

Abstract: Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .

</details>


### [158] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: DexSkin是一种柔软、可贴合的电容式电子皮肤，可用于机器人抓取器表面，实现高灵敏度、可校准的触觉感知，并支持强化学习与任务迁移。


<details>
  <summary>Details</summary>
Motivation: 现有机器人触觉系统难以在复杂曲面上实现全覆盖、高灵敏度的触觉感知，限制了精细操作任务的学习与执行。

Method: 提出DexSkin电容式电子皮肤，贴合平行夹爪手指表面，结合示教学习与在线强化学习，实现对象再定向和弹性带包裹等复杂操作。

Result: DexSkin实现了全指表面触觉感知，支持跨传感器校准与模型迁移，成功完成多个接触密集型操作任务，并应用于真实机器人在线强化学习。

Conclusion: DexSkin为机器人触觉感知提供了实用、可扩展的解决方案，显著提升了接触密集型操作任务的学习与执行能力。

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [159] [Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation](https://arxiv.org/abs/2509.18865)
*Masato Kobayashi,Thanpimon Buamanee*

Main category: cs.RO

TL;DR: 提出Bi-VLA框架，通过视觉-语言融合扩展双边控制模仿学习，支持多任务通用模型，提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统双边控制方法需任务特定模型，泛化能力差，难以处理多任务。

Method: 结合机器人关节角、速度、力矩数据与视觉特征、自然语言指令，采用SigLIP和FiLM融合机制。

Result: 在真实机器人上验证，Bi-VLA能有效理解视觉-语言组合，显著提高两种任务（需语言辅助和仅靠视觉）的成功率。

Conclusion: Bi-VLA突破了传统方法的单任务限制，实证表明视觉与语言融合大幅提升模型通用性。

Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language
Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral
control-based imitation learning to handle more than one task within a single
model. Conventional bilateral control methods exploit joint angle, velocity,
torque, and vision for precise manipulation but require task-specific models,
limiting their generality. Bi-VLA overcomes this limitation by utilizing robot
joint angle, velocity, and torque data from leader-follower bilateral control
with visual features and natural language instructions through SigLIP and
FiLM-based fusion. We validated Bi-VLA on two task types: one requiring
supplementary language cues and another distinguishable solely by vision.
Real-robot experiments showed that Bi-VLA successfully interprets
vision-language combinations and improves task success rates compared to
conventional bilateral control-based imitation learning. Our Bi-VLA addresses
the single-task limitation of prior bilateral approaches and provides empirical
evidence that combining vision and language significantly enhances versatility.
Experimental results validate the effectiveness of Bi-VLA in real-world tasks.
For additional material, please visit the website:
https://mertcookimg.github.io/bi-vla/

</details>


### [160] [Lang2Morph: Language-Driven Morphological Design of Robotic Hands](https://arxiv.org/abs/2509.18937)
*Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes*

Main category: cs.RO

TL;DR: 提出Lang2Morph，首个基于大语言模型的机器人手形态生成框架，通过自然语言描述自动生成可3D打印的任务特异性手部结构。


<details>
  <summary>Details</summary>
Motivation: 现有开源设计工具依赖专家经验和手动调参，自动化方法计算成本高且难适用于灵巧手设计，亟需一种高效、低门槛的设计方法。

Method: Lang2Morph框架包含两个模块：(1) 形态设计：将自然语言任务描述转化为语义标签、结构语法和OPH兼容参数；(2) 选择与精化：基于语义一致性与尺寸兼容性评估候选设计，必要时由LLM引导优化。

Result: 在多样化任务中生成了丰富且任务相关的手部形态，验证了方法的可行性和多样性，为首次实现LLM驱动的机器人手任务条件设计。

Conclusion: LLM可有效用于机器人手形态的零样本设计推理，为自动化、语义驱动的机器人设计开辟新路径。

Abstract: Designing robotic hand morphologies for diverse manipulation tasks requires
balancing dexterity, manufacturability, and task-specific functionality. While
open-source frameworks and parametric tools support reproducible design, they
still rely on expert heuristics and manual tuning. Automated methods using
optimization are often compute-intensive, simulation-dependent, and rarely
target dexterous hands. Large language models (LLMs), with their broad
knowledge of human-object interactions and strong generative capabilities,
offer a promising alternative for zero-shot design reasoning. In this paper, we
present Lang2Morph, a language-driven pipeline for robotic hand design. It uses
LLMs to translate natural-language task descriptions into symbolic structures
and OPH-compatible parameters, enabling 3D-printable task-specific
morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks
into semantic tags, structural grammars, and OPH-compatible parameters; and
(ii) Selection and Refinement, which evaluates design candidates based on
semantic alignment and size compatibility, and optionally applies LLM-guided
refinement when needed. We evaluate Lang2Morph across varied tasks, and results
show that our approach can generate diverse, task-relevant morphologies. To our
knowledge, this is the first attempt to develop an LLM-based framework for
task-conditioned robotic hand design.

</details>


### [161] [Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](https://arxiv.org/abs/2509.18953)
*Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao*

Main category: cs.RO

TL;DR: Eva-VLA首次系统评估VLA模型在真实物理变化下的鲁棒性，发现其在多种扰动下失败率超60%，最高达97.8%，并提出连续优化框架以高效发现最坏场景。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在实验室表现良好，但对真实世界中的物理变化（如物体位姿、光照、对抗补丁）鲁棒性不足，缺乏系统性评估方法。

Method: 将离散物理变化转化为连续优化问题，构建Eva-VLA框架，从物体3D变换、光照变化和对抗补丁三个维度系统评估VLA模型。

Result: 所有测试的OpenVLA模型在各类扰动下失败率均超60%，物体变换在长任务中失败率高达97.8%。

Conclusion: Eva-VLA揭示了VLA模型从实验室到部署的重大差距，为提升其真实环境鲁棒性提供了可扩展的评估与改进路径。

Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.

</details>


### [162] [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](https://arxiv.org/abs/2509.18954)
*Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.RO

TL;DR: 提出一种数据驱动方法，无需参考地图即可预测ICP的注册误差协方差，提升LiDAR定位与SLAM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统ICP在无特征或动态场景中误差大，现有不确定性估计方法依赖手工模型或预建地图，且无法准确建模不确定性。

Method: 利用深度学习直接从LiDAR扫描中预测6自由度误差协方差，无需预建地图，可集成至卡尔曼滤波。

Result: 在KITTI数据集上验证，方法能准确预测协方差，并降低定位误差，提升SLAM与定位的鲁棒性。

Conclusion: 该方法开创了无需地图的ICP不确定性建模新路径，显著提升视觉-激光融合定位系统的可靠性。

Abstract: LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.

</details>


### [163] [Category-Level Object Shape and Pose Estimation in Less Than a Millisecond](https://arxiv.org/abs/2509.18979)
*Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone*

Main category: cs.RO

TL;DR: 提出一种快速局部求解器，仅需类别级先验即可高效估计物体形状与位姿，并提供全局最优性证明。


<details>
  <summary>Details</summary>
Motivation: 形状与位姿估计是机器人领域的基础问题，现有方法往往计算复杂或缺乏最优性保证，亟需高效且可验证的解决方案。

Method: 使用学习前端检测语义关键点，采用线性主动形状模型表示形状，通过单位四元数建模位姿，构建MAP优化问题；利用自洽场迭代求解，每轮仅需计算4x4矩阵的最小特征值对，并通过拉格朗日乘子验证全局最优。

Result: 单次迭代耗时约100微秒，支持快速离群点剔除，在合成数据、公开数据集及无人机追踪场景中表现良好。

Conclusion: 该方法实现了高效、可验证的形状与位姿联合估计，具有实现实时应用的潜力。

Abstract: Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.

</details>


### [164] [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)
*Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou*

Main category: cs.RO

TL;DR: 本文综述了视觉语言动作（VLA）模型的最新进展，系统分类了多种方法并展望了未来方向。


<details>
  <summary>Details</summary>
Motivation: 将视觉语言模型从被动序列生成器转变为能进行操控与决策的主动智能体，推动通用机器人发展。

Method: 对超过300篇研究进行系统梳理，将VLA方法分为自回归、扩散、强化学习、混合与专用五类，并分析其动机、策略与实现。

Result: 构建了完整的VLA技术图谱，涵盖数据集、基准和仿真平台，并总结当前挑战。

Conclusion: VLA是通用机器人的重要范式，未来需解决可扩展性与泛化能力等关键问题。

Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.

</details>


### [165] [Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion](https://arxiv.org/abs/2509.19023)
*Shuai Liu,Meng Cheng Lau*

Main category: cs.RO

TL;DR: ROM-GRL是一种无需动作捕捉数据的两阶段强化学习框架，通过简约模型引导生成稳定自然的人形步态。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖动作捕捉数据或复杂的奖励设计，难以实现无需人类示范的自然步态生成。

Method: 第一阶段用PPO训练4自由度简约模型生成节能步态模板；第二阶段用SAC结合对抗判别器，使全身体策略匹配简约模型的步态特征分布。

Result: 在1m/s和4m/s速度下，ROM-GRL生成的步态更稳定、对称，跟踪误差显著低于纯奖励基线。

Conclusion: ROM-GRL成功融合了奖励学习与模仿学习的优势，无需人类演示即可实现高效、自然的人形行走。

Abstract: We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.

</details>


### [166] [TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors](https://arxiv.org/abs/2509.19037)
*Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang*

Main category: cs.RO

TL;DR: 提出TacEva框架，为视觉触觉传感器提供标准化评估指标，支持任务导向的选型与设计优化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉触觉传感器因结构和机制差异大，缺乏统一评估标准，导致选型与优化困难。

Method: 构建TacEva框架，定义应用场景相关的性能指标，并设计标准化实验流程以实现可重复的定量评估。

Result: 在多种传感器上验证了框架的有效性，提供了各维度的量化性能数据，支持任务适配与设计指导。

Conclusion: TacEva为视觉触觉传感器的评估与优化提供了系统性工具，推动领域标准化发展。

Abstract: Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because
of the high spatial resolution they offer and their relatively low
manufacturing costs. However, variations in their sensing mechanisms,
structural dimension, and other parameters lead to significant performance
disparities between existing VBTSs. This makes it challenging to optimize them
for specific tasks, as both the initial choice and subsequent fine-tuning are
hindered by the lack of standardized metrics. To address this issue, TacEva is
introduced as a comprehensive evaluation framework for the quantitative
analysis of VBTS performance. The framework defines a set of performance
metrics that capture key characteristics in typical application scenarios. For
each metric, a structured experimental pipeline is designed to ensure
consistent and repeatable quantification. The framework is applied to multiple
VBTSs with distinct sensing mechanisms, and the results demonstrate its ability
to provide a thorough evaluation of each design and quantitative indicators for
each performance dimension. This enables researchers to pre-select the most
appropriate VBTS on a task by task basis, while also offering
performance-guided insights into the optimization of VBTS design. A list of
existing VBTS evaluation methods and additional evaluations can be found on our
website: https://stevenoh2003.github.io/TacEva/

</details>


### [167] [ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation](https://arxiv.org/abs/2509.19047)
*Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee*

Main category: cs.RO

TL;DR: 提出ManipForce系统和FMT模型，通过融合高频力-力矩与视觉数据显著提升接触密集操作任务的成功率


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法依赖纯视觉示范，难以精准控制接触力，制约了精密装配等任务的表现

Method: 设计ManipForce设备采集高频力-力矩与RGB数据，并构建频率感知多模态变换器（FMT），通过双向交叉注意力融合多模态信号

Result: 在六个真实接触密集任务上平均成功率83%，显著优于纯视觉基线，实验证明高频力数据与跨模态融合关键

Conclusion: 引入高频力觉反馈与多模态融合是提升接触操作精度与稳定性的有效路径

Abstract: Contact-rich manipulation tasks such as precision assembly require precise
control of interaction forces, yet existing imitation learning methods rely
mainly on vision-only demonstrations. We propose ManipForce, a handheld system
designed to capture high-frequency force-torque (F/T) and RGB data during
natural human demonstrations for contact-rich manipulation. Building on these
demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).
FMT encodes asynchronous RGB and F/T signals using frequency- and
modality-aware embeddings and fuses them via bi-directional cross-attention
within a transformer diffusion policy. Through extensive experiments on six
real-world contact-rich manipulation tasks - such as gear assembly, box
flipping, and battery insertion - FMT trained on ManipForce demonstrations
achieves robust performance with an average success rate of 83% across all
tasks, substantially outperforming RGB-only baselines. Ablation and
sampling-frequency analyses further confirm that incorporating high-frequency
F/T data and cross-modal integration improves policy performance, especially in
tasks demanding high precision and stable contact.

</details>


### [168] [SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions](https://arxiv.org/abs/2509.19076)
*Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet*

Main category: cs.RO

TL;DR: SlicerROS2升级版提升模块化与数据传输，支持四种真实影像引导机器人应用


<details>
  <summary>Details</summary>
Motivation: 需要标准化的医学机器人研究集成平台，以实现真实场景下的实时影像引导机器人操作

Method: 重构SlicerROS2模块，增强模块化设计，支持3D Slicer的Python API及低层功能，优化数据传输协议

Result: 成功实现四个真实影像引导机器人应用场景，验证了系统实时性与灵活性

Conclusion: SlicerROS2的新设计为医学机器人研究提供了强大、灵活且标准化的集成框架

Abstract: Image-guided robotic interventions involve the use of medical imaging in
tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer
and robot operating system (ROS) in pursuit of a standard integration approach
for medical robotics research. The first release of SlicerROS2 demonstrated the
feasibility of using the C++ API from 3D Slicer and ROS to load and visualize
robots in real time. Since this initial release, we've rewritten and redesigned
the module to offer greater modularity, access to low-level features, access to
3D Slicer's Python API, and better data transfer protocols. In this paper, we
introduce this new design as well as four applications that leverage the core
functionalities of SlicerROS2 in realistic image-guided robotics scenarios.

</details>


### [169] [World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2509.19080)
*Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: World4RL利用扩散模型构建高保真仿真环境，在想象环境中优化预训练机器人操作策略，无需真实交互即可显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习受限于专家数据稀缺，强化学习在真实机器人上训练成本高且危险，仿真训练存在仿真到现实的差距，亟需一种高效、安全的策略优化方法。

Method: 提出World4RL框架，基于扩散模型构建高保真世界模型，采用双热动作编码和扩散主干网络，在冻结的世界模型中直接端到端优化策略，避免在线真实交互。

Result: 在仿真和真实实验中，World4RL显著优于模仿学习和其他基线方法，实现了更高的任务成功率和稳定的策略提升。

Conclusion: 扩散模型作为世界模型能有效支撑策略的离线优化，为机器人学习提供了安全、高效、高保真的新范式。

Abstract: Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.

</details>


### [170] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: FunCanon通过功能归一化提升机器人技能的泛化能力，实现跨任务复用和真实世界部署


<details>
  <summary>Details</summary>
Motivation: 端到端演示学习的策略通常局限于训练分布，难以泛化到新任务或新物体

Method: 将长程操作任务分解为由执行者、动词和物体组成的动作块，结合功能归一化与基于大视觉语言模型的寓意线索，构建对象中心和动作中心的扩散策略FuncDiffuser

Result: 在仿真与真实世界基准上实现了类别级泛化、跨任务行为复用和鲁棒的sim2real迁移

Conclusion: 功能归一化为复杂操作任务中的模仿学习提供了强归纳偏置，显著提升泛化与可扩展性

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


### [171] [Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation](https://arxiv.org/abs/2509.19105)
*Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir*

Main category: cs.RO

TL;DR: 提出RS-Net，通过RGB图像预测光谱签名，从而在无需专用硬件的情况下实现地形材料识别与摩擦估计，支持轮式与四足机器人室外导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖几何或语义标签，无法区分材质差异的视觉相似表面；光谱传感虽能提供材质信息，但因硬件昂贵、计算复杂而难以普及。

Method: 设计RS-Net神经网络，从RGB图像补丁预测光谱签名，映射为地形标签和摩擦系数，并集成至运动规划器与MPC控制器。

Result: 仅用RGB传感器即可实现与光谱传感器相当的地形分类与摩擦估计性能，支持轮式与四足机器人在复杂室外环境中的稳定导航。

Conclusion: RS-Net实现了从昂贵光谱传感向低成本RGB传感的迁移，为机器人提供高效、普适的地形物理属性感知方案。

Abstract: Successful navigation in outdoor environments requires accurate prediction of
the physical interactions between the robot and the terrain. To this end,
several methods rely on geometric or semantic labels to classify traversable
surfaces. However, such labels cannot distinguish visually similar surfaces
that differ in material properties. Spectral sensors enable inference of
material composition from surface reflectance measured across multiple
wavelength bands. Although spectral sensing is gaining traction in robotics,
widespread deployment remains constrained by the need for custom hardware
integration, high sensor costs, and compute-intensive processing pipelines. In
this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),
a deep neural network designed to bridge the gap between the accessibility of
RGB sensing and the rich material information provided by spectral data. RS-Net
predicts spectral signatures from RGB patches, which we map to terrain labels
and friction coefficients. The resulting terrain classifications are integrated
into a sampling-based motion planner for a wheeled robot operating in outdoor
environments. Likewise, the friction estimates are incorporated into a
contact-force-based MPC for a quadruped robot navigating slippery surfaces.
Thus, we introduce a framework that learns the task-relevant physical property
once during training and thereafter relies solely on RGB sensing at test time.
The code is available at https://github.com/prajapatisarvesh/RS-Net.

</details>


### [172] [BiGraspFormer: End-to-End Bimanual Grasp Transformer](https://arxiv.org/abs/2509.19142)
*Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee*

Main category: cs.RO

TL;DR: 提出BiGraspFormer，一种端到端Transformer框架，直接从点云生成协调的双臂抓取，显著提升抓取质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注单臂抓取或分阶段生成与评估双臂抓取，导致协调性差、碰撞风险高和受力不均。

Method: 采用Single-Guided Bimanual (SGB)策略，通过Transformer解码器生成单臂抓取候选，再利用注意力机制联合预测双臂位姿与质量分数，降低12-DoF搜索空间复杂度。

Result: 在仿真与真实实验中，BiGraspFormer性能优于现有方法，推理速度低于0.05秒。

Conclusion: BiGraspFormer实现高效、协调的双臂抓取生成，为复杂物体操作提供有效解决方案。

Abstract: Bimanual grasping is essential for robots to handle large and complex
objects. However, existing methods either focus solely on single-arm grasping
or employ separate grasp generation and bimanual evaluation stages, leading to
coordination problems including collision risks and unbalanced force
distribution. To address these limitations, we propose BiGraspFormer, a unified
end-to-end transformer framework that directly generates coordinated bimanual
grasps from object point clouds. Our key idea is the Single-Guided Bimanual
(SGB) strategy, which first generates diverse single grasp candidates using a
transformer decoder, then leverages their learned features through specialized
attention mechanisms to jointly predict bimanual poses and quality scores. This
conditioning strategy reduces the complexity of the 12-DoF search space while
ensuring coordinated bimanual manipulation. Comprehensive simulation
experiments and real-world validation demonstrate that BiGraspFormer
consistently outperforms existing methods while maintaining efficient inference
speed (<0.05s), confirming the effectiveness of our framework. Code and
supplementary materials are available at https://sites.google.com/bigraspformer

</details>


### [173] [A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination](https://arxiv.org/abs/2509.19168)
*Mark Gonzales,Ethan Oh,Joseph Moore*

Main category: cs.RO

TL;DR: 提出一种基于采样的滚动时域规划器，利用交叉熵方法优化多模态策略，提升机器人路径规划的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统规划方法易陷入局部最优，且多机器人协同规划计算复杂度高，难以实现实时避障与全局优化。

Method: 采用交叉熵方法优化多模态策略分布，实现分布式多机器人协同规划，无需中心化优化。

Result: 仿真与硬件实验表明，多模态策略显著提升复杂环境中的成功率，且具备实时可行性。

Conclusion: 该方法有效解决了局部最优与计算复杂性问题，为多机器人系统提供了高效、鲁棒的实时规划方案。

Abstract: In this paper, we present a receding-horizon, sampling-based planner capable
of reasoning over multimodal policy distributions. By using the cross-entropy
method to optimize a multimodal policy under a common cost function, our
approach increases robustness against local minima and promotes effective
exploration of the solution space. We show that our approach naturally extends
to multi-robot collision-free planning, enables agents to share diverse
candidate policies to avoid deadlocks, and allows teams to minimize a global
objective without incurring the computational complexity of centralized
optimization. Numerical simulations demonstrate that employing multiple modes
significantly improves success rates in trap environments and in multi-robot
collision avoidance. Hardware experiments further validate the approach's
real-time feasibility and practical performance.

</details>


### [174] [MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap](https://arxiv.org/abs/2509.19169)
*Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan*

Main category: cs.RO

TL;DR: MagiClaw是一个双指末端执行器，通过硬件一致性和多模态传感简化了人到机器人的技能迁移。


<details>
  <summary>Details</summary>
Motivation: 人类演示到机器人执行的技能迁移常因感知与形态的领域差距而受阻。

Method: MagiClaw结合软多面体网络（SPN）与嵌入式摄像头，融合iPhone的6D位姿、RGB视频和LiDAR深度图，实现同步多模态数据采集与混合现实控制。

Result: 该系统降低了高保真接触数据收集的门槛，加速了可泛化操作策略的开发。

Conclusion: MagiClaw通过硬件和感知统一架构，有效弥合了人机域差距，提升了技能迁移效率。

Abstract: The transfer of manipulation skills from human demonstration to robotic
execution is often hindered by a "domain gap" in sensing and morphology. This
paper introduces MagiClaw, a versatile two-finger end-effector designed to
bridge this gap. MagiClaw functions interchangeably as both a handheld tool for
intuitive data collection and a robotic end-effector for policy deployment,
ensuring hardware consistency and reliability. Each finger incorporates a Soft
Polyhedral Network (SPN) with an embedded camera, enabling vision-based
estimation of 6-DoF forces and contact deformation. This proprioceptive data is
fused with exteroceptive environmental sensing from an integrated iPhone, which
provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS
application, MagiClaw streams synchronized, multi-modal data for real-time
teleoperation, offline policy learning, and immersive control via mixed-reality
interfaces. We demonstrate how this unified system architecture lowers the
barrier to collecting high-fidelity, contact-rich datasets and accelerates the
development of generalizable manipulation policies. Please refer to the iOS app
at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.

</details>


### [175] [Proactive-reactive detection and mitigation of intermittent faults in robot swarms](https://arxiv.org/abs/2509.19246)
*Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: 提出一种基于自组织备份层和分布式共识的主动-被动策略，有效检测与缓解机器人群中间歇性故障，保障形成控制收敛。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于永久性故障，而间歇性故障因难以检测被忽视；SoNS方法使机器人群能构建持久网络，为检测间歇性故障提供新可能。

Method: 提出主动-被动混合策略：主动构建动态备份路径以适应拓扑变化，被动使用单次似然比检验在多路网络中对比信息，实现早期故障检测与自组织路由重定向。

Result: 在形成控制场景中验证，该方法能有效防止间歇性故障破坏收敛，具备高检测准确率与低误报率。

Conclusion: SoNS框架下，通过多路网络与自组织机制，可首次有效应对机器人群中的间歇性故障，为高可靠 swarm 系统提供新范式。

Abstract: Intermittent faults are transient errors that sporadically appear and
disappear. Although intermittent faults pose substantial challenges to
reliability and coordination, existing studies of fault tolerance in robot
swarms focus instead on permanent faults. One reason for this is that
intermittent faults are prohibitively difficult to detect in the fully
self-organized ad-hoc networks typical of robot swarms, as their network
topologies are transient and often unpredictable. However, in the recently
introduced self-organizing nervous systems (SoNS) approach, robot swarms are
able to self-organize persistent network structures for the first time, easing
the problem of detecting intermittent faults. To address intermittent faults in
robot swarms that have persistent networks, we propose a novel
proactive-reactive strategy to detection and mitigation, based on
self-organized backup layers and distributed consensus in a multiplex network.
Proactively, the robots self-organize dynamic backup paths before faults occur,
adapting to changes in the primary network topology and the robots' relative
positions. Reactively, robots use one-shot likelihood ratio tests to compare
information received along different paths in the multiplex network, enabling
early fault detection. Upon detection, communication is temporarily rerouted in
a self-organized way, until the detected fault resolves. We validate the
approach in representative scenarios of faulty positional data occurring during
formation control, demonstrating that intermittent faults are prevented from
disrupting convergence to desired formations, with high fault detection
accuracy and low rates of false positives.

</details>


### [176] [Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces](https://arxiv.org/abs/2509.19261)
*Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani*

Main category: cs.RO

TL;DR: 提出一种模仿引导的双臂规划框架，实现动态环境中稳定高效的抓取转换与运动优化


<details>
  <summary>Details</summary>
Motivation: 现有抓取转换策略难以应对外部力变化且运动性能优化不足

Method: 引入抓取流形中稳定交点采样策略，结合层次化双阶段运动架构（模仿学习全局路径生成 + 二次规划局部规划）

Result: 在强作用力任务中显著提升抓取转换效率与运动性能

Conclusion: 该框架有效增强机器人在动态环境中的稳定性和灵巧性

Abstract: Robotic manipulation in dynamic environments often requires seamless
transitions between different grasp types to maintain stability and efficiency.
However, achieving smooth and adaptive grasp transitions remains a challenge,
particularly when dealing with external forces and complex motion constraints.
Existing grasp transition strategies often fail to account for varying external
forces and do not optimize motion performance effectively. In this work, we
propose an Imitation-Guided Bimanual Planning Framework that integrates
efficient grasp transition strategies and motion performance optimization to
enhance stability and dexterity in robotic manipulation. Our approach
introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for
seamless transitions between uni-manual and bi-manual grasps, reducing
computational costs and regrasping inefficiencies. Additionally, a Hierarchical
Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path
Generator with a Quadratic Programming-driven Local Planner to ensure real-time
motion feasibility, obstacle avoidance, and superior manipulability. The
proposed method is evaluated through a series of force-intensive tasks,
demonstrating significant improvements in grasp transition efficiency and
motion performance. A video demonstrating our simulation results can be viewed
at
\href{https://youtu.be/3DhbUsv4eDo}{\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.

</details>


### [177] [SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration](https://arxiv.org/abs/2509.19292)
*Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: SOE通过流形约束探索提升机器人策略的样本效率与安全性，超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有探索方法依赖随机扰动，导致不安全与行为不稳定，限制了策略改进效果

Method: SOE学习任务相关因子的低维潜在表示，约束探索在有效动作流形上，实现安全、多样且高效的探索

Result: 在仿真与真实任务中，SOE显著提升任务成功率、探索平滑性、安全性与样本效率

Conclusion: 基于流形的探索是实现样本高效策略自改进的有原则方法

Abstract: Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE

</details>


### [178] [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://arxiv.org/abs/2509.19301)
*Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi*

Main category: cs.RO

TL;DR: 通过残差学习框架结合行为克隆与强化学习，实现高效真实世界机器人训练，首次在人形机器人上成功应用RL。


<details>
  <summary>Details</summary>
Motivation: 行为克隆受限于数据质量与收集成本，强化学习在真实世界中面临样本效率低、安全性和稀疏奖励难题，尤其在高自由度系统中。

Method: 采用残差学习框架，以行为克隆策略为基底，通过样本高效的离线强化学习学习每步残差修正，仅需稀疏二值奖励信号。

Result: 在仿真与真实世界中显著提升高自由度系统的操控性能，首次实现人形机器人灵巧手的真实世界RL训练，达到视觉任务的SOTA性能。

Conclusion: 该方法为真实世界部署强化学习提供了实用路径，融合BC与RL的优势，突破了传统方法的瓶颈。

Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor
control policies. However, these approaches are limited by the quality of human
demonstrations, the manual effort required for data collection, and the
diminishing returns from increasing offline data. In comparison, reinforcement
learning (RL) trains an agent through autonomous interaction with the
environment and has shown remarkable success in various domains. Still,
training RL policies directly on real-world robots remains challenging due to
sample inefficiency, safety concerns, and the difficulty of learning from
sparse rewards for long-horizon tasks, especially for high-degree-of-freedom
(DoF) systems. We present a recipe that combines the benefits of BC and RL
through a residual learning framework. Our approach leverages BC policies as
black-box bases and learns lightweight per-step residual corrections via
sample-efficient off-policy RL. We demonstrate that our method requires only
sparse binary reward signals and can effectively improve manipulation policies
on high-degree-of-freedom (DoF) systems in both simulation and the real world.
In particular, we demonstrate, to the best of our knowledge, the first
successful real-world RL training on a humanoid robot with dexterous hands. Our
results demonstrate state-of-the-art performance in various vision-based tasks,
pointing towards a practical pathway for deploying RL in the real world.
Project website: https://residual-offpolicy-rl.github.io

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [179] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 使用图像型机器学习模型分析Ulam螺旋中素数分布的规律性，发现高数值区域更易学习，支持数论猜想，表明ML可作为数论实验工具。


<details>
  <summary>Details</summary>
Motivation: 素数虽定义确定，但表现统计随机性，需新方法探索其内在规律。

Method: 用图像型机器学习模型训练于Ulam螺旋不同区域的图像块，比较分类准确率、精确率与召回率。

Result: 在约500m区域训练的模型精度高于25m以下区域，高区更易学习；模型在低区侧重识别素数，高区侧重剔除合数。

Conclusion: 机器学习可作为数论研究的新实验工具，尤其在探索强/弱素数模式及其密码学应用方面具潜力。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [180] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出基于Wasserstein距离的估值框架，用于联邦学习中的数据价值评估与选择，无需访问原始数据即可支持隐私保护下的高性能数据组合识别。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习的数据市场中，如何在保护隐私的前提下有效评估和选择异构数据源以提升模型性能是关键挑战。

Method: 提出一种基于Wasserstein距离的分布式估计算法，用于预测模型在未见数据组合上的表现，并揭示数据异构性与联邦聚合算法的兼容性，结合神经扩展定律实现无需全训练的数据筛选。

Result: 在标签倾斜、错误标注和无标签数据等多种异构场景中，该方法能稳定识别高性能数据组合，显著提升联邦学习模型性能。

Conclusion: 该框架为构建可信赖、隐私保护的联邦学习数据市场提供了有效的数据估值与筛选机制。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [181] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 研究在随机需求下使用物理信息神经微分方程（UDE）与纯神经网络（NODE）建模库存动态，发现结构化模型在轻尾/相关噪声下表现更优，非结构化模型在极端事件主导时更佳。


<details>
  <summary>Details</summary>
Motivation: 不确定结构偏差在不同需求环境下对牛鞭效应预测的促进或抑制作用，亟需系统性评估。

Method: 比较全神经ODE（NODE）与保持守恒和订货至上结构的物理信息UDE，在AR(1)、高斯和对数正态三种需求 regimes 下训练与多步预测。

Result: UDE在AR(1)和高斯需求下显著降低库存RMSE（如从4.92降至0.26），NODE在重尾对数正态需求下表现更优；UDE外推稳定但忽略尖峰，NODE易相位漂移。

Conclusion: 轻尾或相关噪声环境应强加结构，极端事件主导时应放松结构；该结论对科学与工程中的混合建模具有普适指导意义。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [182] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 利用神经网络代理模型实现桥梁间知识迁移，提升大规模桥梁网络的实时损伤监测效率。


<details>
  <summary>Details</summary>
Motivation: 永久监测系统产生大量数据，但跨桥梁网络的规模化管理面临挑战，需高效共享相似结构的损伤信息。

Method: 基于神经网络代理模型的迁移学习方法，结合贝叶斯推断框架，利用模态特征进行连续损伤评估。

Result: 在两座真实桥梁数据上验证，模型对损伤位置、严重程度和范围高度敏感，实现跨结构知识迁移。

Conclusion: 该方法提升了实时监测能力，支持网络级智能监测与结构韧性增强。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [183] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出AdaMixT模型，通过自适应加权多尺度专家Transformer提升多元时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定尺度块或缺乏有效多尺度特征融合机制，限制了对时间序列复杂模式的捕捉。

Method: 引入多种尺度块，结合通用预训练模型和领域特异模型进行特征提取，并设计门控网络动态加权融合专家输出。

Result: 在8个真实数据集（如Weather、Traffic、Electricity、ILI和ETT系列）上性能显著优于现有方法。

Conclusion: AdaMixT通过自适应多尺度融合有效提升了时间序列预测的准确性与泛化能力。

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [184] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源、模块化框架，利用大语言模型迭代生成算法解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有算法生成方法缺乏可复现性与用户控制，难以协调多个模型分工协作。

Method: EASE构建了包含生成、测试、分析与评估的闭环反馈系统，抽象提示设计与模型管理，支持多LLM分工协同。

Result: 提供了透明、可扩展的平台，支持跨领域算法协同设计。

Conclusion: EASE有效简化了LLM驱动的算法生成过程，提升了可复现性与控制力。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [185] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 使用AIS数据通过机器学习在波罗的海Bornholm海峡实现92%以上准确率的船舶类型分类，关键特征为桥位比和最大航速。


<details>
  <summary>Details</summary>
Motivation: 准确识别船舶类型对海上安全监管和打击非法捕捞至关重要，但现有方法依赖多源数据，本研究仅用AIS数据实现高效分类。

Method: 基于8天AIS数据，预处理轨迹后提取31个动力学、时间、空间和船体特征，采用随机森林+SMOTE，分组交叉验证，评估五类船舶分类性能。

Result: 随机森林模型在测试集上达到92.15%准确率，宏F1为93.27%，ROC-AUC最高达0.9897，桥位比和最大航速为最重要特征，货船与油轮易混淆。

Conclusion: 仅凭轻量级AIS轨迹特征即可实现海峡区域船舶类型的实时分类，具备实际部署价值，未来可引入DBSCAN分段与梯度提升模型进一步优化。

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [186] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出基于补丁的PCA-Net框架，显著降低PCA计算开销，提升PDE神经算子学习效率，速度提升3.7–4倍。


<details>
  <summary>Details</summary>
Motivation: 传统PCA在高维解场中计算开销大，限制了神经算子学习的效率。

Method: 将解场分解为局部补丁，在每个补丁内应用PCA，构建局部-全局和局部-局部两种补丁策略，并引入重叠补丁与CNN微调优化。

Result: 补丁式PCA大幅降低计算复杂度，端到端处理时间减少3.7至4倍，保持高重构精度。

Conclusion: 基于补丁的PCA-Net是高效PDE神经算子学习的有前景方法。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [187] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出一种基于CoOp的子空间表示学习方法，通过提示调优提升视觉语言模型在少样本场景下的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法仅依赖softmax概率，忽视了VLMs中丰富特征嵌入的判别潜力。

Method: 结合子空间表示学习与提示调优，将ID特征投影到提示向量张成的子空间，ID无关特征投影到正交零空间，并设计端到端学习准则。

Result: 在真实数据集上显著提升OOD检测性能，同时保持高ID分类准确率。

Conclusion: 所提方法有效利用VLMs的特征嵌入，为少样本OOD检测提供了新范式。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [188] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 本研究首次系统比较了AI模型在产前胎心监护（CTG）分析中的表现，发现微调后的语言模型性能最优。


<details>
  <summary>Details</summary>
Motivation: 胎心监护解读依赖主观临床判断，存在准确性差异和延误风险，亟需自动化AI方法提升分析一致性与效率。

Method: 对比了时序基础模型、语言模型与传统CTG专用架构，基于500+真实临床CTG记录进行评估。

Result: 微调后的语言模型在自动化CTG分析中表现超越其他AI方法，包括基础模型和领域专用架构。

Conclusion: 语言模型为产前胎心监护提供了有前景的新路径，推动临床AI在孕期监护中的发展。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [189] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 使用BlueField-3 DPU实时检测并缓解多节点张量并行推理中的负载不平衡问题，提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自回归推理在解码阶段因GPU片间负载不均衡导致吞吐下降和延迟尖峰，亟需高效监控与调控机制。

Method: 通过DPU卸载监控任务，分析GPU遥测数据和节点间通信模式，为调度器提供实时反馈。

Result: 成功识别多GPU执行中的负载偏斜与病理状态，并验证其可通过DPU网络追踪与缓解。

Conclusion: DPU辅助框架为大规模LLM推理的负载均衡提供了可行且高效的解决方案。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [190] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出一种新的空间平衡注意力块，用于时空预测，通过分层子图注意力平衡局部与全局空间相关性，实现高效且高性能的预测模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以同时兼顾空间邻近性与全局相关性，亟需一种能平衡二者的新机制。

Method: 将空间图划分为子图，引入Intra-subgraph Attention学习局部相关性，通过Inter-subgraph Attention聚合子图实现全局信息传递，并构建多尺度模型。

Result: 在中大型真实数据集上，性能比基线方法最高提升7.7%，且运行成本低。

Conclusion: 所提模型结构简单、可扩展性强，能有效建模结构化空间相关性，兼具高性能与高效率。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [191] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: ALS通过离线计算潜在空间中的优化方向，显著加速测试时优化，实现速度与精度的双赢


<details>
  <summary>Details</summary>
Motivation: 传统测试时优化方法（如迭代细化）计算成本过高，难以规模化应用

Method: Amortized Latent Steering (ALS)：离线计算成功与失败生成的隐状态均值差异，将优化方向固化为常量向量，在推理时低成本修正隐表示

Result: 在GSM8K和MATH-500上，ALS相比迭代方法提速2-5倍，效率-精度权衡提升高达101%，性能优于贪心CoT和Self-Consistency

Conclusion: 大部分潜在空间优化收益可离线捕获，使复杂推理技术适用于实际部署

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [192] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯统计的在线增量学习算法，实现数字界面根据用户行为动态自适应，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统界面基于群体偏好，无法满足个体用户差异和环境变化的需求，亟需个性化的自适应系统。

Method: 采用贝叶斯统计建模用户浏览行为，结合在线增量学习，构建任务模型并图形化导航路径，持续学习新任务且保留先验知识。

Result: 仿真表明该方法在静态与动态环境中均能有效预测用户行为，即使数据稀少也能保持高可靠性。

Conclusion: 该研究为实现个性化自适应界面系统提供了理论基础与实践路径，显著提升用户导航效率与体验。

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [193] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 本文将轻量级L-SGD算法扩展到RISC-V MCU，并提出8位量化版本，显著降低内存和训练时间，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 物联网设备缺乏GPU，云训练引发隐私和连接依赖问题，现有Federated Learning需高效算法，而RISC-V平台缺乏支持。

Method: 在Arm和RISC-V上用32位浮点评估L-SGD，并提出8位量化L-SGD以应对RISC-V无FPU的限制。

Result: 8位量化L-SGD在RISC-V上实现内存减少4倍、训练加速2.2倍，精度损失可忽略。

Conclusion: 8位量化L-SGD有效支持RISC-V MCU上的设备端训练，推动去中心化联邦学习在开源架构上的应用。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [194] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: 提出MOBILERL框架，通过ADAGRPO算法提升移动GUI代理在强化学习中的性能，显著提升在Android任务中的成功率。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理在强化学习中面临任务难度长尾分布和环境采样效率低的挑战。

Method: 设计Difficulty-Adaptive GRPO（ADAGRPO）算法，包含难度自适应正向回放、失败课程过滤和最短路径奖励调整策略，以稳定训练并提升样本效率。

Result: MOBILERL-9B模型在AndroidWorld和AndroidLab上的成功率分别达到75.8%和46.8%，达到SOTA水平。

Conclusion: MOBILERL显著提升移动GUI代理的性能，已应用于AutoGLM产品并开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [195] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 提出CoCoGen框架，结合生成式AI与博弈论，解决跨场景联邦学习中的竞争与异构性问题，提升社会福利。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注统计异构性，但忽视组织间经济竞争导致的参与意愿低，且二者协同影响未被探索。

Method: CoCoGen利用生成式AI和加权势博弈建模组织行为，通过数据生成策略最大化社会福利。

Result: 在Fashion-MNIST上验证，CoCoGen在不同异构与竞争水平下均优于基线方法。

Conclusion: CoCoGen有效协调竞争与合作，为跨组织联邦学习提供可持续的协作机制。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [196] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 使用监督学习预测咖啡评分，集成方法表现最优


<details>
  <summary>Details</summary>
Motivation: 弥补传统咖啡品鉴依赖专家经验的不足，提供数据驱动的评估方法

Method: 通过TF-IDF和SelectKBest进行特征提取与选择，训练六种模型并优化超参数，使用F1、Gmean和AUC评估性能

Result: Extra Trees、Random Forest、XGBoost和MLP表现优于决策树和KNN

Conclusion: 严谨的特征选择与超参数调优对感官产品预测至关重要，数据方法可 complement 专业品鉴

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [197] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: 提出NurseSchedRL强化学习框架，优化护士-病人分配，提升效率并减少疲劳


<details>
  <summary>Details</summary>
Motivation: 传统调度方法难以应对护士技能差异、患者病情严重性、疲劳和照护连续性等动态多约束环境

Method: 基于PPO的强化学习框架，结合结构化状态编码、约束动作掩码和注意力机制，建模技能、疲劳与地理上下文

Result: 在真实数据模拟中，相比基线方法，NurseSchedRL显著提升调度效率、技能匹配度并降低护士疲劳

Conclusion: 强化学习在复杂高风险医疗人力资源管理中具有重要应用潜力

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [198] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 联邦学习在电动汽车充电站异常检测中表现良好，FedAvgM在非IID和系统异构环境下优于FedAvg，实现隐私保护与高精度检测。


<details>
  <summary>Details</summary>
Motivation: 传统集中式入侵检测系统存在隐私风险，而现有联邦学习评估忽视了物联网充电站中的系统异构和非IID数据等实际挑战。

Method: 在系统异构和非IID数据条件下，使用FedAvg和FedAvgM算法对电动汽车充电站的异常检测进行实验评估。

Result: 在IID环境下，FedAvg性能优于集中式模型；在非IID和异构环境下，FedAvgM收敛更快、检测准确率更高。

Conclusion: 联邦学习可在保持隐私的前提下有效应对EVCS的异构性，FedAvgM是实现鲁棒安全防护的有前景方案。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [199] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: 提出Safe-SAIL框架，通过稀疏自编码器解释大语言模型中的安全相关特征，提升安全性分析能力


<details>
  <summary>Details</summary>
Motivation: 现有安全研究局限于特定任务评估，难以应对未知风险；现有SAE未细粒度解析安全语义，无法有效捕捉毒性生成等高风险行为

Method: Safe-SAIL框架：系统筛选最具安全概念解释力的SAE，精细解释安全相关神经元，并引入高效扩展策略以降低标注成本

Result: 构建了包含SAE检查点与可读神经元解释的工具包，支持对LLM安全风险的实证分析

Conclusion: Safe-SAIL为大语言模型安全性提供了可解释、可扩展的机制分析新路径，推动安全研究从任务评估转向机制理解

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [200] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出高斯-埃尔米特求积方法解耦认知与偶然不确定性，提升物理可靠性分析的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型引入的认知不确定性与输入的偶然不确定性耦合，会降低可靠性预测的准确性。

Method: 采用高斯-埃尔米特求积法，结合一阶和二阶可靠性方法，先计算条件失效概率，再积分整合认知不确定性。

Result: 三个算例表明该方法在保持计算效率的同时，比忽略模型不确定性的传统方法更可信。

Conclusion: 所提方法有效解耦两类不确定性，显著提升可靠性分析的准确性与鲁棒性。

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [201] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出一种融合STL和GRU的地铁换乘客流预测模型，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 为优化地铁运营计划并提高运输效率，需要更准确的换乘客流预测理论支持。

Method: 使用Keras构建GRU模型，结合图深度优先搜索识别乘客路径，用STL分解客流时间序列并剔除残差异常值，最终实现预测。

Result: 在工作日（除周五）、周五和休息日，STL-GRU模型的MAPE分别比对比模型降低至少2.3%、1.36%和6.42%。

Conclusion: STL-GRU模型在多类时间场景下均表现更优，为地铁智能运营提供可靠预测工具。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [202] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: Transformer权重呈现随机性，与物理结构无直接关联，表明机器学习与科学方法可能是互补的认知路径，但可解释性仍存挑战。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer模型在物理问题求解中权重的特性及其与科学解释性的关系。

Method: 分析Transformer权重在两个物理应用中的表现，对比其与物理数学结构的关联性，并探讨其与路径积分的类比。

Result: 权重呈现随机性，无直接物理对应；路径积分类比可解释其随机性，但无法解决可解释性矛盾。

Conclusion: 机器学习与科学方法互补，但无洞察力的知识获取存在风险。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [203] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: 提出MoE-CL框架，通过双LoRA专家与GAN判别器实现大模型持续学习，有效缓解灾难性遗忘，提升工业场景适应性


<details>
  <summary>Details</summary>
Motivation: 工业场景中LLM需持续学习以应对动态任务分布，但现有持续学习方法易导致灾难性遗忘，影响泛化能力

Method: 采用双LoRA专家设计：任务专属LoRA保存特异性知识，共享LoRA实现跨任务迁移，并引入任务感知GAN判别器过滤噪声，促进仅传递任务对齐信息

Result: 在MTL5和Tencent3基准上验证有效性，Tencent视频平台A/B测试中人工审核成本降低15.3%

Conclusion: MoE-CL在保持知识 retention 与跨任务泛化间取得平衡，具备工业级可部署性，支持LLM自演化

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [204] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 提出一种加权梯度追踪的隐私保护分布式优化算法，消除梯度追踪中的隐私泄露风险并保证收敛性


<details>
  <summary>Details</summary>
Motivation: 梯度追踪虽提升收敛速度，但存在隐私泄露风险，亟需在不牺牲性能前提下保护代理私有信息

Method: 引入衰减权重因子修改梯度追踪机制，结合时变异构步长分析收敛性

Result: 理论证明算法在弱假设下精确收敛至最优解，仿真验证其在分布式估计与CNN训练中的有效性

Conclusion: 所提算法在保持收敛性能的同时有效解决了梯度追踪的隐私泄露问题

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [205] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 提出SDGF模型，通过静态-动态图融合捕捉多尺度时间序列相关性，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模多尺度、动态变化的多元时间序列间相关性。

Method: 采用静态图（先验知识）与动态图（小波分解多尺度特征）双路径结构，结合注意力门控融合与多核扩张卷积建模时序模式。

Result: 在多个真实基准数据集上验证了模型的有效性，性能优于现有方法。

Conclusion: 静态-动态图融合能有效捕捉多尺度相关性，为多元时间序列预测提供新思路。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [206] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 构建大规模LLM结构与性能数据集，通过数据挖掘揭示结构配置对性能的影响，指导未来模型优化。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统性、数据驱动地分析LLM结构配置与性能之间的关系。

Method: 构建包含多种开源LLM结构及其在多基准上性能的大型数据集，并结合数据挖掘与机制可解释性技术进行分析。

Result: 量化了结构选择对性能的影响，验证了关键结构因素的作用。

Conclusion: 数据驱动的方法为LLM的针对性设计与优化提供了实证依据。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [207] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出统一基准LoRALib，评估不同LoRA-MoE方法，发现LoRAMoE性能最优，任务相关LoRA选择可进一步提升效果


<details>
  <summary>Details</summary>
Motivation: 现有LoRA-MoE方法缺乏统一标准，难以公平比较，需构建标准化基准以促进系统性研究

Method: 构建LoRALib基准，统一40个下游任务数据格式，用相同超参数训练17种模型共680个LoRA模块，基于OpenCompass评估3种代表性LoRA-MoE方法及LoRA选择机制

Result: LoRAMoE表现最佳，优先选择与目标任务相关的LoRA模块可显著提升MoE性能

Conclusion: LoRALib为LoRA-MoE研究提供标准化平台，未来工作可基于此探索更优的专家选择与路由策略

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [208] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: 提出RIPLM算法，首次在睡眠专家设置中同时实现排名忠实性和方差自适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于专家身份更新，无法保证在排名基准与分布基准之间的等价性，导致性能受限。

Method: RIPLM通过在排名诱导的Plackett-Luce参数化空间中直接更新，确保每轮分布均保持在排名诱导分布类中。

Result: RIPLM是首个在睡眠专家设置中同时满足排名忠实性与方差自适应性的算法。

Conclusion: 该方法为排名感知的在线学习提供了新的理论框架与实用工具。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [209] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: FOLD-SE在二分类和多分类任务中均表现优于FOLD-R++和XGBoost，兼顾可解释性与性能，是黑箱模型的有力替代。


<details>
  <summary>Details</summary>
Motivation: 现有模型在准确性与可解释性间存在权衡，需发展既能保持高精度又具可解释性的规则类算法。

Method: 比较FOLD-SE与FOLD-R++在二分类中的表现，并对比FOLD-SE与XGBoost在多分类中的准确性、F1分数和处理时间。

Result: FOLD-SE在二分类中规则更少、效率更高；在多分类中精度和效率均优于XGBoost，且生成可读规则集。

Conclusion: FOLD-SE有效弥合了可解释性与性能之间的差距，可作为黑箱模型在各类分类任务中的实用替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [210] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 提出一种结合机器学习与通路映射的框架，用于预测2型糖尿病风险并发现潜在治疗靶点，准确率达78.43%。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病在遗传易感人群（如皮马印第安人）中负担沉重，亟需可解释、可扩展的早期预测与靶向干预方法。

Method: 采用逻辑回归与主成分分析（PCA）进行风险预测，并通过基因无偏通路映射连接预测因子到胰岛素、AMPK、PPAR等关键信号通路。

Result: 模型准确率78.43%，识别出GLP-1/GIP双重激动剂、AMPK激活剂、SIRT1调节剂和植物化学物等潜在治疗策略。

Conclusion: 该框架提升了代谢疾病精准医学的可解释性与实用性，为高风险人群提供早期检测与个性化干预新路径。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [211] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT 是首个全自动AI管道，可从Kaplan-Meier图中高精度重建个体患者数据，提升临床研究证据合成效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工数字化，易出错且难以扩展，亟需自动化解决方案以提升数据重建的准确性和可重复性。

Method: 集成高级图像预处理、GPT-5多模态推理与迭代重建算法，结合用户友好Web界面与AI助手，实现无编程干预的IPD自动重建。

Result: 在合成与真实数据集上验证，KM-GPT显著优于现有方法，并成功应用于胃癌免疫治疗荟萃分析，支持生物标志物亚组分析。

Conclusion: KM-GPT通过自动化与可扩展的Web平台，彻底改变临床研究中IPD重建流程，赋能循证决策。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [212] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: 提出AdaSTI模型，通过自适应依赖建模提升时空数据插补性能，显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时空插补中存在依赖关系误差累积和噪声步间变化被忽略的问题。

Method: 采用BiS4PI预插补结合STC提取条件信息，并引入NAST网络通过门控注意力捕捉扩散步间动态依赖。

Result: 在三个真实数据集上，AdaSTI相比现有方法误差降低最高达46.4%。

Conclusion: AdaSTI有效建模了时空依赖的动态性，是当前最优的扩散模型插补方法。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [213] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 提出一种基于多标签分类的ICU早期预警框架，利用24小时数据预测多种生理恶化触发因素，XGBoost模型表现最优。


<details>
  <summary>Details</summary>
Motivation: 传统早期预警系统（如SOFA、MEWS）仅关注单一结局，无法捕捉临床恶化的多维特性。

Method: 使用MIMIC-IV数据库，基于24-72小时数据定义四种CETs，提取前24小时的生命体征、检验值和人口统计特征，训练多标签分类模型并评估性能。

Result: XGBoost模型在呼吸、血流动力学、肾功能和神经系统恶化上的F1分数分别为0.66、0.72、0.76和0.62，显著优于基线模型，关键预测因子与临床定义一致。

Conclusion: 该框架可实现无需复杂时序建模或NLP的早期、可解释临床警示，具有实用潜力。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [214] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 提出ConceptFlow框架，通过概念注意和概念路径追踪CNN内部概念演化，提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视滤波器的语义角色和概念在层间的动态传播。

Method: 引入概念注意机制关联滤波器与高层概念，并构建概念转移矩阵量化概念跨层传播路径。

Result: ConceptFlow能生成语义清晰的模型推理路径，有效解释决策行为，提升解释的忠实度和人类对齐性。

Conclusion: 通过建模分层概念路径，ConceptFlow为CNN内部逻辑提供了更深入的洞察。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [215] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 提出一种基于稀疏表示的高效训练框架STS，通过视觉令牌压缩和层动态跳过提升MLLM训练效率。


<details>
  <summary>Details</summary>
Motivation: MLLM训练因多模态数据导致输入序列过长和层间计算利用率低而效率低下。

Method: 引入视觉令牌压缩器减少视觉信息负载，结合层动态跳过器在前向和反向传播中动态跳过冗余层。

Result: 在多个基准上验证，STS显著提升训练效率并适用于多种MLLM架构。

Conclusion: STS是一种通用且高效的MLLM训练优化方法，有效缓解计算负担与资源浪费。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [216] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS通过全局编码和共享超网络提升神经架构搜索的预测效率，在少量样本下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经预测器泛化能力差，难以捕捉架构间复杂关系，且性能评估耗时。

Method: 提出HyperNAS，包含全局编码方案捕捉宏观结构信息，共享超网络辅助学习架构间模式，并采用动态自适应多任务损失提升训练稳定性。

Result: 在五个搜索空间（含ViT）上实验，使用至少5倍更少样本下，CIFAR-10达到97.60%准确率，ImageNet达到82.4%准确率，创下新SOTA。

Conclusion: HyperNAS有效提升架构表示学习能力，在低样本场景下显著优于现有方法，推动NAS高效化发展。

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [217] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: 提出WLFM基础模型，通过预训练和少样本微调显著提升测井解释精度，具备可解释性与迁移能力。


<details>
  <summary>Details</summary>
Motivation: 测井解释受仪器响应非均质性、信号噪声和标签稀缺限制，需更鲁棒的通用模型。

Method: 基于1200口井多曲线测井数据，采用三阶段方法：测井块分词为地质标记、掩码标记建模与地层感知对比学习预训练、少样本多任务微调。

Result: WLFM在孔隙度估计中达到0.0041 MSE，岩性分类准确率74.13%；微调后提升至0.0038 MSE和78.10%；具备层感知能力、可复用地质词汇和曲线重建能力。

Conclusion: WLFM是可扩展、可解释、可迁移的地质AI基础骨架，为测井、地震与文本多模态融合奠定基础。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [218] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion使用深度强化学习设计出高效抗菌肽，100%候选物在体外表现优异，可快速生成广谱抗生素。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性预计到2050年每年将导致1000万人死亡，急需新型抗生素。

Method: 基于64亿参数的蛋白语言模型，结合强化学习与多目标优化，通过MIC分类器和理化性质目标联合训练。

Result: 100个设计肽均具低MIC值（部分为纳摩尔级），99个具广谱活性，主要通过靶向细胞膜杀菌。

Conclusion: ApexAmphion实现了生成、评分与优化一体化，为肽类抗生素研发提供了高效可扩展平台。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [219] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5 是一个8B参数的多模态大模型，通过架构、数据和训练三方面优化，在低资源消耗下超越了更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型训练与推理效率低下，限制了其可访问性和可扩展性。

Method: 提出统一的3D-Resampler架构、无繁重数据工程的统一学习范式，以及兼顾长短推理的混合强化学习策略。

Result: 在OpenCompass评估中超越GPT-4o-latest和Qwen2.5-VL 72B等大模型，在VideoMME上以46.7%显存和8.7%推理时间超越Qwen2.5-VL 7B。

Conclusion: MiniCPM-V 4.5实现了高效率与强性能的平衡，为轻量级多模态模型树立了新标杆。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [220] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 通过优化神经网络激活函数的形状，使用参数化B样条激活函数，显著降低模型误差，但增加训练复杂度和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统静态激活函数（如ReLU）性能受限，通过动态优化激活函数形状可提升模型效率与准确性。

Method: 提出并比较9种训练方法，使用参数化线性B样条激活函数进行双优化。

Result: 在前馈网络中误差降低94%，卷积网络中降低51%，相比ReLU模型显著提升。

Conclusion: 参数化激活函数能显著提升模型性能，但需权衡开发复杂度与推理延迟。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [221] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 提出一种混合强化学习方法，结合ALNS卡车路径与注意力策略调度无人机，优化带电池约束的最后一公里配送，显著降低总耗时。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无人机电池续航与充电约束下难以高效协同卡车与无人机，亟需能动态平衡两者任务分配的智能调度策略。

Method: 结合ALNS（2/3-opt和Or-opt）生成卡车路线，引入小规模指针/注意力策略调度无人机起降序列，使用硬约束掩码和精确时间线模拟器确保续航与充电可行性，采用掩码贪婪/束解码优化使时间。

Result: 在N=50, E=0.7, R=0.1的欧几里得实例上，平均耗时5.203±0.093，优于ALNS（5.349±0.038）2.73%，略优于NN（5.208±0.124）0.10%；在所有种子中均不劣于ALNS，且两次优于NN。

Conclusion: 所提方法能有效平衡卡车等待与无人机任务调度，在严格电池约束下实现更优总耗时，且具备可复现性与稳定性。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [222] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: 提出DSFT方法，通过调整掩码和损失函数提升扩散大语言模型在数学与逻辑任务上的表现，小数据下提升5-10%。


<details>
  <summary>Details</summary>
Motivation: 现有训练方法缺乏对数学与逻辑模式的深入理解，扩散大语言模型在数值敏感和序敏感任务上表现不佳。

Method: 提出DSFT（Diffusion SFT）策略，优化掩码机制与损失函数，引导模型学习数学与逻辑模式，可与预训练、强化学习等方法灵活结合。

Result: 在LLaDA和Dream系列模型上验证，小数据下数学任务提升5-10%，逻辑任务提升约2%。

Conclusion: DSFT是一种高效简单的方法，能显著提升dLLMs的数学逻辑能力，且易于集成，为未来模式学习提供新思路。

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [223] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: 提出MobiGPT统一基础模型，实现基站流量、用户行为和信道质量三类移动数据的精准预测，显著提升准确率与泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有预测方法为不同类型数据定制模型，导致复杂度高、部署成本高，难以应对大规模异构网络需求

Method: 设计MobiGPT统一结构，引入软提示学习识别数据特征，结合时间掩码机制支持短时、长时预测与分布生成三种任务

Result: 在超10万样本真实数据上，MobiGPT较现有模型提升27.37%、20.08%、7.27%准确率，零样本/小样本性能提升超21.51%

Conclusion: MobiGPT作为移动数据基础模型，兼具高精度、强泛化和优迁移能力，适用于多样化网络优化场景

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [224] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE是一种将计算与推理内生集成的架构，相比微调LLM和多智能体系统，在精度、延迟、令牌使用和能耗上表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型无法内生支持高精度数值计算，多智能体方法存在通信开销大、效率低和可扩展性差的问题。

Method: 提出PiMoE架构，通过分别训练文本到计算模块、路由器和专家模块，实现_token_级的计算与推理交替，无需工具调用。

Result: 在两个推理-计算任务上，PiMoE比直接微调LLM更准确，且相较多智能体系统显著降低延迟、令牌使用和GPU能耗。

Conclusion: PiMoE提供了一种高效、可解释且可扩展的下一代科学与工业智能系统范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [225] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA提出先投影后聚合的联邦图学习框架，通过重要性感知去噪梯度，提升异构图数据下的收敛稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习中领域偏斜导致客户端模型表示不兼容，传统聚合方法因梯度噪声大而失效。

Method: FedIA采用两阶段投影优先策略：服务器端Top-$\rho$掩码保留5%关键坐标，结合影响正则化动量抑制异常客户端。

Result: 在Twitch和Wikipedia等数据集上，FedIA比9种基线方法收敛更稳定、精度更高，且无额外上行流量和极少服务器内存开销。

Conclusion: 动态投影机制保持了最优收敛速率，证明了投影优先策略在联邦图学习中的有效性与实用性。

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [226] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: SBVR是一种新型LLM量化方法，通过硬件友好的比特向量求和实现高精度压缩与快速推理，在4-bit下比FP16快2.21-3.04倍。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法要么忽略LLM权重的高斯分布（如RTN），要么因内存访问不规律导致推理慢（如码本法），亟需兼顾精度与速度的量化方案。

Method: 提出SBVR，将权重映射为遵循高斯分布的非均匀编码点，并设计自定义CUDA核函数，直接在编码格式下执行矩阵向量乘法，避免解压缩。

Result: 在多种模型上达到SOTA的困惑度和准确率，4-bit量化下相较FP16实现2.21x-3.04x端到端生成速度提升。

Conclusion: SBVR首次实现高斯感知量化与硬件高效执行的统一，为LLM低比特部署提供了高效新范式。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [227] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 构建大规模基准评估LLM的地理路径认知能力，发现其转向路径能力有限且自信度高但错误多。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏量化指标、数据集有限且研究层次不清晰，需系统评估LLM的地理路径认知能力。

Method: 构建包含3.6万条全球12个大都市路径的基准数据集，提出PathBuilder工具实现自然语言与路径双向转换，并设计新评估框架测试11个SOTA LLM的路径逆转任务。

Result: LLM在路径逆转任务中表现差，多数路径无法返回起点或接近最优路径，且生成鲁棒性低、对错误答案置信度高。

Conclusion: LLM当前难以可靠地进行地理路径推理，亟需改进其空间认知能力与不确定性表达。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [228] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 提出MCoT框架，实现多模态对话中自我中心到参照中心方位的高精度推理，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 在无GPS和详细地图的室内环境中，对话代理需准确理解egocentric指令（如“我右边”）并转换为allocentric方向，现有方法在此多模态空间推理任务上表现不足。

Method: 设计MCoT框架，结合ASR语音与地标坐标，通过三步结构化推理：提取空间关系、映射坐标到绝对方向、推断用户朝向，并采用课程学习在台湾LLM上训练。

Result: 在清洁语音上达到100%方位准确率，ASR转录下为98.1%，显著优于单模态与非结构化方法，且在噪声、语码转换、跨域与语言变异下保持鲁棒性。

Conclusion: 结构化MCoT推理为资源受限场景下可解释、高效的具身导航提供了有效路径。

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [229] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 提出一种变分任务向量合成方法，通过贝叶斯框架估计样本特异性组合系数，引入脉冲-板状先验和门控采样机制，提升稀疏高维空间中的稳定性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量合成方法在任务层面操作，忽略样本特异性，且在高维稀疏空间中存在方差大、采样效率低的问题。

Method: 采用贝叶斯推断框架，将组合系数视为潜在变量，引入脉冲-板状先验以促进稀疏性，并设计门控采样机制基于不确定性与重要性过滤系数。

Result: 在所有数据集上均优于现有方法，显著降低方差，提升可解释性与泛化能力。

Conclusion: 所提方法通过选择性利用最可靠的信息成分，建立了高效且有效的任务向量合成新标准。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [230] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: 提出MolPILE，一个包含2.22亿化合物的大规模、多样化、严格筛选的分子数据集，显著提升分子表示学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有小分子数据集规模小、多样性不足，限制了化学信息学中分子表示学习的效果。

Method: 从6个大型数据库中通过自动化流程构建MolPILE数据集，包含2.22亿化合物，并对现有预训练数据集进行系统分析。

Result: 在MolPILE上重新训练现有模型后，泛化性能显著提升，为化学领域提供了类似ImageNet的标准数据集。

Conclusion: MolPILE解决了化学分子预训练数据匮乏的问题，推动了基础模型在化学领域的应用与发展。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [231] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP通过优化多令牌预测提升LLM推理速度，实现2.03倍加速且无质量损失


<details>
  <summary>Details</summary>
Motivation: 传统自回归生成存在吞吐瓶颈，多令牌预测（MTP）在训练中有效但推理加速潜力未被挖掘

Method: 通过位置共享权重微调MTP头，结合自蒸馏数据和语言感知动态词汇压缩，提升多步草稿质量与推理效率

Result: 在七个基准上平均加速2.03倍，较原始MTP提升82%，保持无损输出质量

Conclusion: FastMTP轻量易部署，可无缝集成现有推理框架，是高效加速LLM推理的实用方案

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [232] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文提出M-DSL算法，通过引入数据异构性度量来优化分布式群体学习中的多工人选择，提升在非i.i.d.数据下的模型性能与收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式群体学习（DSL）在非i.i.d.数据下性能下降，缺乏对数据异构性如何影响训练精度的理论指导。

Method: 提出M-DSL算法，定义新的非i.i.d.度量指标，基于数据异构性选择贡献显著的多工人进行全局模型更新，并提供收敛性理论分析。

Result: 在多种异构数据集和非i.i.d.设置下，M-DSL显著优于基准方法，验证了其性能提升与网络智能增强。

Conclusion: M-DSL有效应对数据异构性挑战，为DSL提供了理论与实践双赢的解决方案。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [233] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: 提出GnnXemplar，一种基于认知科学示例理论的全局图神经网络解释方法，通过代表性节点和自然语言规则提升可解释性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有全局解释方法在大规模图中因子图重复少、属性维度高、结构-属性交互复杂而失效，亟需更鲁棒的全局解释框架。

Method: 采用示例理论，通过反k近邻覆盖最大化选取代表性节点（exemplars），并利用大语言模型自优化提示生成自然语言解释规则。

Result: 在多个基准上显著优于现有方法，在保真度、可扩展性和人类可解释性上表现优异，经60人用户研究验证。

Conclusion: GnnXemplar为图神经网络提供了高效、可扩展且人类可理解的全局解释新范式。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [234] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: GETAD通过融合道路网络图结构与语义信息，提升轨迹异常检测的精度，尤其在复杂路网中发现细微异常。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将轨迹视为采样点序列，忽略道路网络的拓扑约束与语义信息，导致异常检测精度不足。

Method: 采用图注意力网络学习路网感知嵌入，结合图编码与Transformer解码器，通过多目标损失函数优化轨迹建模，并引入CW-NLL异常评分机制。

Result: 在真实与合成数据集上，GETAD在路网约束环境下显著优于现有方法，尤其擅长检测细微异常。

Conclusion: 融入图结构与上下文语义的轨迹建模能实现更精准、上下文感知的异常检测。

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [235] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文研究了强化学习预训练为何能实现上下文强化学习（ICRL），并通过Transformer策略评估案例证明预训练损失的全局极小值可实现上下文时序差分学习。


<details>
  <summary>Details</summary>
Motivation: 现有ICRL工作多使用标准RL算法预训练，但未解释为何这些参数能支持无参数更新的上下文学习，本文旨在探究其内在原因。

Method: 通过理论分析，以Transformer为模型，证明在策略评估预训练中，预训练损失的某些全局极小值能够实现上下文时序差分学习。

Result: 证明了在特定条件下，预训练损失的全局极小值具备支持ICRL的能力，尤其在上下文信息充足时可实现无参数更新的TD学习。

Conclusion: ICRL现象源于预训练损失的极小值结构，而非传统RL更新机制，为理解ICRL提供了理论基础。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [236] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 本文综述了深度学习中从SGD到AdamW、Sophia、Muon等主流优化器的发展历程，详细解析其更新规则、技术特点与超参数设置，并探讨了当前优化领域的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习快速发展，大量优化器被提出，但缺乏系统性整理与对比，亟需全面综述以帮助研究者理解现状与未来方向。

Method: 按时间顺序逐一分析代表性优化器，详述其更新规则、核心概念、贡献及默认超参数，并总结优化技术的演进脉络。

Result: 系统梳理了深度学习优化器的发展谱系，清晰呈现每种优化器的技术创新与适用场景，同时指出当前存在的优化难题。

Conclusion: 本文为研究者提供了深度学习优化器的权威参考，可作为理解现状、探索新优化方法的起点。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [237] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出一种可逆的混沌博弈表示法R-CGR，实现生物序列的无信息损失映射与完美重建。


<details>
  <summary>Details</summary>
Motivation: 传统CGR方法在几何映射中丢失序列信息，无法还原原始序列。

Method: 通过显式路径编码和有理数算术精度控制，完整存储每一步的位置与字符信息，实现可逆映射。

Result: 在生物序列分类任务中表现与传统方法相当，同时生成可解释的几何图像并支持完美序列重建。

Conclusion: R-CGR为生物信息学提供了兼顾准确性与可解释性的新范式，适用于深度学习与序列恢复双重需求的场景。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [238] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 提出KANDI方法，结合Kolmogorov-Arnold网络与扩散策略，解决医疗离线强化学习中奖励函数难以定义的问题，并在老年人防跌倒活动中取得优于SOTA的效果。


<details>
  <summary>Details</summary>
Motivation: 医疗离线强化学习面临奖励函数难以直接定义、逆强化学习在复杂环境中难以准确推断以及策略与人类行为难以对齐的挑战，尤其是在老年人物理活动促进场景中。

Method: 提出KANDI框架，利用Kolmogorov-Arnold网络从低跌倒风险老年人行为中学习奖励函数，并结合扩散策略在Actor-Critic框架中实现动作优化与离线强化学习效率提升。

Result: 在PEER临床试验数据上验证了KANDI的实际应用效果，并在D4RL基准上超越现有SOTA方法，有效促进老年人身体活动。

Conclusion: KANDI为医疗场景中的离线强化学习提供了新范式，能有效解决奖励建模与行为对齐难题，具备临床落地潜力。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [239] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: 提出MeshODENet，结合GNN与神经常微分方程，显著提升网格模拟的长期预测精度与稳定性，同时加速计算。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在多查询任务中计算开销大，现有GNN模型在长期预测中存在误差累积和不稳定问题。

Method: 提出MeshODENet框架，融合GNN的空间推理能力与神经常微分方程的连续时间建模能力。

Result: 在一维和二维大非线性变形弹性体问题上，显著优于基线模型，兼具高精度、高稳定性和计算加速。

Conclusion: MeshODENet为复杂结构系统的数据驱动代理模型提供了强大且可推广的解决方案。

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [240] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: AI框架通过图神经网络和贝叶斯优化自动推荐MCMC参数，显著提升稀疏线性系统预条件效果，减少50%搜索成本和约10%收敛迭代次数。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC预条件参数依赖人工或网格搜索，代价高且参数适应性差；需自动化的参数推荐方法提升效率。

Method: 使用图神经网络作为代理模型预测预条件速度，结合贝叶斯获取函数优化参数选择。

Result: 在未见过的病态系统上，该框架以50%搜索成本实现更优预条件，收敛迭代减少约10%。

Conclusion: 该AI驱动框架为MCMC预条件器在大规模系统中的实用化提供了可行路径。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [241] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind是一个基于Transformer的多模态框架，用于持续长期血糖预测，通过交叉注意力和多尺度注意力机制提升预测精度，并引入知识保留模块防止灾难性遗忘，在AIREADI数据集上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有血糖预测模型难以处理多源异步生理信号、长期依赖及持续学习中的灾难性遗忘问题。

Method: 提出GluMind框架，结合交叉注意力融合多模态信号（如活动、压力、心率），多尺度注意力捕捉长期时序依赖，并集成知识保留模块以稳定持续学习。

Result: 在AIREADI数据集上，GluMind相比SOTA模型RMSE降低15%，MAE降低9%，具有更好的稳定性和适应性。

Conclusion: GluMind能有效实现长期、持续的个性化血糖预测，为糖尿病管理提供可靠工具。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [242] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出PGPCA，一种结合非线性流形的概率维度约减方法，优于传统PPCA，适用于神经科学中的高维噪声数据。


<details>
  <summary>Details</summary>
Motivation: 传统PPCA基于线性模型和欧氏空间，无法有效建模神经科学中常见的非线性流形分布数据。

Method: 开发PGPCA，通过先拟合数据流形，构建几何坐标系，并推导数据驱动的EM算法学习模型参数。

Result: PGPCA在模拟和脑数据中优于PPCA，能同时建模流形上及周围的分布，并提供几何与欧氏坐标系的比较检验。

Conclusion: PGPCA是PPCA的泛化，能更准确描述非线性流形上的高维数据，提升神经科学数据分析的效率。

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [243] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 本文提出几种离散时间扩散类模型，用于语音生成，在保持与连续模型相当质量的同时，实现更高效一致的训练与推理。


<details>
  <summary>Details</summary>
Motivation: 传统连续时间扩散模型在训练与推理间存在离散化不一致问题，且受限于加性高斯噪声；离散时间过程可避免这些问题并减少推理步数。

Method: 提出多类离散时间扩散过程，包括加性高斯噪声、乘性高斯噪声、模糊噪声及其混合形式。

Result: 实验表明，所提离散时间模型在主观和客观语音质量上与传统连续模型相当，且训练推理更高效一致。

Conclusion: 离散时间扩散过程是语音生成中优于传统连续模型的有前景替代方案。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [244] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ提出一种高效非均匀向量量化方法，在保持高保真度的同时显著降低向量存储与计算成本。


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量在存储和检索中成本高昂，现有向量搜索技术面临空间与计算效率瓶颈。

Method: 引入逐向量独立学习的非均匀量化器，采用轻量非线性变换实现高效压缩。

Result: NVQ在计算开销极低的情况下，精度超越当前最优方法。

Conclusion: NVQ为高维向量压缩提供了一种兼顾精度与效率的新范式。

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [245] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold是首个仅使用通用Transformer块的流匹配蛋白质折叠模型，无需复杂域特定模块，即可在标准基准上取得 competitive 性能，并支持消费级硬件高效推理。


<details>
  <summary>Details</summary>
Motivation: 挑战蛋白质折叠模型对复杂域特定架构的依赖，探索是否通用架构也能实现高性能。

Method: 采用标准Transformer块加自适应层，结合流匹配目标与结构项进行训练，参数规模达3B，训练数据包括9M蒸馏蛋白结构和实验PDB数据。

Result: SimpleFold-3B在标准折叠基准上表现与SOTA相当，尤其在集合预测上优势明显，且部署和推理效率高。

Conclusion: 通用架构可替代复杂域专用设计，为蛋白质折叠模型提供新范式，推动未来研究方向。

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [246] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: 使用物理信息Kolmogorov Arnold网络（KANs）高效预测量子动力学演化，仅需5.4%数据即可超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 量子系统高维希尔伯特空间导致传统数值方法计算昂贵，现有神经网络需大量数据且物理不可解释。

Method: 提出物理信息KANs，结合Ehrenfest定理约束损失函数，并引入Chain of KANs架构嵌入时间因果性。

Result: 仅用200个样本（传统方法需3700个）实现更高精度，避免虚假振荡，保持物理一致性。

Conclusion: 物理信息KANs在数据效率与物理合理性上显著优于黑箱模型，为量子动力学预测提供新范式。

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [247] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: 使用混合数据集提升反洗钱模型性能，兼顾隐私与实用性


<details>
  <summary>Details</summary>
Motivation: 由于隐私限制，真实交易数据难以获取，纯合成数据性能不足，需寻找更好解决方案

Method: 结合公开真实特征与合成数据构建混合数据集来训练GNN模型

Result: 混合数据集在保护隐私的同时显著提升模型有效性

Conclusion: 混合数据集为金融机构提供了一种实用且可落地的反洗钱系统优化路径

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [248] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL通过主动部分_rollout策略显著提升RL训练效率，减少GPU空闲时间，提高吞吐量和最终准确率。


<details>
  <summary>Details</summary>
Motivation: 当前RL训练中rollout生成占运行时间90%以上，且因响应长度长尾分布导致GPU利用率低，制约可扩展性。

Method: APRIL在rollout阶段超额分配请求，达到目标响应数后即终止，并回收未完成响应用于后续继续生成，避免丢弃任何rollout。

Result: 在GRPO、DAPO、GSPO等算法上，APRIL提升吞吐量达44%，加速收敛，最终准确率最高提升8%，且兼容多种框架与硬件。

Conclusion: APRIL统一系统与算法优化，显著提升RL训练效率，为未来RL系统设计提供新方向。

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [249] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: 提出RCCR方法，通过惩罚序列与其反向互补序列预测差异，显著提升DNA语言模型的反向互补一致性，同时保持或提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有DNA语言模型未能捕捉DNA序列与其反向互补序列的生物对称性，导致预测不一致，影响可靠性。

Method: 引入RCCR，一种模型无关的微调目标，直接惩罚序列与其反向互补序列预测之间的差异。

Result: 在三个主流DNA模型上验证，RCCR显著减少预测翻转和错误，提升RC鲁棒性，且不牺牲任务准确率。

Conclusion: RCCR是一种简单、高效、通用的生物先验嵌入方法，适用于多种基因组任务，是理想的微调策略。

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [250] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: 提出Symphony-MoE，通过融合多个异构预训练模型的专家，实现无需从头训练的高性能混合专家模型。


<details>
  <summary>Details</summary>
Motivation: 现有上采样方法因所有专家源自单一稠密模型，导致专家多样性不足；亟需利用多源异构模型提升MoE性能。

Method: 提出双阶段框架：第一阶段通过层感知融合与激活为基础的功能对齐实现无训练对齐；第二阶段仅需轻量级路由器训练协调专家。

Result: 在多域任务和分布外泛化上显著优于基线，成功整合来自Llama2-Chat、Code Llama等不同模型的专家。

Conclusion: Symphony-MoE有效解决了多源专家参数空间不一致问题，为高效构建异构MoE提供了新范式。

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [251] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: 本文理论解释了SigLIP模型中可训练的温度和偏置如何通过Sigmoid损失实现表示对齐，并提出新概念(m, b_rel)-Constellations以解释其成功与局限性。


<details>
  <summary>Details</summary>
Motivation: 理解SigLIP和SigLIP2为何在对比预训练中表现优异，填补理论空白。

Method: 提出(m, b_rel)-Constellations数学框架，分析Sigmoid损失在温度和偏置控制下收敛条件，并推导表示质量与维度关系。

Result: 理论证明了SigLIP在检索任务中的优势，解释了模态差距，并给出了高质量表示所需的最小维度。

Conclusion: 通过引入相对偏置重参数化Sigmoid损失，可改善训练动态，为未来对比学习提供理论指导。

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [252] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: 本文首次系统综述了可解释图神经网络（XGNNs）在痴呆研究中的应用，涵盖多种痴呆类型，并提出分类法与未来挑战。


<details>
  <summary>Details</summary>
Motivation: 痴呆具有临床和生物异质性，传统诊断困难；GNNs虽能建模脑连接，但缺乏鲁棒性与可解释性，限制了临床应用。

Method: 提出首个针对痴呆研究的XGNN综述，构建可解释性方法分类体系，比较临床场景下模型表现，并探讨与大语言模型（LLMs）融合的潜力。

Result: 系统梳理了XGNN在阿尔茨海默病、帕金森病、轻度认知障碍等中的应用，识别出关键生物标志物与脑网络异常，指出现有模型在泛化性与领域覆盖上的不足。

Conclusion: XGNN有潜力推动可信、可解释、可扩展的痴呆诊断，未来需解决泛化性、多模态整合及LLMs协同等问题，以实现临床落地。

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [253] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出Interaction Topological Transformer (ITT)，通过多尺度拓扑建模实现多孔材料性能的高效预测


<details>
  <summary>Details</summary>
Motivation: 多孔材料结构复杂、数据稀疏且不均，传统方法难以建模多尺度结构-性能关系

Method: ITT框架结合互动拓扑结构与Transformer，通过自监督预训练（60万无标签结构）加有监督微调，提取并融合结构、元素、原子和成对元素层次特征

Result: 在吸附、传输和稳定性预测上达到SOTA性能，具有优异的迁移能力

Conclusion: ITT为异构多孔材料的智能发现提供了可扩展、原理清晰的深度学习路径

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [254] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: 提出DS-Diffusion模型，通过风格引导核和层次去噪机制避免重训练、减小分布偏差并提升可解释性


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型需重训练以支持条件引导，存在分布偏差和推理不可解释问题

Method: 采用风格引导核扩散框架与基于时间信息的层次去噪机制（THD）

Result: 相比ImagenTime，预测分下降5.56%，判别分下降61.55%，分布偏差减小，推理更可解释，无需重训练

Conclusion: DS-Diffusion在灵活性、适应性与可解释性上显著优于现有方法

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [255] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT通过在行动前增加反思步骤，显著提升大语言模型在交互任务中的决策准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在交互决策中易积累错误且缺乏有效的自我修正机制。

Method: 提出REBACT方法，在每一步行动前引入反思机制，以纠正错误并适应环境反馈。

Result: 在ALFWorld、WebShop和TextCraft三个任务上，REBACT分别将成功率提升6.72%、24%和0.5%，且计算开销极低。

Conclusion: REBACT通过轻量级反思机制有效提升LLM交互决策的性能与稳定性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [256] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: 提出Flow Marching算法，构建生成式PDE基础模型，结合P2VAE和FMT，实现高效、稳定且具不确定性感知的物理系统建模。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型多为确定性Transformer，缺乏生成灵活性，难以满足科学与工程中对不确定性建模和长期稳定推理的需求。

Method: 引入Flow Marching算法联合采样噪声级与物理时间步，学习统一速度场；设计Physics-Pretrained VAE（P2VAE）压缩状态空间，并构建高效Flow Marching Transformer（FMT）结合扩散强制与潜在时间金字塔。

Result: 在2.5M条PDE轨迹上预训练，计算效率提升15倍，实现长期滚动稳定、低漂移，并在未知Kolmogorov湍流上展现优异的少样本适应与不确定性分层集成结果。

Conclusion: 生成式PDE基础模型显著优于确定性模型，为真实世界物理系统建模提供了更可靠、可扩展且具不确定性感知的新范式。

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [257] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt是一种参数高效微调方法，用极少可训练参数实现接近全量微调的性能。


<details>
  <summary>Details</summary>
Motivation: 传统微调内存和计算开销大，现有PEFT方法仍需较多参数，需更高效的替代方案。

Method: 通过行和列的对角矩阵缩放预训练权重矩阵，仅需n+m个可训练参数实现高秩更新。

Result: 在GLUE、算术推理和常识推理基准上，性能媲美全量微调和SOTA PEFT方法，参数量减少数个数量级。

Conclusion: HyperAdapt在保持高性能的同时极大降低参数开销，是高效微调的有力候选方案。

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [258] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 提出一种基于列空间的矩阵聚类新框架SCoS，通过张量分解实现子空间聚类，显著提升高噪环境下聚类精度。


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类假设数据为向量，难以处理矩阵形式的高维数据，亟需直接建模矩阵样本的聚类方法。

Method: 构造三阶张量并采用块项分解（BTD）联合估计聚类成员与部分共享子空间，提出可扩展优化算法。

Result: 首次给出该框架的可辨识性理论，实验表明在高光谱图像数据上优于现有方法，尤其在高噪声下表现更鲁棒。

Conclusion: SCoS为超越向量结构的高维数据聚类提供了新范式，具有广泛应用潜力。

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [259] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: 使用图机器学习加速农药设计，构建首个蜜蜂毒性的大型数据集ApisTox，发现药物发现模型在农药领域泛化效果差，需领域专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统农药研发周期长、生态风险高，受药物发现中计算方法启发，需高效、安全的农药设计方法。

Method: 构建ApisTox数据集，评估分子指纹、图核、GNN和预训练 Transformer 等多种图机器学习模型用于农药毒性分类。

Result: 药物发现中有效的模型在农药任务上表现不佳，表明需开发针对农药特性的专用模型与基准。

Conclusion: 未来需建立全面的农药AI基准套件，并设计适配农药发现独特挑战的机器学习模型。

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [260] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 提出广义双模拟度量（GBSM），严格证明其三个数学性质，并在多MDP场景中提升策略迁移、状态聚合和采样估计的理论界限。


<details>
  <summary>Details</summary>
Motivation: 现有双模拟度量（BSM）在单MDP中有效，但在多MDP场景如策略迁移中缺乏理论支持，亟需广义化并严格分析其数学性质。

Method: 形式化定义广义双模拟度量（GBSM），并证明其对称性、跨MDP三角不等式和相同状态空间距离上界三项基本性质。

Result: 获得比标准BSM更紧的理论界限，提供估计的闭式样本复杂度，超越现有渐近结果。

Conclusion: GBSM在多MDP任务中理论更严谨、性能更优，数值实验验证其有效性。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [261] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 本文提出一种结合强化学习与大语言模型的电商支付欺诈检测方法，通过LLM优化奖励函数，提升检测精度并实现零样本能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习奖励函数设计依赖大量人工经验，难以应对支付场景的复杂性和多变性。

Method: 将交易风险建模为多步马尔可夫决策过程（MDP），利用大语言模型（LLM）迭代优化奖励函数，增强强化学习的欺诈检测性能。

Result: 在真实数据上的实验表明，该方法显著提升检测准确率，具备强鲁棒性和零样本能力，且在长期评估中表现稳定。

Conclusion: LLM可有效增强工业级强化学习应用，为支付欺诈检测提供新范式。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [262] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: 提出周期CNN架构，理论证明其可逼近高维脊函数，优于传统CNN。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在高维脊函数逼近上存在局限，需探索更具表达力的架构。

Method: 引入周期性边界条件的卷积层，构建周期CNN，并给出严格的逼近定理。

Result: 周期CNN可逼近依赖d-1个线性变量的d维脊函数，而d-2维以下不可行，明确其表达能力边界。

Conclusion: 周期CNN在图像、物理和材料科学等高维周期性问题中具实用价值，拓展了CNN的理论基础。

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [263] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: 提出MOMEMTO，一种基于补丁记忆模块的时间序列基础模型，有效缓解过泛化问题，在多数据集上实现更优的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有重建模型易过泛化，内存方法训练成本高且难与时间序列基础模型集成。

Method: 引入补丁级记忆模块，利用预训练编码器初始化，通过注意力机制更新，支持多领域联合微调。

Result: 在23个单变量基准数据集上，MOMEMTO在AUC和VUS指标上超越基线，显著提升骨干TFM性能，尤其在少样本场景下。

Conclusion: MOMEMTO通过记忆机制有效抑制过泛化，实现高效、通用的时序异常检测，为TFM落地提供新思路。

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [264] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR:  diagonal linear网络的训练轨迹与Lasso正则化路径密切相关，训练时间充当逆正则化参数。


<details>
  <summary>Details</summary>
Motivation: 已有研究指出对角线性网络在小初始化下收敛到最小1-范数线性预测器，但其完整训练轨迹与Lasso路径的关系尚未深入探讨。

Method: 通过理论分析和数值模拟，研究训练轨迹与Lasso路径的关联，并在单调性假设下证明精确连接，一般情形下证明近似连接。

Result: 训练时间等价于Lasso的逆正则化参数，轨迹与Lasso路径高度相关，单调情形下精确一致，一般情形下近似一致。

Conclusion: 对角线性网络的动态过程隐式实现了类似Lasso的路径正则化，为理解神经网络的隐式正则化提供了新的理论视角。

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [265] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: 使用集成概率机器学习提升数据驱动的一致性故障诊断中的不确定性量化与诊断性能


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在故障诊断中虽预测能力强，但难以评估置信度，而在基于一致性的诊断中，误报敏感性高，亟需不确定性量化方法

Method: 提出基于集成概率机器学习的诊断框架，通过量化和自动化预测不确定性来改进诊断性能

Result: 在多个案例中通过消融和对比分析验证，诊断指标持续提升

Conclusion: 该框架有效提升了数据驱动一致性诊断的可靠性与鲁棒性

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [266] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出一种基于预训练扩散模型的轻量级数据同化方法，无需再训练，结合粒子滤波用于动力系统状态估计。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法依赖复杂模型或再训练，缺乏通用性和效率，需更轻量、通用的解决方案。

Method: 利用预训练扩散模型作为动力系统模拟器，结合粒子滤波框架，在不进行额外训练的情况下实现数据同化。

Result: 方法在GenCast气象模型上验证，有效提升状态估计精度并保持计算效率。

Conclusion: 该方法为数据同化提供了一种通用、轻量且无需再训练的新范式，适用于各类预训练扩散模型。

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [267] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 提出LoRD和B-LoRD模型，通过保留低秩、非负、双随机约束并仅松弛正交性，结合块对角正则化，显著提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有谱聚类等方法过度松弛低秩、非负、双随机和正交约束，影响聚类效果。

Method: 提出LoRD仅松弛正交约束，引入块对角正则化得B-LoRD，并将双随机约束转为线性凸约束以保证数值可解性。

Result: 理论证明了正交性与块对角性的等价性，建立了梯度Lipschitz连续性，设计了全局收敛的投影梯度算法。

Conclusion: LoRD和B-LoRD在实验中表现优异，代码已开源，为图聚类提供了更严格的约束优化框架。

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [268] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出SWE和SVoD方法，有效避免新神经元失活，提升网络扩展性能


<details>
  <summary>Details</summary>
Motivation: 新添加的神经元常因无法适应训练网络而失活，导致容量增长失效

Method: 采用Shared-Weights Extender（SWE）耦合新旧神经元，结合Steepest Voting Distributor（SVoD）基于梯度分配神经元

Result: 在四个数据集上验证，显著抑制神经元失活，性能优于其他扩展方法和基线

Conclusion: SWE与SVoD协同工作，实现高效无失活的神经网络动态扩展

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [269] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO通过引入优势校准和非对称裁剪，解决GRPO在同质错误样本上无法学习的问题，显著提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在所有响应同为正确或错误时无法产生有效梯度，导致学习信号丢失，尤其在同质错误组中表现致命。

Method: NGRPO提出优势校准（引入虚拟最大奖励样本）和非对称裁剪（不同约束正负样本更新幅度），将错误转化为有效学习信号。

Result: 在Qwen2.5-Math-7B上，NGRPO在MATH500、AMC23、AIME2025等数学基准上显著超越PPO、GRPO、DAPO和PSR-NSR。

Conclusion: NGRPO成功将同质错误转化为稳定学习信号，大幅提升数学推理能力，代码已开源。

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [270] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: 首次分析异配性对图水平任务的影响，提出频率自适应模型优于传统频率主导模型。


<details>
  <summary>Details</summary>
Motivation: 异配性在节点级任务中已被广泛研究，但在图水平任务中的影响尚不明确，亟需理论与实证探索。

Method: 构建图水平标签分类体系，聚焦基于动机的局部结构标签，采用基于能量的梯度流分析，揭示动机检测需混合频率动态。

Result: 理论证明动机目标与全局频率主导不一致，实验表明频率自适应模型在合成与真实分子属性预测任务中性能更优。

Conclusion: 建立了图水平学习中异配性的新理论理解，为设计高效GNN架构提供指导。

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [271] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出一种动态优化后门触发器的方法，解耦主任务与后门任务，在联邦学习中实现更持久的后门攻击


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击依赖固定触发器，易被良性更新稀释且难以对抗联邦防御

Method: 在最小-最大框架下动态优化后门触发器，内层最大化毒化与良性样本性能差距，外层将自适应触发器注入本地模型

Result: 在计算机视觉和自然语言任务上超越六种现有方法，对六种防御算法均表现优异，且易于集成到现有攻击方法中

Conclusion: 该方法有效解耦主任务与后门任务，显著提升后门攻击在联邦学习中的鲁棒性与持久性

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [272] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 将神经算法推理（NAR）重构为马尔可夫决策过程，提出GNARL框架，提升组合优化问题的求解能力并突破对专家算法的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有NAR方法无法无后处理生成有效解、难以处理多解问题、在NP难问题上表现差，且require已有强算法。

Method: 将算法轨迹建模为马尔可夫决策过程，结合模仿学习与强化学习，提出GNARL框架实现通用图问题求解。

Result: 在CLRS-30上达到高准确率，媲美或超越窄域NAR方法，且无需专家算法也能有效应用。

Conclusion: GNARL为算法学习提供了更通用、强大的范式，突破了传统NAR的关键限制。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [273] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: 本文提出使用信念网络（CN）平衡贝叶斯网络（BN）的隐私与效用，通过遮蔽而非加噪实现隐私保护，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有BN模型发布时忽视隐私，加噪保护方法会严重降低模型效用，亟需一种既能防追踪攻击又不牺牲准确性的新方法。

Method: 引入信念网络（CN）作为BN的遮蔽版本，通过结构化不确定性掩盖训练数据敏感信息，避免引入噪声，同时识别需隐藏的关键学习信息以防止模型恢复。

Result: 数值实验表明，通过调整CN超参数可灵活控制隐私强度，CN在有效防御追踪攻击的同时保持高推理效用。

Conclusion: CN是一种原则性强、实用且有效的方法，为构建隐私感知的概率图模型提供了新方向。

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [274] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: 提出HEROS方法，通过ζ策略在保证高精度的同时显著降低资源消耗，实现绿色在线学习


<details>
  <summary>Details</summary>
Motivation: 现有集成方法过于关注预测性能，忽视模型计算成本，不符合可持续性要求

Method: 引入马尔可夫决策过程，构建HEROS框架，提出ζ策略选择近优模型进行训练

Result: 理论证明ζ策略接近最优性能且资源消耗更低，实验证明其在11个数据集上准确率领先且更节能

Conclusion: ζ策略是绿色在线学习的有力贡献，实现性能与资源效率的平衡

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [275] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 本文建立了异步更新下Polyak-Ruppert平均Q学习的中心极限定理，并给出了非渐近形式和函数形式的收敛结果。


<details>
  <summary>Details</summary>
Motivation: 为理解Q学习在异步更新下的统计收敛性质，特别是其分布收敛行为，需建立严格的中心极限定理。

Method: 推导了非渐近中心极限定理，以Wasserstein距离刻画收敛速率，并证明了部分和过程弱收敛于布朗运动的函数中心极限定理。

Result: 得到了依赖于迭代次数、状态-动作空间大小、折扣因子和探索质量的显式收敛速率，并证实了部分和过程的布朗运动收敛性。

Conclusion: 该工作为Q学习的统计推断提供了理论基础，首次在异步设置下建立了完整的中心极限定理框架。

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [276] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 利用光电子器件的自然衰减特性替代传统数字计算，实现更节能的TTFS脉冲神经网络，大幅提升能效并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统TTFS编码因需计算时间衰减函数和权重乘法而消耗大量能量，未能实现理论上的高能效优势。

Method: 设计一种铟氧化物光电子突触，利用其天然信号衰减直接实现时间衰减函数，并与权重融合；提出新型量化神经网络到SNN转换算法，支持Transformer架构。

Result: 在七个GLUE基准上取得SOTA准确率，能效比现有领先SNN提升1.77倍，基于22nm工艺的全面能耗分析验证。

Conclusion: 该工作确立了将器件物理特性直接转化为计算原语的新范式，实现硬件-软件协同设计，推动高能效SNN发展。

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [277] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs是基于仿真数据训练的神经网络，理论上可实现贝叶斯最优预测，并能学习经验方法无法捕获的不可观测科学量。


<details>
  <summary>Details</summary>
Motivation: 现有仿真接地神经网络缺乏理论基础，且在真实标签稀缺时表现优异但机制不明。

Method: 建立仿真接地学习的理论框架，证明其为 amortized 贝叶斯推断，推导泛化界，并提出基于仿真机制的可解释性方法。

Result: SGNNs在参数恢复、模型选择中性能优于AIC，鲁棒性强，可学习不可观测变量，且提供科学一致的解释。

Conclusion: SGNNs是数据有限场景下科学预测的有理论基础且实用的框架。

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [278] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net是一种新型低秩架构，通过跨层残差重建激活信息，在减少参数和内存开销的同时提升了大语言模型预训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩方法存在性能下降、计算开销大和激活内存节省有限三个主要问题。

Method: 提出CR-Net，利用层间激活残差的低秩特性，采用双路径结构重建激活，并设计专用重计算策略降低内存需求。

Result: 在60M至7B参数规模的预训练实验中，CR-Net相比现有方法性能更优、计算与内存需求更低。

Conclusion: CR-Net有效解决了低秩架构的三大瓶颈，为高效大模型预训练提供了新范式。

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [279] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 本文综述了无监督表示学习的最新理论进展，探讨深度学习模型为何能从无标签数据中学习到有效表示。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型（如视觉基础模型）通过自监督或去噪自编码器从大量无标签数据中学习表示，但其表示机制和性能优异的原因尚不清晰，传统理论难以解释。

Method: 结合统计学与优化理论的数学工具，分析无监督表示学习的理论基础。

Result: 提供了近期在无监督表示学习理论方面的进展综述，并提出了作者的贡献。

Conclusion: 需融合统计与优化理论才能彻底理解深度学习中无监督表示学习的机制与性能。

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [280] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 提出一种完全可学习的神经奖励机（FLNRM），无需先验知识即可端到端学习符号接地和自动机，优于RNN方法。


<details>
  <summary>Details</summary>
Motivation: 非马尔可夫强化学习任务需要推理整个状态-动作轨迹，现有符号方法依赖预定义符号接地或任务先验，限制了适用性。

Method: 引入完全可学习的神经奖励机（FLNRM），联合学习符号接地函数与有限状态自动机，实现端到端训练。

Result: FLNRM在不依赖先验的情况下性能超越RNN基线，且因自动机结构更具可解释性。

Conclusion: FLNRM兼具深度强化学习的易用性与符号方法的可解释性，为非马尔可夫任务提供了新范式。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [281] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge提出了一种统一的多模态框架，通过轻量级双向潜在对齐模块和两阶段训练策略，在不从头训练LLM的情况下实现了视觉-语言理解、生成和检索的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型通常孤立处理不同任务或需要从头训练LLM，导致计算成本高、跨模态泛化能力差。

Method: 采用语言中心设计，复用预训练LLM，引入轻量级双向潜在对齐模块；提出两阶段解耦训练：监督微调+潜在空间对齐，结合语义引导的扩散训练通过可学习查询嵌入对齐跨模态潜在空间。

Result: 在多个基准上实现竞争性或SOTA性能，验证了潜在空间对齐在统一多模态建模中的有效性。

Conclusion: OmniBridge通过模块化和高效对齐机制，实现了多任务统一建模，降低计算开销并提升跨模态泛化能力。

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [282] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: 提出一种基于Transformer编码器的GAN混合模型，用于生成高质量的信用卡欺诈样本，显著提升不平衡数据下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统过采样方法（如SMOTE）生成的样本过于简单，无法捕捉复杂欺诈模式；CTGAN和TVAE虽有提升，但仍难以建模高维依赖关系。

Method: 采用GAN结合Transformer编码器的混合架构，利用自注意力机制学习特征间复杂交互，生成真实 fraudulent 交易样本。

Result: 在公开信用卡欺诈数据集上，相较于SMOTE、CTGAN、TVAE等方法，所提模型在Recall、F1-score和AUC指标上均有显著提升。

Conclusion: Transformer-GAN能有效缓解类别不平衡问题，在信用卡欺诈检测任务中表现出优越的合成样本生成能力与分类性能。

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [283] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一种基于潜在扩散模型的黑盒对抗攻击方法，通过联合注意力蒸馏实现跨架构高效迁移


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击方法依赖特定网络架构或需要大量查询，导致迁移性差、查询成本高

Method: 提出JAD框架，利用从CNN和ViT中蒸馏的注意力图引导潜在扩散模型生成对抗样本，聚焦跨架构敏感区域

Result: JAD在攻击泛化性、生成效率和跨架构迁移性上优于现有方法

Conclusion: JAD为黑盒对抗攻击提供了一种架构无关、高效且有效的新范式

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [284] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: MF算法在无需反向传播的情况下，比BP更准确、更节能，且硬件效率更高。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络因反向传播导致计算与能耗过高，亟需可持续的替代训练方法。

Method: 在MLP和CNN架构上对比FF、CaFo和MF三种无BP训练方法，使用Optuna调参并统一早停策略，结合NVML和CodeCarbon测量能耗与碳足迹。

Result: MF在准确率上超越BP，训练时间缩短最多34%，能耗降低最多41%，并收敛至更优的验证损失极小点。

Conclusion: MF是兼具精度与能效的首选无BP训练方法，为未来绿色AI提供了数据驱动的演进路径。

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [285] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: DBVI通过可学习的数据依赖初始化改进DDVI，显著提升深高斯过程推理效率与精度。


<details>
  <summary>Details</summary>
Motivation: DDVI使用固定先验启动反向扩散，与真实后验差距大，导致采样效率低、收敛慢。

Method: 提出DBVI，采用神经网络参数化可学习初始分布，基于诱导输入进行 amortized 优化，并结合Girsanov和Doob桥理论重构ELBO。

Result: DBVI在回归、分类和图像重建任务中，预测精度、收敛速度和后验质量均优于DDVI及其他变分基线。

Conclusion: DBVI通过数据驱动的扩散桥设计，在保持数学严谨性的同时，实现了高效可扩展的深高斯过程推理。

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [286] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: 提出AxelGNN，一种受Axelrod文化传播模型启发的新GNN架构，通过相似性门控和特征段级复制机制解决过平滑、异质关系处理和特征向量不可分问题。


<details>
  <summary>Details</summary>
Motivation: 现有GNN存在特征过平滑、难以处理异质关系、特征向量作为整体处理导致灵活性不足等根本限制。

Method: AxelGNN引入相似性门控概率交互、特征段级复制机制，并保持全局极化以维护节点区分性，统一处理同质与异质图。

Result: 在节点分类和影响力估计任务上，AxelGNN在不同同质-异质特性图上均优于或持平现有SOTA方法。

Conclusion: AxelGNN通过生物学启发的动态机制有效解决GNN核心局限，为异构图学习提供统一高效框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [287] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 本文在迁移学习框架下研究非上下文多臂老虎机问题，提出KL-UCB-Transfer算法，利用源分布信息降低累积遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机未考虑源分布先验信息，迁移学习场景下需要利用源数据提升目标域表现。

Method: 推导包含转移参数(d_k, L_k, N'_k)的渐近下界，并设计KL-UCB-Transfer指数策略匹配该下界。

Result: 在高斯分布下，KL-UCB-Transfer达到理论下界，仿真显示其显著优于无先验基线。

Conclusion: 利用源分布的先验信息可有效提升多臂老虎机性能，尤其在源目标分布相近时。

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [288] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 本文研究了深度学习模型在安全关键应用中的鲁棒性问题，涵盖对抗样本、域泛化和大语言模型越狱三个方向，并提出了新的算法与防御方法。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型广泛用于安全关键场景，其易受对抗攻击的特性亟需解决，需提升模型在对抗扰动、分布外泛化和恶意提示下的鲁棒性。

Method: 提出针对计算机视觉的对抗样本新训练范式与认证算法、面向域泛化的新型泛化算法（用于医学影像等）、以及针对大语言模型的新型越狱攻击与防御机制。

Result: 在医学影像、分子识别、图像分类等任务中实现最优泛化性能，并在对抗样本和LLM越狱领域取得前沿进展。

Conclusion: 通过系统性研究对抗鲁棒性三大关键问题，本文为构建更安全可靠的AI系统提供了理论与方法支持。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [289] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: 提出DRO-REBEL方法，通过鲁棒优化缓解RLHF中的过优化问题，在多个基准上实现更强的鲁棒性和最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有离线RLHF方法存在过优化，模型对奖励误设过拟合，偏离训练中观察到的偏好行为。

Method: 引入基于p-Wasserstein、KL和χ²模糊集的鲁棒REBEL更新，利用对偶性简化为相对奖励回归，避免PPO式剪裁与辅助值网络。

Result: 理论证明O(n^{-1/4})估计界，通过局部Rademacher分析达到最优O(n^{-1/2})速率；χ²-REBEL在实验中表现最稳。

Conclusion: DRO-REBEL在保证覆盖率与最优速率间存在权衡，χ²版本兼具理论优势与强实证性能。

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [290] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO是一种可扩展的多标签因果发现方法，用于处理高维事件序列，通过双Transformer模型高效构建全局马尔可夫边界。


<details>
  <summary>Details</summary>
Motivation: 事件序列中的因果关系识别在医疗和车辆诊断等领域仍是未解难题，传统条件独立性测试计算成本过高。

Method: 使用两个预训练的因果Transformer对每个事件序列进行一次性因果图推断，并通过自适应频率融合聚合结果，重建标签的全局马尔可夫边界。

Result: 在包含29,100种事件类型和474个不平衡标签的汽车故障预测数据集上，CARGO实现了高效且结构化的因果推理。

Conclusion: CARGO通过两阶段设计避免了全数据集条件测试的不可行性，为大规模事件序列的因果分析提供了新范式。

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [291] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS是一种新颖的联邦学习框架，通过基于适应度的客户端选择和分槽聚合，提升在非IID数据和对抗攻击下的鲁棒性与公平性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在医疗等敏感领域面临非IID数据、客户端不可靠和对抗攻击的挑战，亟需更鲁棒和公平的训练机制。

Method: FedFiTS提出三阶段参与策略：自由训练、自然选择与分槽团队参与，结合动态评分、自适应阈值和队列调度，实现信任感知聚合与公平客户端选择。

Result: 理论分析证明其收敛性边界，通信复杂度低于FedAvg；实验在医学影像、视觉和农业数据上均优于FedAvg、FedRand和FedPow，在准确率、收敛速度和抗攻击性上显著提升。

Conclusion: FedFiTS通过整合信任与 fairness 机制，显著提升联邦学习在现实敏感场景中的可扩展性与安全性，特别适合医疗与跨领域部署。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [292] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: 提出标准差向量和聚类向量来分析大语言模型权重特征，发现其可区分模型差异并保持微调后相关性稳定。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型架构与参数特性理解不足，亟需有效方法量化权重分布与相关性。

Method: 提出标准差向量（归一化投影矩阵标准差）和聚类向量（K-Means分组奇异值）描述模型权重特征。

Result: 两向量能有效区分不同模型，同一模型族内相似性高；LoRA微调后标准差向量受数据集影响，聚类向量保持与预训练模型高度一致。

Conclusion: 权重的分布与相关性特征可被结构化表征，聚类向量对微调鲁棒，揭示模型内在结构稳定性。

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [293] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL通过异步并行生成与训练和飞行中权重更新，实现LLM强化学习的高效加速和数据新颖性，比传统方法快2倍。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在扩展时难以兼顾AI加速器利用率与数据策略内性，导致训练效率低下。

Method: 采用并发异步数据生成与模型训练，引入飞行中权重更新机制，使生成引擎在不中断情况下接收最新权重。

Result: 在128块H100 GPU上实验显示，PipelineRL学习速度提升约2倍，同时保持高策略内性数据。

Conclusion: PipelineRL有效平衡了硬件效率与数据新鲜度，为大规模LLM强化学习提供可扩展解决方案。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [294] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU通过融合时空语义与用户生活方式建模，显著提升移动行为预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉人类移动行为中的语义复杂性与生活方式规律，亟需更精细的建模框架。

Method: 提出GSTM-HMU框架，包含时空概念编码器(STCE)、认知轨迹记忆(CTM)、生活方式概念库(LCB)及任务生成头，联合建模地理位置、POI语义、时间周期与用户偏好。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare四个数据集上，于下一位置预测、轨迹身份识别和时间估计任务中均显著超越基线模型。

Conclusion: 生成式建模为构建更具鲁棒性、可解释性和通用性的人类移动智能系统提供了有力基础。

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [295] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 通过引入新型激活函数 elephant activation，显著提升强化学习中神经网络对灾难性遗忘的抵抗能力，提高样本与内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注算法层面缓解灾难性遗忘，但对神经网络架构属性（如激活函数）如何影响遗忘理解不足。

Method: 分析激活函数的梯度稀疏性对训练动态的影响，提出同时具备稀疏输出与稀疏梯度的 elephant 激活函数，并在值基强化学习算法中替换传统激活函数。

Result: 使用 elephant 激活函数可显著降低灾难性遗忘，提升学习的样本效率与内存效率。

Conclusion: 激活函数的梯度稀疏性是缓解灾难性遗忘的关键架构因素，elephant 激活函数为高效强化学习提供了简单有效的架构改进方案。

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [296] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 提出功能扩展定律(FSL)，首次将学习率调度显式纳入训练动态分析，理论解释了LLM预训练中多个经验实践的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩展定律仅关注最终损失，忽略训练过程动态与学习率调度(LRS)的影响，导致对训练效率理解不足。

Method: 基于教师-学生核回归模型与在线SGD，提出内生时间视角与SDE建模，推导出FSL公式，通过卷积项显式刻画LRS作用。

Result: FSL理论上验证了：大模型更高效、学习率衰减提升效率、WSD优于直接衰减，并在0.1B–1B参数模型上验证其预测与拟合能力。

Conclusion: FSL为LLM预训练提供新的理论框架，可作为损失曲线建模与优化的代理模型，推动训练策略的系统化设计。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [297] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: 通过从训练数据中提取弱鲁棒样本，先验评估模型鲁棒性，提升对抗和常见扰动下的可靠性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在干净数据上表现良好，但对对抗和常见扰动敏感，传统鲁棒性评估依赖扰动测试集，缺乏早期、敏感的评估手段

Method: 从训练集通过局部鲁棒性分析提取最易受扰动影响的“弱鲁棒”样本，以此作为评估模型脆弱性的指标

Result: 在CIFAR-10、CIFAR-100和ImageNet上验证了该方法的有效性，能更精准地指导模型鲁棒性优化

Conclusion: 基于弱鲁棒样本的验证方法能早期发现模型缺陷，显著提升模型在对抗和常见扰动下的可靠性

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [298] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill通过知识蒸馏在资源受限设备上高效部署PPG模型，提升性能并大幅降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大规模PPG基础模型难以部署在资源受限的可穿戴设备上。

Method: 提出PPG-Distill框架，通过预测、特征和补丁级知识蒸馏，结合形态蒸馏和节律蒸馏保留波形局部模式与时间结构。

Result: 在心率估计和房颤检测任务中，学生模型性能提升高达21.8%，推理速度提升7倍，内存使用减少19倍。

Conclusion: PPG-Distill实现了可穿戴设备上高效、精准的PPG分析。

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [299] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion通过融合领域自适应与低成本标注，在异构、稀疏标签的联邦学习中提升准确性、鲁棒性与公平性。


<details>
  <summary>Details</summary>
Motivation: 应对联邦学习中特征空间异构、数据非独立同分布及标签稀缺三大现实挑战。

Method: 采用多样性和聚类感知编码器（DivEn系列），通过置信度过滤的伪标签和领域自适应传输，结合相似性加权分类器耦合与个性化编码器，实现低成本标注与全局一致性。

Result: 在表格和图像基准上，FedFusion在IID、非IID及标签稀缺场景下均超越现有方法，提升准确率、鲁棒性和公平性，同时保持通信与计算开销相当。

Conclusion: 个性化、领域自适应与标签高效性的协同设计是应对真实联邦学习约束的有效路径。

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [300] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本研究系统分析了主流开源文本到视频生成模型的延迟与能耗，发现其计算成本随空间分辨率、时间长度和去噪步数呈二次或线性增长，并为可持续视频生成系统提供基准与设计建议。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到视频生成技术取得进展，但其高计算成本和能源消耗尚未被充分理解，亟需系统性评估以推动可持续发展。

Method: 构建了以计算为约束的分析模型，预测缩放规律，并在WAN2.1-T2V上通过精细实验验证，再扩展至六种模型进行运行时与能耗对比。

Result: 能耗与空间分辨率、时间长度呈二次增长，与去噪步数呈线性增长；六种模型的能耗和运行时差异显著，提供了实用基准。

Conclusion: 研究为未来高效、低能耗的文本到视频系统设计提供了量化依据与可持续性指导。

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [301] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: 本文通过LIPS基准评估了多种物理信息混合模型在电力系统潮流仿真中的性能，发现融入物理约束显著提升准确性、物理合规性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源接入和跨国电力交换增加，电网不确定性上升，传统物理求解器速度慢，机器学习模型作为快速代理需更好符合物理规律。

Method: 使用LIPS基准对多种混合模型进行消融研究，涵盖正则化、无监督损失、MLP到图神经网络等架构，评估其在精度、物理合规性、工业适用性和分布外泛化四方面表现。

Result: 物理知识嵌入显著提升模型物理合规性与泛化能力，图形网络在复杂场景下表现最优，且所有实现均开源可复现。

Conclusion: 物理信息混合模型是提升电力系统实时仿真效率与可靠性的有效途径，架构设计与约束嵌入方式对性能至关重要。

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [302] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文基于稳定性分析了分布式环境下对抗训练的泛化性能，发现泛化误差随对抗扰动强度和训练步数增加而增大。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽提升鲁棒性，但存在鲁棒过拟合与泛化差距扩大问题，且分布式场景下的泛化性质尚未被探索。

Method: 针对凸损失函数，基于扩散策略下的算法稳定性，推导泛化误差上界。

Result: 泛化误差随对抗扰动强度和训练步数增加而增长，与单智能体情况一致，但在分布式环境中为新发现。

Conclusion: 理论分析和数值实验共同验证了该稳定性分析的有效性，为分布式对抗训练的泛化提供理论依据。

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [303] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: 有效思维链的关键不是长度或重审次数，而是失败步骤比例（FSF）更低，结构感知的测试时扩展优于盲目延长思维链。


<details>
  <summary>Details</summary>
Motivation: 现有研究对有效思维链的特征缺乏清晰认识，尽管延长思维链和增加重审常被认为有益，但近期研究质疑其有效性。

Method: 在10个大推理模型上系统评估数学与科学推理任务，提出图结构视角提取思维链结构，定义失败步骤分数（FSF）作为关键指标，并设计两种干预实验验证因果性。

Result: 延长思维链和增加重审反而降低准确率；FSF是预测正确性的最优指标；通过筛选低FSF思维链或移除失败分支可显著提升准确率。

Conclusion: 有效思维链应追求更少失败，而非更长或更多重审，未来测试时扩展应基于结构感知而非盲目增加计算量。

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>
