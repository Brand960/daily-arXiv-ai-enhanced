<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 49]
- [cs.RO](#cs.RO) [Total: 25]
- [cs.LG](#cs.LG) [Total: 60]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Future Optical Flow Prediction Improves Robot Control & Video Generation](https://arxiv.org/abs/2601.10781)
*Kanchana Ranasinghe,Honglu Zhou,Yu Fang,Luyu Yang,Le Xue,Ran Xu,Caiming Xiong,Silvio Savarese,Michael S Ryoo,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: FOFPred是一个基于视觉-语言模型与扩散架构的光学流预测模型，能从嘈杂的网络视频数据中学习并用于控制与生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从噪声真实数据中预测通用密集运动表示方面存在挑战，且相关研究较少。

Method: 提出FOFPred，结合统一的视觉-语言模型与扩散架构，利用大规模网络视频-文本数据进行训练，并通过数据预处理提升信号质量。

Result: 模型在机器人操控和语言驱动的视频生成任务中展现出跨域泛化能力。

Conclusion: 统一的VLM-扩散架构结合大规模网络数据学习，显著提升了光学流预测的性能与实用性。

Abstract: Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.

</details>


### [2] [ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research](https://arxiv.org/abs/2601.10802)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 提出ICONIC-444大规模工业图像数据集，用于改进离群检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有离群检测研究受限于缺乏具有明确离群类别和难度分级的大规模高质量数据集。

Method: 构建ICONIC-444数据集，包含310万张RGB图像、444类，基于工业分拣设备采集，并定义四个基准任务评估22种先进方法。

Result: 提供首个支持细粒度与粗粒度任务的结构化工业离群检测数据集，并给出22种方法的基线结果。

Conclusion: ICONIC-444填补了离群检测数据空白，推动更严谨、多样化的离群检测研究。

Abstract: Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.

</details>


### [3] [A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems](https://arxiv.org/abs/2601.10819)
*Yizhou Wang,Sameer Pusegaonkar,Yuxing Wang,Anqi Li,Vishal Kumar,Chetan Sethi,Ganapathy Aiyer,Yun He,Kartikay Thakkar,Swapnil Rathi,Bhushan Rupde,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 本文提出一种优化的Sparse4D框架，用于大规模基础设施中的多摄像头3D目标跟踪，通过几何先验、遮挡感知ReID和生成式数据增强，在AI City Challenge 2025上实现SOTA HOTA 45.22，并通过TensorRT加速实现单卡支持64路摄像头实时处理。


<details>
  <summary>Details</summary>
Motivation: 将车内感知模型迁移到外部静态摄像头网络面临相机分布异构和严重遮挡的挑战，亟需适应基础设施场景的高效跟踪方案。

Method: 基于Sparse4D框架，引入绝对世界坐标几何先验与遮挡感知ReID嵌入模块，结合NVIDIA COSMOS生成式数据增强缓解Sim2Real差距，并开发TensorRT加速的MSDA插件提升推理速度。

Result: 在AI City Challenge 2025上达到HOTA 45.22的SOTA性能，TensorRT优化后推理速度提升2.15倍，单张Blackwell GPU可支持64路摄像头并发处理。

Conclusion: 该框架为大规模静态监控网络提供了高精度、实时、无标注数据的3D多目标跟踪解决方案，显著推动工业基础设施的数字化转型。

Abstract: Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.

</details>


### [4] [Can Vision-Language Models Understand Construction Workers? An Exploratory Study](https://arxiv.org/abs/2601.10835)
*Hieu Bui,Nathaniel E. Chodosh,Arash Tavakoli*

Main category: cs.CV

TL;DR: 评估了GPT-4o、Florence 2和LLaVa-1.5在建筑工地图像中识别工人行为与情绪的表现，GPT-4o表现最佳，但所有模型在语义相近类别上仍表现不佳。


<details>
  <summary>Details</summary>
Motivation: 建筑领域标注数据稀缺，亟需无需大量训练的通用视觉语言模型来监测工人行为与情绪，以提升安全与效率。

Method: 使用1000张标注图像，涵盖10种行为和10种情绪类别，通过标准化推理流程和多种评估指标比较GPT-4o、Florence 2和LLaVa-1.5的性能。

Result: GPT-4o在行为识别上F1=0.756、准确率0.799，情绪识别F1=0.712、准确率0.773；Florence 2次之，LLaVa-1.5最差；所有模型均难以区分语义相近类别（如团队协作 vs. 与主管沟通）。

Conclusion: 通用VLMs可作为建筑场景下行为识别的基线方案，但需通过领域自适应、时序建模或多模态传感提升实际应用可靠性。

Abstract: As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.

</details>


### [5] [One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection](https://arxiv.org/abs/2601.10836)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 研究发现，提高in-distribution准确率并不总是提升out-of-distribution检测性能，二者呈非单调关系，且训练策略与检测器选择高度相关。


<details>
  <summary>Details</summary>
Motivation: 尽管OOD检测方法不断进步，但其与现代训练管道（以最大化ID准确率为目标）之间的相互作用尚未被充分探索。

Method: 固定使用ResNet-50架构，在56个ImageNet训练模型上基准测试21种先进后处理OOD检测方法，并在8个OOD测试集上评估。

Result: OOD性能随ID准确率提升先增后降，不存在单一最优检测方法，性能受训练策略与检测器组合显著影响。

Conclusion: OOD检测效果不能仅靠提升ID准确率实现，需协同优化训练策略与检测器设计。

Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.

</details>


### [6] [Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification](https://arxiv.org/abs/2601.10854)
*Mohammad Rasras,Iuliana Marin,Serban Radu,Irina Mocanu*

Main category: cs.CV

TL;DR: 通过提升帧分辨率并减少时间信息，结合注意力机制改进3D ResNet模型，在UCF101上 achieving 88.98%准确率，揭示时间信息缺失对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索在减少时间特征提取的情况下，提高帧分辨率并引入注意力机制对动作识别性能的影响。

Method: 基于MC3、R3D和R(2+1)D构建改进模型，添加dropout层，并引入CBAM、TCN、多头注意力和通道注意力等变体，共设计30个模型，在UCF101上测试。

Result: 多头注意力增强的改进R(2+1)D模型达到88.98%准确率，不同注意力模块对类级准确率影响不一。

Conclusion: 时间信息的减少显著影响模型性能，尽管整体精度提升，但各类别表现差异明显，暗示时间特征不可完全替代。

Abstract: Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.

</details>


### [7] [Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation](https://arxiv.org/abs/2601.10880)
*Chongcong Jiang,Tianxingjian Ding,Chuhan Song,Jiachen Tu,Ziyang Yan,Yihua Shao,Zhenyi Wang,Yuzhang Shang,Tianyu Han,Yu Tian*

Main category: cs.CV

TL;DR: 通过全量微调SAM3于多模态医学影像数据，构建了适用于医学图像分割的通用提示驱动模型Medical SAM3，显著提升复杂场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 原始SAM3在医学影像上性能下降，依赖几何先验（如真实边界框），且缺乏对解剖结构和三维上下文的建模能力。

Method: 在33个跨10种医学影像模态的数据集上对SAM3进行全参数微调，结合配对的分割掩码与文本提示，实现域自适应。

Result: Medical SAM3在器官、模态和维度上均显著优于原始SAM3，尤其在语义模糊、形态复杂和长程3D上下文中表现突出。

Conclusion: 医学图像分割需 holistic 模型适配而非仅提示工程，Medical SAM3是首个通用文本引导的医学分割基础模型。

Abstract: Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.

</details>


### [8] [FrankenMotion: Part-level Human Motion Generation and Composition](https://arxiv.org/abs/2601.10909)
*Chuqiao Li,Xianghui Xie,Yong Cao,Andreas Geiger,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 提出首个具有精细时序部分级文本标注的运动数据集，并基于此构建可控制身体各部位独立运动的扩散模型FrankenMotion。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖序列或动作级描述，缺乏对身体部位的细粒度控制能力，因缺少部分级运动标注。

Method: 利用大语言模型构建含原子化、时序感知的部分级文本标注数据集，并提出扩散框架FrankenMotion，为每个身体部位提供独立的时序文本引导。

Result: FrankenMotion在所有基线模型上表现优越，能生成训练中未见的复合动作，实现空间（部位）与时间（原子动作）双维度控制。

Conclusion: 该工作首次实现基于部分级时序文本的高精度运动生成，为未来可控人体运动合成奠定基础。

Abstract: Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

</details>


### [9] [Classification of Chest XRay Diseases through image processing and analysis techniques](https://arxiv.org/abs/2601.10913)
*Santiago Martínez Novoa,María Catalina Ibáñez,Lina Gómez Mesa,Jeremias Kramer*

Main category: cs.CV

TL;DR: 本研究评估了多种胸片多分类方法，包括DenseNet121，并部署了开源Web应用以比较性能，同时分析了方法的不足并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 胸片多分类是诊断胸部疾病的重要手段，需提升分类准确性和可解释性。

Method: 采用DenseNet121等方法，通过开源Web应用进行实验比较，并分析模型缺陷。

Result: 对比了不同方法的性能，识别出当前模型的局限性。

Conclusion: 所提方法具有实用性，未来可通过优化架构和数据增强提升效果。

Abstract: Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML

</details>


### [10] [Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images](https://arxiv.org/abs/2601.10917)
*Pouya Afshin,David Helminiak,Tianling Niu,Julie M. Jorns,Tina Yen,Bing Yu,Dong Hye Ye*

Main category: cs.CV

TL;DR: 提出一种自监督学习引导的潜在扩散模型，生成高质量合成组织切片数据，显著提升乳腺保乳手术中显微图像分类精度。


<details>
  <summary>Details</summary>
Motivation: 由于标注的深紫外荧光显微镜数据稀缺，导致深度学习模型训练困难，影响术中切缘评估的准确性。

Method: 采用自监督学习引导的潜在扩散模型（LDM），通过微调的DINO教师模型嵌入细胞结构语义信息，生成合成训练样本，并结合真实与合成数据微调Vision Transformer进行全切片分类。

Result: 5折交叉验证下准确率达96.47%，FID分数降至45.72，显著优于条件分类基线模型。

Conclusion: 所提方法有效缓解了小样本标注数据瓶颈，为高精度术中病理分析提供了新范式。

Abstract: Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.

</details>


### [11] [RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions](https://arxiv.org/abs/2601.10921)
*Tasneem Shaffee,Sherief Reda*

Main category: cs.CV

TL;DR: RobuMTL通过动态选择任务特定的LoRA模块，提升恶劣天气下多任务学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 真实环境中的恶劣天气会严重降低自动驾驶系统模型的性能和可靠性，现有多任务学习方法缺乏对输入扰动的自适应能力。

Method: 提出RobuMTL架构，基于输入扰动以混合专家方式动态选择任务特定的分层LoRA模块和LoRA专家团。

Result: 在PASCAL上，单扰动下平均相对提升2.8%，混合天气下最高提升44.4%；在NYUD-v2上平均提升9.7%。

Conclusion: RobuMTL显著提升了多任务学习在复杂环境中的鲁棒性，为自动驾驶系统提供了有效的自适应解决方案。

Abstract: Robust Multi-Task Learning (MTL) is crucial for autonomous systems operating in real-world environments, where adverse weather conditions can severely degrade model performance and reliability. In this paper, we introduce RobuMTL, a novel architecture designed to adaptively address visual degradation by dynamically selecting task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations in a mixture-of-experts fashion. Our framework enables adaptive specialization based on input characteristics, improving robustness across diverse real-world conditions. To validate our approach, we evaluated it on the PASCAL and NYUD-v2 datasets and compared it against single-task models, standard MTL baselines, and state-of-the-art methods. On the PASCAL benchmark, RobuMTL delivers a +2.8% average relative improvement under single perturbations and up to +44.4% under mixed weather conditions compared to the MTL baseline. On NYUD-v2, RobuMTL achieves a +9.7% average relative improvement across tasks. The code is available at GitHub.

</details>


### [12] [Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images](https://arxiv.org/abs/2601.10931)
*David Szczecina,Hudson Sun,Anthony Bertnyk,Niloofar Azad,Kyle Gao,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 在仅有150张标注图像的极端数据稀缺条件下，卷积模型（如YOLOv11和Mask R-CNN）优于Transformer模型，适用于树冠检测任务。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像中树冠检测因标注数据极少导致的过拟合问题，评估不同架构在低数据环境下的表现。

Method: 在150张标注图像上测试YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet和DINOv2五种模型，对比其在树冠分割任务中的泛化能力。

Result: YOLOv11和Mask R-CNN表现最优，Transformer模型因数据需求高、任务类型不匹配和缺乏归纳偏置而表现较差。

Conclusion: 在数据稀缺场景下，轻量级卷积模型比Transformer更可靠，适合树冠检测任务。

Abstract: Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.

</details>


### [13] [PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis](https://arxiv.org/abs/2601.10945)
*K Lokesh,Abhirama Subramanyam Penamakuri,Uday Agarwal,Apoorva Challa,Shreya K Gowda,Somesh Gupta,Anand Mishra*

Main category: cs.CV

TL;DR: 提出一种预问诊对话框架，通过视觉语言模型模拟医患对话，提升基于图像的医学诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有AI医学诊断过度依赖图像分析，忽视患者自述症状，导致诊断准确性受限。

Method: 构建DocVLM与PatientVLM的双向对话系统，DocVLM提问，PatientVLM基于真实诊断生成症状回应，并通过临床专家验证合成症状的真实性。

Result: 对话监督显著提升DocVLM诊断性能，优于仅使用图像的训练方式，合成症状被临床医生认可为真实可靠。

Conclusion: 引入真实症状 elicitation 的对话框架能有效增强AI医学诊断能力，为未来临床辅助系统提供新范式。

Abstract: Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.

</details>


### [14] [MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement](https://arxiv.org/abs/2601.10949)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Haiqin Zhong,Xiaoling Luo,Wenting Chen,Linlin Shen*

Main category: cs.CV

TL;DR: 提出MMedExpert-R1，通过领域自适应和临床指南强化学习，提升医学视觉语言模型的复杂临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型在复杂临床推理上表现不足，因缺乏深度推理数据、冷启动问题及标准强化学习算法无法建模临床推理多样性。

Method: 构建10K样本的MMedExpert数据集，采用领域特定自适应（DSA）生成专科LoRA模块，并引入基于指南的优势（GBA）建模多元临床推理视角，最后通过冲突感知能力整合统一专家模型。

Result: 7B模型在MedXpert-MM上得分27.50，在OmniMedVQA上得分83.03，达到SOTA性能。

Conclusion: MMedExpert-R1为可靠多模态医学推理系统提供了坚实基础，有效解决了多专科对齐与推理多样性挑战。

Abstract: Medical Vision-Language Models (MedVLMs) excel at perception tasks but struggle with complex clinical reasoning required in real-world scenarios. While reinforcement learning (RL) has been explored to enhance reasoning capabilities, existing approaches face critical mismatches: the scarcity of deep reasoning data, cold-start limits multi-specialty alignment, and standard RL algorithms fail to model clinical reasoning diversity. We propose MMedExpert-R1, a novel reasoning MedVLM that addresses these challenges through domain-specific adaptation and clinical guideline reinforcement. We construct MMedExpert, a high-quality dataset of 10K samples across four specialties with step-by-step reasoning traces. Our Domain-Specific Adaptation (DSA) creates specialty-specific LoRA modules to provide diverse initialization, while Guideline-Based Advantages (GBA) explicitly models different clinical reasoning perspectives to align with real-world diagnostic strategies. Conflict-Aware Capability Integration then merges these specialized experts into a unified agent, ensuring robust multi-specialty alignment. Comprehensive experiments demonstrate state-of-the-art performance, with our 7B model achieving 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, establishing a robust foundation for reliable multimodal medical reasoning systems.

</details>


### [15] [IDDR-NGP: Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field](https://arxiv.org/abs/2601.11030)
*Xianliang Huang,Jiajie Gou,Shuhang Chen,Zhizhou Zhong,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 提出首个统一的干扰物去除方法IDDR-NGP，可在隐式3D表示中高效移除多种干扰物，如雪花、纸屑等，并构建了新基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅针对特定类型干扰物，缺乏统一处理隐式3D场景中多种干扰物的方案。

Method: 基于Instant-NPG，结合2D检测器，引入LPIPS损失和多视角补偿损失（MVCL），端到端优化多视角受损图像的重建。

Result: IDDR-NGP能有效移除多种真实与合成干扰物，性能媲美现有最佳去雪方法，且在新构建的基准上表现鲁棒。

Conclusion: IDDR-NGP是首个统一处理隐式3D干扰物的方法，为该领域提供了新基准与有效解决方案。

Abstract: This paper presents the first unified distractor removal method, named IDDR-NGP, which directly operates on Instant-NPG. The method is able to remove a wide range of distractors in 3D scenes, such as snowflakes, confetti, defoliation and petals, whereas existing methods usually focus on a specific type of distractors. By incorporating implicit 3D representations with 2D detectors, we demonstrate that it is possible to efficiently restore 3D scenes from multiple corrupted images. We design the learned perceptual image patch similarity~( LPIPS) loss and the multi-view compensation loss (MVCL) to jointly optimize the rendering results of IDDR-NGP, which could aggregate information from multi-view corrupted images. All of them can be trained in an end-to-end manner to synthesize high-quality 3D scenes. To support the research on distractors removal in implicit 3D representations, we build a new benchmark dataset that consists of both synthetic and real-world distractors. To validate the effectiveness and robustness of IDDR-NGP, we provide a wide range of distractors with corresponding annotated labels added to both realistic and synthetic scenes. Extensive experimental results demonstrate the effectiveness and robustness of IDDR-NGP in removing multiple types of distractors. In addition, our approach achieves results comparable with the existing SOTA desnow methods and is capable of accurately removing both realistic and synthetic distractors.

</details>


### [16] [Your One-Stop Solution for AI-Generated Video Detection](https://arxiv.org/abs/2601.11035)
*Long Ma,Zihao Xue,Yan Wang,Zhiyuan Yan,Jin Xu,Xiaorui Jiang,Haiyang Yu,Yong Liao,Zhen Bi*

Main category: cs.CV

TL;DR: 提出AIGVDBench基准，涵盖31个先进生成模型和44万视频，通过1500+次评估揭示4项新发现，推动AI生成视频检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模小、模型过时、质量低，且评估基准缺乏深度分析，难以应对快速演进的生成视频技术。

Method: 构建AIGVDBench基准，涵盖31个SOTA模型、44万视频，对33个检测器进行1500+次评估，并开展8项多维度深度分析。

Result: 识别出4项新发现，系统评估了现有检测器性能，提供了全面的基准测试框架。

Conclusion: AIGVDBench为AI生成视频检测领域提供了可靠、全面的评估基础，推动未来研究发展。

Abstract: Recent advances in generative modeling can create remarkably realistic synthetic videos, making it increasingly difficult for humans to distinguish them from real ones and necessitating reliable detection methods.
  However, two key limitations hinder the development of this field.
  \textbf{From the dataset perspective}, existing datasets are often limited in scale and constructed using outdated or narrowly scoped generative models, making it difficult to capture the diversity and rapid evolution of modern generative techniques. Moreover, the dataset construction process frequently prioritizes quantity over quality, neglecting essential aspects such as semantic diversity, scenario coverage, and technological representativeness.
  \textbf{From the benchmark perspective}, current benchmarks largely remain at the stage of dataset creation, leaving many fundamental issues and in-depth analysis yet to be systematically explored.
  Addressing this gap, we propose AIGVDBench, a benchmark designed to be comprehensive and representative, covering \textbf{31} state-of-the-art generation models and over \textbf{440,000} videos. By executing more than \textbf{1,500} evaluations on \textbf{33} existing detectors belonging to four distinct categories. This work presents \textbf{8 in-depth analyses} from multiple perspectives and identifies \textbf{4 novel findings} that offer valuable insights for future research. We hope this work provides a solid foundation for advancing the field of AI-generated video detection.
  Our benchmark is open-sourced at https://github.com/LongMa-2025/AIGVDBench.

</details>


### [17] [M3DDM+: An improved video outpainting by a modified masking strategy](https://arxiv.org/abs/2601.11048)
*Takuya Murakawa,Takumi Fukuzawa,Ning Ding,Toru Tamaki*

Main category: cs.CV

TL;DR: M3DDM+通过统一训练中的掩码策略，显著提升视频外推在低运动或大外推区域的视觉质量与时序一致性。


<details>
  <summary>Details</summary>
Motivation: M3DDM在摄像头运动有限或外推区域较大时出现空间模糊和时序不一致问题，根源在于训练与推理阶段掩码策略不一致。

Method: 提出M3DDM+，在训练中对所有帧采用统一的掩码方向和宽度，并对预训练模型进行微调。

Result: M3DDM+在信息受限场景下显著提升视觉保真度和时序连贯性，同时保持计算效率。

Conclusion: 统一掩码策略可有效缓解训练-推理不匹配问题，是提升视频外推性能的关键。

Abstract: M3DDM provides a computationally efficient framework for video outpainting via latent diffusion modeling. However, it exhibits significant quality degradation -- manifested as spatial blur and temporal inconsistency -- under challenging scenarios characterized by limited camera motion or large outpainting regions, where inter-frame information is limited. We identify the cause as a training-inference mismatch in the masking strategy: M3DDM's training applies random mask directions and widths across frames, whereas inference requires consistent directional outpainting throughout the video. To address this, we propose M3DDM+, which applies uniform mask direction and width across all frames during training, followed by fine-tuning of the pretrained M3DDM model. Experiments demonstrate that M3DDM+ substantially improves visual fidelity and temporal coherence in information-limited scenarios while maintaining computational efficiency. The code is available at https://github.com/tamaki-lab/M3DDM-Plus.

</details>


### [18] [PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models](https://arxiv.org/abs/2601.11087)
*Qiyuan Zhang,Biao Gong,Shuai Tan,Zheng Zhang,Yujun Shen,Xing Zhu,Yuyuan Li,Kelu Yao,Chunhua Shen,Changqing Zou*

Main category: cs.CV

TL;DR: 首次将物理感知的强化学习引入视频生成，通过MDcycle框架严格约束碰撞规则，提升生成视频的物理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有变换器视频生成模型忽视刚体运动物理原理，将物理约束视为优化条件而非硬性规则，导致生成结果缺乏真实物理行为。

Method: 提出物理感知强化学习范式，并构建Mimicry-Discovery Cycle (MDcycle) 框架，在高维空间中直接施加物理碰撞规则，保持模型微调能力的同时确保物理一致性。

Result: 构建了新基准PhysRVGBench，实验表明该方法在定性和定量评估中显著提升视频的物理真实性。

Conclusion: 将物理定律作为硬性约束而非优化条件，是实现高真实感视频生成的关键，MDcycle为物理驱动视频生成提供了新范式。

Abstract: Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.

</details>


### [19] [CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation](https://arxiv.org/abs/2601.11096)
*Shuai Tan,Biao Gong,Ke Ma,Yutong Feng,Qiyuan Zhang,Yan Wang,Yujun Shen,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 提出CoDance框架，通过解绑-重绑机制实现任意数量和类型角色的动画生成，突破传统方法对姿势-参考图严格对齐的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理多角色、多样类型及姿势-参考图空间错位的问题，因过于僵化的像素级绑定和无法一致地将运动分配给目标角色。

Method: 提出Unbind-Rebind框架：Unbind模块使用姿态偏移编码器引入随机扰动，打破空间绑定，学习位置无关的运动表征；Rebind模块利用文本语义和角色掩码将运动精准重绑定到目标角色。

Result: 在新构建的CoDanceBench及现有数据集上达到SOTA性能，展现出对多样化角色和空间布局的卓越泛化能力。

Conclusion: CoDance实现了灵活、鲁棒的多角色动画生成，为复杂场景下的角色动画提供了新范式，代码与权重将开源。

Abstract: Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.

</details>


### [20] [Graph Smoothing for Enhanced Local Geometry Learning in Point Cloud Analysis](https://arxiv.org/abs/2601.11102)
*Shangbo Yuan,Jie Xu,Ping Hu,Xiaofeng Zhu,Na Zhao*

Main category: cs.CV

TL;DR: 提出一种融合图平滑与局部几何学习的新型方法，提升3D点云分析中图结构的质量和特征提取能力。


<details>
  <summary>Details</summary>
Motivation: 传统图结构在边界点处连接稀疏、在交汇区域存在噪声连接，导致性能受限。

Method: 引入图平滑模块优化图结构，并结合基于特征向量的自适应几何描述符和柱坐标变换的分布特征增强局部几何学习。

Result: 在真实数据集上验证了方法在分类、部件分割和语义分割任务中的有效性。

Conclusion: 该方法显著改善了图结构质量，提升了点云分析的性能。

Abstract: Graph-based methods have proven to be effective in capturing relationships among points for 3D point cloud analysis. However, these methods often suffer from suboptimal graph structures, particularly due to sparse connections at boundary points and noisy connections in junction areas. To address these challenges, we propose a novel method that integrates a graph smoothing module with an enhanced local geometry learning module. Specifically, we identify the limitations of conventional graph structures, particularly in handling boundary points and junction areas. In response, we introduce a graph smoothing module designed to optimize the graph structure and minimize the negative impact of unreliable sparse and noisy connections. Based on the optimized graph structure, we improve the feature extract function with local geometry information. These include shape features derived from adaptive geometric descriptors based on eigenvectors and distribution features obtained through cylindrical coordinate transformation. Experimental results on real-world datasets validate the effectiveness of our method in various point cloud learning tasks, i.e., classification, part segmentation, and semantic segmentation.

</details>


### [21] [Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning](https://arxiv.org/abs/2601.11109)
*Shaofeng Yin,Jiaxin Ge,Zora Zhiruo Wang,Xiuyu Li,Michael J. Black,Trevor Darrell,Angjoo Kanazawa,Haiwen Feng*

Main category: cs.CV

TL;DR: VIGA通过迭代的写-运行-渲染-比较-修正流程，实现无需微调的视觉反图形生成，显著提升多任务场景重建与编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型缺乏细粒度空间与物理感知能力，难以一次性完成图像到可编辑图形程序的逆图形重建。

Method: VIGA采用闭环写-运行-渲染-比较-修正流程，结合技能库（生成器与验证器交替）和动态上下文记忆（计划、代码差异、渲染历史），实现任务无关的多模态推理。

Result: 在BlenderGym和SlideBench上分别提升35.32%和117.17%，在新基准BlenderBench上提升124.70%，且无需微调，支持异构VLM统一评估。

Conclusion: VIGA通过迭代多模态推理突破了一次性逆图形生成的瓶颈，为视觉反图形任务提供了通用、可扩展的框架。

Abstract: Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren't able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn't require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn't require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%.

</details>


### [22] [SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention](https://arxiv.org/abs/2601.11164)
*Ruibang Li,Guan Luo,Yiwei Zhang,Jin Gao,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: 提出SoLA-Vision，通过细粒度层间混合softmax与线性注意力，在保持高精度的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力计算复杂度高，线性注意力虽高效但表达能力不足，亟需在效率与性能间取得平衡。

Method: 系统分析层间混合注意力模式，提出SoLA-Vision，灵活插入少量softmax层以优化全局建模。

Result: 在ImageNet-1K上超越纯线性及其它混合模型，在密集预测任务上显著优于强基线。

Conclusion: 细粒度层间混合策略优于块内混合，少量softmax层即可实现高性能，为视觉模型提供高效新范式。

Abstract: Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.

</details>


### [23] [Democratizing planetary-scale analysis: An ultra-lightweight Earth embedding database for accurate and flexible global land monitoring](https://arxiv.org/abs/2601.11183)
*Shuang Chen,Jie Wang,Shuai Yuan,Jiayang Li,Yu Xia,Yuanhong Liao,Junbo Wei,Jincheng Yuan,Xiaoqing Xu,Xiaolin Zhu,Peng Zhu,Hongsheng Zhang,Yuyu Zhou,Haohuan Fu,Huabing Huang,Bin Chen,Fan Dai,Peng Gong*

Main category: cs.CV

TL;DR: ESD是一种超轻量级全球地球嵌入数据库，将25年卫星数据压缩340倍，实现本地工作站进行年代际分析，显著提升地表分类与AI建模效率。


<details>
  <summary>Details</summary>
Motivation: 全球尺度地球观测数据分析面临计算与存储瓶颈，限制了广泛使用与行星尺度研究。

Method: 利用ESDNet与有限标量量化（FSQ）技术，将Landsat和MODIS多传感器数据转化为低维潜向量，压缩数据体积并保留关键地物特征。

Result: 数据体积缩减340倍（单年仅2.4TB），重建精度高（MAE=0.0130, CC=0.8543），地表分类准确率达79.74%，超越原始反射率数据。

Conclusion: ESD为行星尺度地球科学研究提供了低门槛、高效率的嵌入基础，推动地理空间AI的民主化与下一代发展。

Abstract: The rapid evolution of satellite-borne Earth Observation (EO) systems has revolutionized terrestrial monitoring, yielding petabyte-scale archives. However, the immense computational and storage requirements for global-scale analysis often preclude widespread use, hindering planetary-scale studies. To address these barriers, we present Embedded Seamless Data (ESD), an ultra-lightweight, 30-m global Earth embedding database spanning the 25-year period from 2000 to 2024. By transforming high-dimensional, multi-sensor observations from the Landsat series (5, 7, 8, and 9) and MODIS Terra into information-dense, quantized latent vectors, ESD distills essential geophysical and semantic features into a unified latent space. Utilizing the ESDNet architecture and Finite Scalar Quantization (FSQ), the dataset achieves a transformative ~340-fold reduction in data volume compared to raw archives. This compression allows the entire global land surface for a single year to be encapsulated within approximately 2.4 TB, enabling decadal-scale global analysis on standard local workstations. Rigorous validation demonstrates high reconstructive fidelity (MAE: 0.0130; RMSE: 0.0179; CC: 0.8543). By condensing the annual phenological cycle into 12 temporal steps, the embeddings provide inherent denoising and a semantically organized space that outperforms raw reflectance in land-cover classification, achieving 79.74% accuracy (vs. 76.92% for raw fusion). With robust few-shot learning capabilities and longitudinal consistency, ESD provides a versatile foundation for democratizing planetary-scale research and advancing next-generation geospatial artificial intelligence.

</details>


### [24] [ATATA: One Algorithm to Align Them All](https://arxiv.org/abs/2601.11194)
*Boyi Pang,Savva Ignatyev,Vladimir Ippolitov,Ramil Khafizov,Yurii Melnik,Oleg Voynov,Maksim Nakhodnov,Aibek Alanov,Xiaopeng Fan,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: 提出一种基于校正流模型的多模态联合推理算法，显著提升结构对齐样本生成的速度与质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如SDS存在计算慢、模式坍缩和结果卡通化问题，且未从结构对齐视角处理联合生成。

Method: 通过结构化潜在空间中的样本段联合传输，基于任意校正流模型实现高效联合推理。

Result: 在图像、视频和3D形状生成中实现高结构对齐与视觉质量，3D生成速度提升数个数量级，图像/视频生成达SOTA。

Conclusion: 该方法在保持高质量的同时大幅加速联合生成，为多模态结构对齐任务提供高效新范式。

Abstract: We suggest a new multi-modal algorithm for joint inference of paired structurally aligned samples with Rectified Flow models. While some existing methods propose a codependent generation process, they do not view the problem of joint generation from a structural alignment perspective. Recent work uses Score Distillation Sampling to generate aligned 3D models, but SDS is known to be time-consuming, prone to mode collapse, and often provides cartoonish results. By contrast, our suggested approach relies on the joint transport of a segment in the sample space, yielding faster computation at inference time. Our approach can be built on top of an arbitrary Rectified Flow model operating on the structured latent space. We show the applicability of our method to the domains of image, video, and 3D shape generation using state-of-the-art baselines and evaluate it against both editing-based and joint inference-based competing approaches. We demonstrate a high degree of structural alignment for the sample pairs obtained with our method and a high visual quality of the samples. Our method improves the state-of-the-art for image and video generation pipelines. For 3D generation, it is able to show comparable quality while working orders of magnitude faster.

</details>


### [25] [Bio-inspired fine-tuning for selective transfer learning in image classification](https://arxiv.org/abs/2601.11235)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: BioTune是一种基于进化优化的自适应微调方法，显著提升小样本图像分类的迁移学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习因源域与目标域差异导致性能下降，亟需更智能的微调策略以适应不同数据分布。

Method: BioTune利用进化算法自动选择冻结层并优化未冻结层的学习率，实现跨域自适应微调。

Result: 在9个图像分类数据集（含医学影像）和4种CNN架构上，BioTune均优于AutoRGN和LoRA等先进方法。

Conclusion: BioTune具有强泛化性与架构无关性，为小样本图像分析提供高效解决方案，代码已开源。

Abstract: Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.

</details>


### [26] [Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification](https://arxiv.org/abs/2601.11243)
*Zhiqi Pang,Lingling Zhao,Yang Liu,Chunyu Wang,Gaurav Sharma*

Main category: cs.CV

TL;DR: 提出无监督多场景行人重识别任务UMS-ReID，并引入ITKM框架，利用视觉-语言模型提升跨场景性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法局限于单一场景，难以应对跨分辨率、换装等多场景挑战，亟需统一框架整合多场景知识。

Method: ITKM框架包含三阶段：1）在CLIP图像编码器中引入场景嵌入并微调；2）优化文本嵌入并引入多场景分离损失；3）设计异构匹配模块与动态文本更新策略。

Result: ITKM在多个场景中显著超越现有单场景方法，通过融合多场景知识提升整体性能。

Conclusion: ITKM有效利用视觉-语言模型实现无监督多场景行人重识别，具有强泛化能力与实用价值。

Abstract: We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.

</details>


### [27] [Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval](https://arxiv.org/abs/2601.11248)
*Fangke Chen,Tianhao Dong,Sirry Chen,Guobin Zhang,Yishu Zhang,Yining Chen*

Main category: cs.CV

TL;DR: 提出一种轻量级非对称双编码器框架，实现高效跨语言手写词检索，显著降低计算开销并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 手写词检索在数字档案中至关重要，但因手写变异性大和跨语言语义差距而具有挑战性，现有大模型计算成本过高难以部署。

Method: 设计轻量级非对称双编码器，联合优化实例级对齐与类别级语义一致性，将视觉嵌入锚定到语言无关的语义原型，实现跨字体与风格的不变性。

Result: 在单语言检索中超越28个基线方法，达到SOTA性能；在跨语言检索中表现优异，参数量远低于现有模型。

Conclusion: 该框架实现了高精度、低资源消耗的跨脚本手写词检索，推动了边缘设备上的实际应用。

Abstract: Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.

</details>


### [28] [FTDMamba: Frequency-Assisted Temporal Dilation Mamba for Unmanned Aerial Vehicle Video Anomaly Detection](https://arxiv.org/abs/2601.11254)
*Cheng-Zhuang Liu,Si-Bao Chen,Qing-Ling Shu,Chris Ding,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 提出FTDMamba网络和MUVAD数据集，解决动态背景下无人机视频异常检测难题，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态背景无人机视频中难以区分目标运动与无人机自身运动，易误检或漏检异常，且缺乏对多尺度时空关联的联合建模。

Method: 提出FTDMamba网络，包含频率解耦时空相关模块和时间膨胀Mamba模块，分别解耦运动并建模多尺度时序与空间结构；同时构建大规模MUVAD数据集。

Result: FTDMamba在两个静态基准和新MUVAD数据集上均达到SOTA性能。

Conclusion: 频率解耦与Mamba时序建模有效提升动态背景下的异常检测精度，MUVAD数据集推动该领域发展。

Abstract: Recent advances in video anomaly detection (VAD) mainly focus on ground-based surveillance or unmanned aerial vehicle (UAV) videos with static backgrounds, whereas research on UAV videos with dynamic backgrounds remains limited. Unlike static scenarios, dynamically captured UAV videos exhibit multi-source motion coupling, where the motion of objects and UAV-induced global motion are intricately intertwined. Consequently, existing methods may misclassify normal UAV movements as anomalies or fail to capture true anomalies concealed within dynamic backgrounds. Moreover, many approaches do not adequately address the joint modeling of inter-frame continuity and local spatial correlations across diverse temporal scales. To overcome these limitations, we propose the Frequency-Assisted Temporal Dilation Mamba (FTDMamba) network for UAV VAD, including two core components: (1) a Frequency Decoupled Spatiotemporal Correlation Module, which disentangles coupled motion patterns and models global spatiotemporal dependencies through frequency analysis; and (2) a Temporal Dilation Mamba Module, which leverages Mamba's sequence modeling capability to jointly learn fine-grained temporal dynamics and local spatial structures across multiple temporal receptive fields. Additionally, unlike existing UAV VAD datasets which focus on static backgrounds, we construct a large-scale Moving UAV VAD dataset (MUVAD), comprising 222,736 frames with 240 anomaly events across 12 anomaly types. Extensive experiments demonstrate that FTDMamba achieves state-of-the-art (SOTA) performance on two public static benchmarks and the new MUVAD dataset. The code and MUVAD dataset will be available at: https://github.com/uavano/FTDMamba.

</details>


### [29] [X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning](https://arxiv.org/abs/2601.11269)
*Maanping Shao,Feihong Zhang,Gu Zhang,Baiye Cheng,Zhengrong Xue,Huazhe Xu*

Main category: cs.CV

TL;DR: X-Distill通过知识蒸馏将DINOv2的知识迁移到ResNet-18，在数据稀缺的机器人任务中实现高性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉Transformer数据需求高，难以在数据稀缺的机器人学习中优化，而紧凑CNN虽易优化但表征能力弱，亟需折中方案。

Method: 使用DINOv2作为教师模型，通过离线跨架构知识蒸馏将视觉表征迁移到ResNet-18学生模型，再联合微调扩散策略头。

Result: 在34个模拟和5个真实机器人任务中，X-Distill显著优于从零训练ResNet、微调DINOv2及使用点云或大VL模型的方法。

Conclusion: 简单而扎实的知识蒸馏策略可实现数据高效机器人操控的最优性能。

Abstract: Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.

</details>


### [30] [Efficient On-Board Processing of Oblique UAV Video for Rapid Flood Extent Mapping](https://arxiv.org/abs/2601.11290)
*Vishisht Sharma,Sam Leroux,Lisa Landuyt,Nick Witvrouwen,Pieter Simoens*

Main category: cs.CV

TL;DR: 提出Temporal Token Reuse (TTR)方法，通过复用时空冗余特征，在嵌入式设备上加速无人机斜视视频分割，降低30%延迟且精度几乎不变。


<details>
  <summary>Details</summary>
Motivation: 无人机受限于SWaP约束，无法高效处理高分辨率斜视视频的实时推理，亟需轻量级加速方法。

Method: 将图像块建模为token，利用轻量相似性度量动态识别静态区域，复用其预计算深度特征，跳过冗余主干网络计算。

Result: 在标准基准和新构建的斜视洪水数据集上，TTR在边缘设备上实现30%推理延迟降低，mIoU下降<0.5%。

Conclusion: TTR有效突破边缘计算瓶颈，实现高精度实时斜视视频理解，适用于紧急遥感任务。

Abstract: Effective disaster response relies on rapid disaster response, where oblique aerial video is the primary modality for initial scouting due to its ability to maximize spatial coverage and situational awareness in limited flight time. However, the on-board processing of high-resolution oblique streams is severely bottlenecked by the strict Size, Weight, and Power (SWaP) constraints of Unmanned Aerial Vehicles (UAVs). The computational density required to process these wide-field-of-view streams precludes low-latency inference on standard edge hardware. To address this, we propose Temporal Token Reuse (TTR), an adaptive inference framework capable of accelerating video segmentation on embedded devices. TTR exploits the intrinsic spatiotemporal redundancy of aerial video by formulating image patches as tokens; it utilizes a lightweight similarity metric to dynamically identify static regions and propagate their precomputed deep features, thereby bypassing redundant backbone computations. We validate the framework on standard benchmarks and a newly curated Oblique Floodwater Dataset designed for hydrological monitoring. Experimental results on edge-grade hardware demonstrate that TTR achieves a 30% reduction in inference latency with negligible degradation in segmentation accuracy (< 0.5% mIoU). These findings confirm that TTR effectively shifts the operational Pareto frontier, enabling high-fidelity, real-time oblique video understanding for time-critical remote sensing missions

</details>


### [31] [SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2](https://arxiv.org/abs/2601.11301)
*Gergely Dinya,András Gelencsér,Krisztina Kupán,Clemens Küpper,Kristóf Karacs,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: SAMannot是一个开源本地框架，集成SAM2模型，提供私密、低成本、高效率的视频实例分割标注工具


<details>
  <summary>Details</summary>
Motivation: 现有视频分割工作流面临人工标注耗时、商业平台昂贵、云服务侵犯隐私的三重瓶颈

Method: 基于SAM2构建本地人机协作流程，优化计算开销，引入持久实例管理、锁-精炼工作流和基于掩膜骨架的自动提示机制

Result: 支持YOLO和PNG格式输出，生成结构化交互日志，在动物行为跟踪和LVOS/DAVIS基准上验证有效

Conclusion: SAMannot为复杂视频标注提供了可扩展、私密且经济高效的替代方案，突破了商业平台的限制

Abstract: Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.

</details>


### [32] [Context-Aware Semantic Segmentation via Stage-Wise Attention](https://arxiv.org/abs/2601.11310)
*Antoine Carreaud,Elias Naha,Arthur Chansel,Nina Lahellec,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: CASWiT是一种双分支Swin架构，通过上下文感知的跨尺度融合和SimMIM预训练，在超高分辨率遥感图像分割中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在超高分辨率图像分割中因内存随token数量二次增长而受限，难以兼顾上下文范围与空间分辨率。

Method: 提出CASWiT双分支架构：一个上下文编码器处理下采样邻域以捕获长程依赖，一个高分辨率编码器提取细节特征，并通过交叉注意力与门控特征注入进行跨尺度融合；同时采用SimMIM风格预训练，掩码75%的高分辨率token和对应低分辨率中心区域进行重建。

Result: 在IGN FLAIR-HUB数据集上达到65.83% mIoU，超越RGB基线1.78点；在URUR数据集上达到49.1% mIoU，超越当前SOTA 0.9%。

Conclusion: CASWiT有效解决了超高分辨率图像中上下文与分辨率的权衡问题，通过架构创新与预训练策略显著提升分割性能。

Abstract: Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.

</details>


### [33] [Enhancing Vision Language Models with Logic Reasoning for Situational Awareness](https://arxiv.org/abs/2601.11322)
*Pavana Pradeep,Krishna Kant,Suya Yu*

Main category: cs.CV

TL;DR: 通过结合视觉语言模型与传统计算机视觉方法及逻辑推理，提升情境感知中罕见事件的检测精度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在情境感知中难以可靠识别罕见但重要的事件，且缺乏对输出结果的细粒度细节提取与质量评估能力。

Method: 将视觉语言模型与传统计算机视觉方法结合，引入显式逻辑推理，实现细粒度事件提取、智能微调策略及推理时的输出合理性验证。

Result: 智能微调策略显著提升准确率，并能在推理阶段提供输出有效性的判断依据。

Conclusion: 该方法有效增强视觉语言模型在关键情境感知任务中的可靠性、准确性与可解释性。

Abstract: Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.

</details>


### [34] [Beer-Lambert Autoencoder for Unsupervised Stain Representation Learning and Deconvolution in Multi-immunohistochemical Brightfield Histology Images](https://arxiv.org/abs/2601.11336)
*Mark Eastwood,Thomas McKee,Zedong Hu,Sabine Tejpar,Fayyaz Minhas*

Main category: cs.CV

TL;DR: 提出一种数据驱动的编码器-解码器架构，用于多路免疫组化RGB图像的染色分离，显著减少通道串扰。


<details>
  <summary>Details</summary>
Motivation: 经典Beer-Lambert方法在K>3染色的多路免疫组化中不稳定且欠定，亟需更鲁棒的解混方法。

Method: 采用紧凑U-Net编码器预测K个非负浓度通道，结合可学习的染色矩阵与可微分BL前向模型，无监督训练并引入抑制染色混合的损失项。

Result: 在5染色结直肠mIHC数据上实现优异的RGB重建效果，染色间串扰显著低于传统矩阵法。

Conclusion: 该方法为多染色WSI提供高效、稳定、无监督的染色分离方案，具有临床应用潜力。

Abstract: Separating the contributions of individual chromogenic stains in RGB histology whole slide images (WSIs) is essential for stain normalization, quantitative assessment of marker expression, and cell-level readouts in immunohistochemistry (IHC). Classical Beer-Lambert (BL) color deconvolution is well-established for two- or three-stain settings, but becomes under-determined and unstable for multiplex IHC (mIHC) with K>3 chromogens. We present a simple, data-driven encoder-decoder architecture that learns cohort-specific stain characteristics for mIHC RGB WSIs and yields crisp, well-separated per-stain concentration maps. The encoder is a compact U-Net that predicts K nonnegative concentration channels; the decoder is a differentiable BL forward model with a learnable stain matrix initialized from typical chromogen hues. Training is unsupervised with a perceptual reconstruction objective augmented by loss terms that discourage unnecessary stain mixing. On a colorectal mIHC panel comprising 5 stains (H, CDX2, MUC2, MUC5, CD8) we show excellent RGB reconstruction, and significantly reduced inter-channel bleed-through compared with matrix-based deconvolution. Code and model are available at https://github.com/measty/StainQuant.git.

</details>


### [35] [Assessing Building Heat Resilience Using UAV and Street-View Imagery with Coupled Global Context Vision Transformer](https://arxiv.org/abs/2601.11357)
*Steffen Knoblauch,Ram Kumar Muthusamy,Hao Li,Iddy Chazua,Benedcto Adamu,Innocent Maholi,Alexander Zipf*

Main category: cs.CV

TL;DR: 利用无人机和街景图像结合机器学习，识别城市建筑与热暴露风险的关系，助力赤道城市气候适应。


<details>
  <summary>Details</summary>
Motivation: 全球南方城市因廉价建材和高热容表面加剧热浪风险，但缺乏可扩展的建筑热属性评估方法。

Method: 提出CGCViT模型，融合UAV与街景图像，结合HotSat-1热红外数据，学习建筑热相关表征。

Result: 双模态模型比单模态最佳模型性能提升达9.3%；植被、浅色屋顶、混凝土/黏土/木制屋顶显著降低地表温度。

Conclusion: 该框架可识别社会经济不平等导致的热暴露差异，支持数据驱动、公平导向的气候适应策略。

Abstract: Climate change is intensifying human heat exposure, particularly in densely built urban centers of the Global South. Low-cost construction materials and high thermal-mass surfaces further exacerbate this risk. Yet scalable methods for assessing such heat-relevant building attributes remain scarce. We propose a machine learning framework that fuses openly available unmanned aerial vehicle (UAV) and street-view (SV) imagery via a coupled global context vision transformer (CGCViT) to learn heat-relevant representations of urban structures. Thermal infrared (TIR) measurements from HotSat-1 are used to quantify the relationship between building attributes and heat-associated health risks. Our dual-modality cross-view learning approach outperforms the best single-modality models by up to $9.3\%$, demonstrating that UAV and SV imagery provide valuable complementary perspectives on urban structures. The presence of vegetation surrounding buildings (versus no vegetation), brighter roofing (versus darker roofing), and roofing made of concrete, clay, or wood (versus metal or tarpaulin) are all significantly associated with lower HotSat-1 TIR values. Deployed across the city of Dar es Salaam, Tanzania, the proposed framework illustrates how household-level inequalities in heat exposure - often linked to socio-economic disadvantage and reflected in building materials - can be identified and addressed using machine learning. Our results point to the critical role of localized, data-driven risk assessment in shaping climate adaptation strategies that deliver equitable outcomes.

</details>


### [36] [Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding](https://arxiv.org/abs/2601.11359)
*Wenhui Tan,Ruihua Song,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: 提出无训练框架TCS，通过多查询推理和剪辑级慢速-快速采样提升长视频理解效率与精度


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在长视频理解上受限于计算成本和帧采样策略低效

Method: 引入多查询推理生成互补问题相关查询，结合剪辑级慢速-快速采样平衡局部细节与全局上下文

Result: 在MLVU、LongVideoBench和VideoMME上提升最高6.9%准确率，推理时间减少50%仍保持相当性能

Conclusion: TCS无需训练，显著提升长视频理解的效率与效果，具有广泛适用性

Abstract: Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.

</details>


### [37] [Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning](https://arxiv.org/abs/2601.11393)
*Haomiao Tang,Jinpeng Wang,Minyi Zhao,Guanghao Meng,Ruisheng Luo,Long Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出HUG框架，通过异质不确定性建模提升复合图像检索的鲁棒性与精度


<details>
  <summary>Details</summary>
Motivation: 现有概率学习方法在CIR任务中因实例级整体建模和同质处理查询与目标而性能受限，且未能有效处理三元组中的内在噪声

Method: 引入高斯嵌入表示查询与目标，设计异质不确定性估计机制，结合多模态协调与单模态质量评估，采用动态加权与不确定性引导的对比学习目标

Result: 在多个基准数据集上超越现有SOTA方法，实验验证了方法的有效性和不确定性建模的合理性

Conclusion: HUG通过细粒度异质不确定性建模显著提升了CIR任务的鲁棒性与判别能力，为概率学习在多模态检索中的应用提供了新范式

Abstract: Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.

</details>


### [38] [SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction](https://arxiv.org/abs/2601.11396)
*Hanlin Wu,Pengfei Lin,Ehsan Javanmardi,Nanren Bao,Bo Qian,Hao Si,Manabu Tsukada*

Main category: cs.CV

TL;DR: 提出SUG-Occ框架，利用语义与不确定性引导的稀疏学习，实现高效高精度的3D语义占用预测，显著提升准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 传统3D语义占用预测计算与内存开销大，难以实现实时部署，亟需一种高效且保持几何与语义完整性的方法。

Method: 1) 利用语义与不确定性先验抑制空域投影，结合无符号距离编码增强几何一致性；2) 设计级联稀疏补全模块，通过超交叉稀疏卷积与生成上采样实现粗到细推理；3) 构建基于对象上下文表示的轻量掩码解码器，避免昂贵的体素注意力操作。

Result: 在SemanticKITTI上取得7.34%的准确率提升和57.8%的效率增益，超越基线方法。

Conclusion: SUG-Occ通过显式稀疏学习有效平衡了精度与效率，为实时3D语义占用预测提供了实用解决方案。

Abstract: As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.

</details>


### [39] [Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model](https://arxiv.org/abs/2601.11400)
*Shuai Yuan,Tianwu Lin,Shuang Chen,Yu Xia,Peng Qin,Xiangyu Liu,Xiaoqing Xu,Nan Xu,Hongsheng Zhang,Jie Wang,Peng Gong*

Main category: cs.CV

TL;DR: WetSAM是一种基于SAM的框架，利用卫星时间序列数据和稀疏点标签实现高精度湿地测绘，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在稀疏点标签下表现不佳，且单期影像无法捕捉湿地的季节性和年际动态，而SAM等基础模型缺乏时序建模能力，导致分割结果碎片化。

Method: 提出WetSAM框架，采用双分支设计：时间分支通过层次适配器和动态时序聚合分离湿地特征与物候变异，空间分支使用时序约束的区域生长生成伪标签，并通过双向一致性正则化联合优化两分支。

Result: 在八个全球区域（每个约5000 km²）的实验中，WetSAM平均F1分数达到85.58%，实现高精度、结构一致的湿地分割，且标注成本极低。

Conclusion: WetSAM具备强泛化能力，为可扩展、低成本、高分辨率的湿地测绘提供了有效解决方案。

Abstract: Accurate wetland mapping is essential for ecosystem monitoring, yet dense pixel-level annotation is prohibitively expensive and practical applications usually rely on sparse point labels, under which existing deep learning models perform poorly, while strong seasonal and inter-annual wetland dynamics further render single-date imagery inadequate and lead to significant mapping errors; although foundation models such as SAM show promising generalization from point prompts, they are inherently designed for static images and fail to model temporal information, resulting in fragmented masks in heterogeneous wetlands. To overcome these limitations, we propose WetSAM, a SAM-based framework that integrates satellite image time series for wetland mapping from sparse point supervision through a dual-branch design, where a temporally prompted branch extends SAM with hierarchical adapters and dynamic temporal aggregation to disentangle wetland characteristics from phenological variability, and a spatial branch employs a temporally constrained region-growing strategy to generate reliable dense pseudo-labels, while a bidirectional consistency regularization jointly optimizes both branches. Extensive experiments across eight global regions of approximately 5,000 km2 each demonstrate that WetSAM substantially outperforms state-of-the-art methods, achieving an average F1-score of 85.58%, and delivering accurate and structurally consistent wetland segmentation with minimal labeling effort, highlighting its strong generalization capability and potential for scalable, low-cost, high-resolution wetland mapping.

</details>


### [40] [SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces](https://arxiv.org/abs/2601.11402)
*Meng Han*

Main category: cs.CV

TL;DR: 提出SME-YOLO框架，通过NWDLoss、EUCB和MSFA模块显著提升PCB微小缺陷检测精度


<details>
  <summary>Details</summary>
Motivation: PCB缺陷尺寸小、纹理相似、尺度不均，传统方法检测精度低

Method: 基于YOLOv11n，引入NWDLoss降低位置敏感性，EUCB增强细节恢复，MSFA模块实现多尺度特征自适应聚焦

Result: 在PKU-PCB数据集上，mAP提升2.2%，Precision提升4%，达到SOTA

Conclusion: SME-YOLO有效解决微小缺陷检测难题，为PCB质量检测提供新方案

Abstract: Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.

</details>


### [41] [Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints](https://arxiv.org/abs/2601.11409)
*Wenxiao Li,Xue-Cheng Tai,Jun Liu*

Main category: cs.CV

TL;DR: 提出一种融合宽度信息的拓扑先验方法，改进持久同调用于图像分割，同时保留连接性、亏格和结构粗细等属性


<details>
  <summary>Details</summary>
Motivation: 传统拓扑方法缺乏宽度信息，限制了其在图像分割中对结构厚度和长度的精确建模

Method: 结合持久同调与PDE平滑技术，修改上水平集的局部极值，将宽度信息嵌入拓扑结构，并整合到变分模型和神经网络中

Result: 在保持拓扑不变量（如连接性、亏格）的同时，有效保留了线段粗细和长度等宽度属性

Conclusion: 所提方法显著提升了图像分割的拓扑保真度与几何精确性，为医学影像等应用提供新工具

Abstract: Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. Accurately capturing these topological features often requires incorporating width-related information, including the thickness and length inherent to the image structures. However, traditional mathematical definitions of topological structures lack this dimensional width information, limiting methods like persistent homology from fully addressing practical segmentation needs. To overcome this limitation, we propose a novel mathematical framework that explicitly integrates width information into the characterization of topological structures. This method leverages persistent homology, complemented by smoothing concepts from partial differential equations (PDEs), to modify local extrema of upper-level sets. This approach enables the resulting topological structures to inherently capture width properties. We incorporate this enhanced topological description into variational image segmentation models. Using some proper loss functions, we are also able to design neural networks that can segment images with the required topological and width properties. Through variational constraints on the relevant topological energies, our approach successfully preserves essential topological invariants such as connectivity and genus counts, simultaneously ensuring that segmented structures retain critical width attributes, including line thickness and length. Numerical experiments demonstrate the effectiveness of our method, showcasing its capability to maintain topological fidelity while explicitly embedding width characteristics into segmented image structures.

</details>


### [42] [PubMed-OCR: PMC Open Access OCR Annotations](https://arxiv.org/abs/2601.11425)
*Hunter Heidenreich,Yosheb Getachew,Olivia Dinica,Ben Elliott*

Main category: cs.CV

TL;DR: 发布了一个基于PubMed Central的OCR语料库，包含150万页科学文章的标注数据，支持布局感知建模和OCR评估。


<details>
  <summary>Details</summary>
Motivation: 现有OCR语料库缺乏大规模、高精度的科学文献标注数据，限制了布局感知模型和OCR依赖管道的发展。

Method: 从PubMed Central开放获取PDF中提取页面图像，使用Google Cloud Vision进行OCR标注，并以紧凑JSON格式发布词、行、段落级别的边界框信息。

Result: 构建了包含20.95万篇文章、150万页、约13亿词的语料库，支持多种下游任务，同时分析了其期刊覆盖与布局特征及OCR引擎依赖等局限性。

Conclusion: 该语料库为科学文献的OCR与布局分析研究提供了宝贵资源，鼓励社区进一步扩展与优化。

Abstract: PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.

</details>


### [43] [Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps](https://arxiv.org/abs/2601.11442)
*Xiangjun Gao,Zhensong Zhang,Dave Zhenyu Chen,Songcen Xu,Long Quan,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: Map2Thought通过Metric-CogMap和Cog-CoT实现可解释的3D视觉语言模型空间推理，仅用一半数据达到接近全量数据的性能，并显著超越SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D VLM缺乏明确且可解释的空间推理能力，难以理解三维结构关系。

Method: 引入Metric-CogMap（离散网格+连续度量表示）和Cog-CoT（向量运算、边界框距离、遮挡感知顺序等确定性操作）进行显式几何推理。

Result: 在VSI-Bench上，仅用50%数据达到60.9%基线的59.9%准确率，且在10%/25%/50%数据子集下分别超越SOTA 5.3%/4.8%/4.0%。

Conclusion: Map2Thought实现了高效、可解释的3D空间推理，显著降低数据依赖并提升性能。

Abstract: We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.

</details>


### [44] [PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs](https://arxiv.org/abs/2601.11451)
*Oishee Bintey Hoque,Nibir Chandra Mandal,Kyle Luong,Amanda Wilson,Samarth Swarup,Madhav Marathe,Abhijin Adiga*

Main category: cs.CV

TL;DR: 提出一种基于航空与卫星图像的可解释CAFO识别管道，结合YOLOv8与SAM2实现基础设施检测，并通过空间交叉注意力分类器提升分类性能，达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 大规模畜禽养殖场对人类健康和环境构成风险，且易受疾病与极端天气影响，亟需可扩展的精准测绘方法。

Method: 使用领域调优的YOLOv8检测基础设施（如畜舍、粪池等），生成SAM2掩码并筛选；提取结构化特征（数量、面积、朝向等）与视觉特征融合，采用轻量级空间交叉注意力分类器进行CAFO类型预测与掩码归因。

Result: Swin-B+PRISM-CAFO模型超越最优基线高达15%，在全美多样区域表现稳健，并通过梯度-激活分析验证领域先验的有效性。

Conclusion: 该方法实现高精度、可解释的CAFO自动识别，为环境监管与风险评估提供可落地的技术支持。

Abstract: Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho

</details>


### [45] [MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models](https://arxiv.org/abs/2601.11464)
*Xiaoran Fan,Zhichao Sun,Tao Ji,Lixing Shen,Tao Gui*

Main category: cs.CV

TL;DR: 提出MHA2MLA-VLM框架，无需重新预训练即可将现有视觉语言模型高效转换为MLA架构，显著压缩KV缓存并保持性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型推理中KV缓存膨胀导致内存和计算瓶颈，现有MLA压缩方法依赖昂贵预训练，缺乏高效适配方案。

Method: 采用模态自适应部分RoPE和模态解耦低秩近似，结合参数高效微调，以最小化输出激活误差而非参数距离进行优化。

Result: 在三个主流VLM上验证，显著减少KV缓存体积，仅需少量标注数据即可恢复原始性能，并与KV量化无缝兼容。

Conclusion: MHA2MLA-VLM为现有VLM提供高效、轻量级的MLA迁移方案，兼顾推理速度与模型性能。

Abstract: As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.

</details>


### [46] [Generative Scenario Rollouts for End-to-End Autonomous Driving](https://arxiv.org/abs/2601.11475)
*Rajeev Yasarla,Deepti Hegde,Shizhong Han,Hsin-Pai Cheng,Yunxiao Shi,Meysam Sadeghigooghari,Shweta Mahajan,Apratim Bhattacharyya,Litian Liu,Risheek Garrepalli,Thomas Svantesson,Fatih Porikli,Hong Cai*

Main category: cs.CV

TL;DR: GeRo通过语言条件生成和自回归滚动提升VLA模型的长 horizon 规划能力，显著提高驾驶性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖模仿学习，未充分挖掘其作为生成模型的潜力，难以支持长时程、可解释的决策。

Method: GeRo框架联合训练VLA模型编码动态与语言信息，并通过语言条件的自回归生成未来潜在标记与文本响应，辅以滚动一致性损失稳定预测。

Result: 在Bench2Drive上，驾驶得分和成功率分别提升+15.7和+26.2，结合强化学习后达到SOTA性能，具备强零样本鲁棒性。

Conclusion: 语言条件的生成式推理为更安全、可解释的端到端自动驾驶提供了新范式。

Abstract: Vision-Language-Action (VLA) models are emerging as highly effective planning models for end-to-end autonomous driving systems. However, current works mostly rely on imitation learning from sparse trajectory annotations and under-utilize their potential as generative models. We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy. First, a VLA model is trained to encode ego vehicle and agent dynamics into latent tokens under supervision from planning, motion, and language tasks, facilitating text-aligned generation. Next, GeRo performs language-conditioned autoregressive generation. Given multi-view images, a scenario description, and ego-action questions, it generates future latent tokens and textual responses to guide long-horizon rollouts. A rollout-consistency loss stabilizes predictions using ground truth or pseudo-labels, mitigating drift and preserving text-action alignment. This design enables GeRo to perform temporally consistent, language-grounded rollouts that support long-horizon reasoning and multi-agent planning. On Bench2Drive, GeRo improves driving score and success rate by +15.7 and +26.2, respectively. By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness. These results highlight the promise of generative, language-conditioned reasoning as a foundation for safer and more interpretable end-to-end autonomous driving.

</details>


### [47] [ReScene4D: Temporally Consistent Semantic Instance Segmentation of Evolving Indoor 3D Scenes](https://arxiv.org/abs/2601.11508)
*Emily Steiner,Jianhao Zheng,Henry Howard-Jenkins,Chris Xie,Iro Armeni*

Main category: cs.CV

TL;DR: 提出ReScene4D方法，实现稀疏时序4D室内语义实例分割，提升实例追踪一致性与3D分割质量


<details>
  <summary>Details</summary>
Motivation: 现有3DSIS方法缺乏时序推理，4D LiDAR方法依赖高频采样，难以应对室内环境长期演变中的稀疏观测问题

Method: ReScene4D通过跨观测信息共享机制，扩展3DSIS架构以支持稀疏时序实例分割，无需密集观测

Result: 在3RScan数据集上达到SOTA性能，提出新评估指标t-mAP以衡量时序身份一致性

Conclusion: 该方法有效解决了稀疏时序4D分割难题，同时提升3D分割效果，为理解动态室内场景建立新基准

Abstract: Indoor environments evolve as objects move, appear, or disappear. Capturing these dynamics requires maintaining temporally consistent instance identities across intermittently captured 3D scans, even when changes are unobserved. We introduce and formalize the task of temporally sparse 4D indoor semantic instance segmentation (SIS), which jointly segments, identifies, and temporally associates object instances. This setting poses a challenge for existing 3DSIS methods, which require a discrete matching step due to their lack of temporal reasoning, and for 4D LiDAR approaches, which perform poorly due to their reliance on high-frequency temporal measurements that are uncommon in the longer-horizon evolution of indoor environments. We propose ReScene4D, a novel method that adapts 3DSIS architectures for 4DSIS without needing dense observations. It explores strategies to share information across observations, demonstrating that this shared context not only enables consistent instance tracking but also improves standard 3DSIS quality. To evaluate this task, we define a new metric, t-mAP, that extends mAP to reward temporal identity consistency. ReScene4D achieves state-of-the-art performance on the 3RScan dataset, establishing a new benchmark for understanding evolving indoor scenes.

</details>


### [48] [ShapeR: Robust Conditional 3D Shape Generation from Casual Captures](https://arxiv.org/abs/2601.11514)
*Yawar Siddiqui,Duncan Frost,Samir Aroudj,Armen Avetisyan,Henry Howard-Jenkins,Daniel DeTone,Pierre Moulon,Qirui Wu,Zhengqin Li,Julian Straub,Richard Newcombe,Jakob Engel*

Main category: cs.CV

TL;DR: ShapeR提出一种从随意拍摄序列生成高精度3D形状的新方法，显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状生成方法依赖干净、无遮挡的输入，难以应对真实世界中随意捕获的复杂数据。

Method: 利用SLAM、3D检测和视觉语言模型提取稀疏点云、多视角图像和文本描述，通过校正流变换器生成3D形状，并采用在线增强、课程学习和背景处理策略提升鲁棒性。

Result: 在新构建的178个真实场景对象基准上，ShapeR的Chamfer距离比现有方法提升2.7倍。

Conclusion: ShapeR首次实现从随意捕获序列生成高保真3D形状，为真实世界3D生成开辟新路径。

Abstract: Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.

</details>


### [49] [UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation](https://arxiv.org/abs/2601.11522)
*Ruiheng Zhang,Jingfeng Yao,Huangxuan Zhao,Hao Yan,Xiao He,Lei Chen,Zhou Wei,Yong Luo,Zengmao Wang,Lefei Zhang,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: UniX通过解耦理解与生成任务，结合自注意力机制和扩散模型，在医疗图像理解与生成上实现协同提升，性能超越现有模型且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数共享自回归架构的方法在医学图像理解与生成任务间存在目标冲突，导致性能妥协。

Method: UniX采用自回归分支负责理解，扩散分支负责生成，并引入跨模态自注意力机制动态引导生成，配合数据清洗与多阶段训练策略。

Result: 在两个基准上，理解性能提升46.1%（Micro-F1），生成质量提升24.2%（FD-RadDino），参数仅为LLM-CXR的四分之一。

Conclusion: UniX实现了与任务专用模型相当的性能，建立了可扩展的医学图像理解与生成协同范式。

Abstract: Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [50] [Verified Design of Robotic Autonomous Systems using Probabilistic Model Checking](https://arxiv.org/abs/2601.10720)
*Atef Azaiez,Alireza David Anisi*

Main category: cs.RO

TL;DR: 提出使用概率模型检测（PMC）系统评估农业机器人设计概念，生成经验证的设计方案。


<details>
  <summary>Details</summary>
Motivation: 机器人自主系统（RAS）在复杂动态环境中设计困难，传统方法难以系统化评估设计概念，需更严谨的分析方法。

Method: 采用概率模型检测（PMC）工具PRISM，结合农业机器人专用设计评估标准，对多个设计概念进行形式化建模与验证。

Result: 成功应用PMC方法筛选出经验证的农业机器人设计概念，并提出了领域特定的评估标准。

Conclusion: PMC方法能有效支持RAS早期设计阶段的系统化决策，提升安全与可靠性。

Abstract: Safety and reliability play a crucial role when designing Robotic Autonomous Systems (RAS). Early consideration of hazards, risks and mitigation actions -- already in the concept study phase -- are important steps in building a solid foundations for the subsequent steps in the system engineering life cycle. The complex nature of RAS, as well as the uncertain and dynamic environments the robots operate within, do not merely effect fault management and operation robustness, but also makes the task of system design concept selection, a hard problem to address. Approaches to tackle the mentioned challenges and their implications on system design, range from ad-hoc concept development and design practices, to systematic, statistical and analytical techniques of Model Based Systems Engineering. In this paper, we propose a methodology to apply a formal method, namely Probabilistic Model Checking (PMC), to enable systematic evaluation and analysis of a given set of system design concepts, ultimately leading to a set of Verified Designs (VD). We illustrate the application of the suggested methodology -- using PRISM as probabilistic model checker -- to a practical RAS concept selection use-case from agriculture robotics. Along the way, we also develop and present a domain-specific Design Evaluation Criteria for agri-RAS.

</details>


### [51] [Collaborative Continuum Robots: A Survey](https://arxiv.org/abs/2601.10721)
*Xinyu Li,Qian Tang,Guoxin Yin,Gang Zheng,Jessica Burgner-Kahrs,Cesare Stefanini,Ke Wu*

Main category: cs.RO

TL;DR: 本文综述了协作连续机器人（CCRs）的最新进展，分类为三种协作模式，并系统总结了结构设计、建模、运动规划与控制的进展。


<details>
  <summary>Details</summary>
Motivation: 协作连续机器人能显著提升任务适应性、工作空间、灵活性、负载能力和操作稳定性，但缺乏系统性综述，亟需全面框架指导研究。

Method: 基于系统架构层次，将CCRs分为分离协作、辅助协作与并行协作三类，系统梳理各模式下的结构设计、建模、运动规划与控制研究进展。

Result: 建立了CCRs的分类框架，总结了各协作模式的技术进展，并指出了当前挑战与未来机遇。

Conclusion: CCRs具有广阔应用前景，未来需在协同控制、动态建模与模块化设计等方面突破，推动其在复杂场景中的实用化。

Abstract: Continuum robots (CRs), owing to their compact structure, inherent compliance, and flexible deformation, have been widely applied in various fields. By coordinating multiple CRs to form collaborative continuum robots (CCRs), task adaptability, workspace, flexibility, load capacity, and operational stability can be further improved, thus offering significant advantages. In recent years, interest in this emerging field has grown steadily within the continuum-robotics community, accompanied by a consistent rise in related publications. By presenting a comprehensive overview of recent progress from different system-architecture levels, this survey provides a clear framework for research on CCRs. First, CCRs are classified into the three collaboration modes of separated collaboration, assistance collaboration, and parallel collaboration, with definitions provided. Next, advances in structural design, modeling, motion planning, and control for each mode are systematically summarized. Finally, current challenges and future opportunities for CCRs are discussed.

</details>


### [52] [A Survey of Real-Time Support, Analysis, and Advancements in ROS 2](https://arxiv.org/abs/2601.10722)
*Daniel Casini,Jian-Jia Chen,Jing Li,Federico Reghenzani,Harun Teper*

Main category: cs.RO

TL;DR: 本文综述了ROS 2在实时系统领域的研究进展，涵盖其调度机制、通信优化及社区增强方案，旨在系统化提升ROS 2的实时性能。


<details>
  <summary>Details</summary>
Motivation: ROS 2虽广泛用于机器人系统，但其原生设计非为实时场景，近年来实时系统领域与工业界对其实时能力关注度上升，亟需系统性梳理相关研究。

Method: 通过分析ROS 2内部调度架构与DDS通信机制，系统回顾单/多线程执行器的定时分析、实时指标（响应时间、反应时间、数据年龄）、通信模式、运行时增强（如micro-ROS、GPU管理）及延迟绑定技术，并构建分类 taxonomy。

Result: 整理了近六年关键研究成果，提出了分类框架，明确了实时增强的技术路径，为后续研究与工业部署提供系统参考。

Conclusion: ROS 2的实时能力可通过架构优化、通信改进与工具链完善显著提升，未来需进一步标准化评估方法与跨平台支持。

Abstract: The Robot Operating System 2 (ROS~2) has emerged as a relevant middleware framework for robotic applications, offering modularity, distributed execution, and communication. In the last six years, ROS~2 has drawn increasing attention from the real-time systems community and industry. This survey presents a comprehensive overview of research efforts that analyze, enhance, and extend ROS~2 to support real-time execution. We first provide a detailed description of the internal scheduling mechanisms of ROS~2 and its layered architecture, including the interaction with DDS-based communication and other communication middleware. We then review key contributions from the literature, covering timing analysis for both single- and multi-threaded executors, metrics such as response time, reaction time, and data age, and different communication modes. The survey also discusses community-driven enhancements to the ROS~2 runtime, including new executor algorithm designs, real-time GPU management, and microcontroller support via micro-ROS. Furthermore, we summarize techniques for bounding DDS communication delays, message filters, and profiling tools that have been developed to support analysis and experimentation. To help systematize this growing body of work, we introduce taxonomies that classify the surveyed contributions based on different criteria. This survey aims to guide both researchers and practitioners in understanding and improving the real-time capabilities of ROS~2.

</details>


### [53] [Energy-Efficient Omnidirectional Locomotion for Wheeled Quadrupeds via Predictive Energy-Aware Nominal Gait Selection](https://arxiv.org/abs/2601.10723)
*Xu Yang,Wei Yang,Kaibo He,Bo Yang,Yanan Sui,Yilin Mo*

Main category: cs.RO

TL;DR: 提出一种分层控制框架，结合预测能耗模型与强化学习，优化轮腿机器人多方向运动能效，降低能耗达35%。


<details>
  <summary>Details</summary>
Motivation: 轮腿机器人在多样环境中面临显著的能量优化挑战，传统固定步态方法能效低。

Method: 采用新颖的功率预测网络预测1秒内不同步态的能耗，选择最优标称步态，再通过强化学习生成残差调整以平衡能效与性能。

Result: 相比固定步态方法，能耗降低高达35%，同时保持相近的速度跟踪性能，在仿真和真实机器人上验证有效。

Conclusion: 所提框架显著提升轮腿机器人能效，具备鲁棒性与实用性，适用于复杂环境中的高效运动。

Abstract: Wheeled-legged robots combine the efficiency of wheels with the versatility of legs, but face significant energy optimization challenges when navigating diverse environments. In this work, we present a hierarchical control framework that integrates predictive power modeling with residual reinforcement learning to optimize omnidirectional locomotion efficiency for wheeled quadrupedal robots. Our approach employs a novel power prediction network that forecasts energy consumption across different gait patterns over a 1-second horizon, enabling intelligent selection of the most energy-efficient nominal gait. A reinforcement learning policy then generates residual adjustments to this nominal gait, fine-tuning the robot's actions to balance energy efficiency with performance objectives. Comparative analysis shows our method reduces energy consumption by up to 35\% compared to fixed-gait approaches while maintaining comparable velocity tracking performance. We validate our framework through extensive simulations and real-world experiments on a modified Unitree Go1 platform, demonstrating robust performance even under external disturbances. Videos and implementation details are available at \href{https://sites.google.com/view/switching-wpg}{https://sites.google.com/view/switching-wpg}.

</details>


### [54] [Adaptive Sliding Mode Control for Vehicle Platoons with State-Dependent Friction Uncertainty](https://arxiv.org/abs/2601.10724)
*Rishabh Dev Yadav*

Main category: cs.RO

TL;DR: 提出一种新型自适应滑模控制器，用于轮式移动机器人编队，无需先验知识即可应对未知摩擦力影响。


<details>
  <summary>Details</summary>
Motivation: 现有自适应控制器难以准确建模和识别状态依赖且不可先验有界的摩擦力，影响编队控制精度与鲁棒性。

Method: 采用两阶段自适应滑模控制：第一阶段基于期望轨迹计算期望速度（运动学层），第二阶段生成控制命令实现运动（动力学层），分离运动学与动力学以提升控制效率与鲁棒性。

Result: 控制器可在无摩擦力参数先验知识的情况下，有效应对外部扰动和系统不确定性，维持预设安全间距与编队速度。

Conclusion: 所提方法显著提升了轮式机器人编队在复杂地面条件下的控制性能，为实际应用提供了更鲁棒的解决方案。

Abstract: Multi-robot formation control has various applications in domains such as vehicle troops, platoons, payload transportation, and surveillance. Maintaining formation in a vehicle platoon requires designing a suitable control scheme that can tackle external disturbances and uncertain system parameters while maintaining a predefined safe distance between the robots. A crucial challenge in this context is dealing with the unknown/uncertain friction forces between wheels and the ground, which vary with changes in road surface, wear in tires, and speed of the vehicle. Although state-of-the-art adaptive controllers can handle a priori bounded uncertainties, they struggle with accurately modeling and identifying frictional forces, which are often state-dependent and cannot be a priori bounded.
  This thesis proposes a new adaptive sliding mode controller for wheeled mobile robot-based vehicle platoons that can handle the unknown and complex behavior of frictional forces without prior knowledge of their parameters and structures. The controller uses the adaptive sliding mode control techniques to regulate the platoon's speed and maintain a predefined inter-robot distance, even in the presence of external disturbances and uncertain system parameters. This approach involves a two-stage process: first, the kinematic controller calculates the desired velocities based on the desired trajectory; and second, the dynamics model generates the commands to achieve the desired motion. By separating the kinematics and dynamics of the robot, this approach can simplify the control problem and allow for more efficient and robust control of the wheeled mobile robot.

</details>


### [55] [Multi-Agent Formation Navigation Using Diffusion-Based Trajectory Generation](https://arxiv.org/abs/2601.10725)
*Hieu Do Quang,Chien Truong-Quoc,Quoc Van Tran*

Main category: cs.RO

TL;DR: 使用扩散模型规划领导者-跟随者编队轨迹，在复杂环境中实现低误差平滑运动


<details>
  <summary>Details</summary>
Motivation: 传统编队控制方法在复杂环境中难以保证轨迹平滑性与鲁棒性，需探索更高效的规划方法

Method: 基于扩散策略生成双领导者中点轨迹，跟随者利用局部相对位置信息进行距离约束跟踪

Result: 实现平滑运动与低跟踪误差，失败主要发生在训练数据外的狭窄或异常障碍场景

Conclusion: 扩散模型在多智能体编队规划中具有较高可靠性与应用潜力

Abstract: This paper introduces a diffusion-based planner for leader--follower formation control in cluttered environments. The diffusion policy is used to generate the trajectory of the midpoint of two leaders as a rigid bar in the plane, thereby defining their desired motion paths in a planar formation. While the followers track the leaders and form desired foramtion geometry using a distance-constrained formation controller based only on the relative positions in followers' local coordinates. The proposed approach produces smooth motions and low tracking errors, with most failures occurring in narrow obstacle-free space, or obstacle configurations that are not in the training data set. Simulation results demonstrate the potential of diffusion models for reliable multi-agent formation planning.

</details>


### [56] [Bidirectional Human-Robot Communication for Physical Human-Robot Interaction](https://arxiv.org/abs/2601.10796)
*Junxiang Wang,Cindy Wang,Rana Soltani Zarrin,Zackory Erickson*

Main category: cs.RO

TL;DR: BRIDGE系统通过自然语言实现实时人机轨迹交互，并提供双向语音反馈，显著提升老年人使用机器人的互动性与透明度。


<details>
  <summary>Details</summary>
Motivation: 传统机器人辅助系统缺乏透明性和双向沟通，用户难以理解或修改机器人的动作，影响信任与使用体验。

Method: 利用大语言模型（LLM）解析用户自然语言指令，实时调整机器人的位置、速度和力，并通过语音反馈确认或追问，实现双向通信。

Result: 18位老年用户在三项辅助任务中成功实时修改轨迹，加入语音反馈的BRIDGE系统在互动性和透明度评分上显著优于无反馈基线。

Conclusion: 双向语音反馈是提升人机物理交互直观性与信任的关键，BRIDGE为未来辅助机器人提供了可扩展的沟通框架。

Abstract: Effective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot's planned trajectory -- position, velocity, and force -- in real time using natural language. We utilize a large language model (LLM) to interpret any trajectory modifications implied by user commands in the context of the planned motion and conversation history. Importantly, our system provides verbal feedback in response to the user, either assuring any resulting changes or posing a clarifying question. We evaluated our method in a user study with 18 older adults across three assistive tasks, comparing BRIDGE to an ablation without verbal feedback and a baseline. Results show that participants successfully used the system to modify trajectories in real time. Moreover, the bidirectional feedback led to significantly higher ratings of interactivity and transparency, demonstrating that the robot's verbal response is critical for a more intuitive user experience. Videos and code can be found on our project website: https://bidir-comm.github.io/

</details>


### [57] [SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM](https://arxiv.org/abs/2601.10814)
*Onur Bagoren,Seth Isaacson,Sacchin Sundar,Yung-Ching Sun,Anja Sheppard,Haoyu Ma,Abrar Shariff,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 提出一种基于仿真训练和自监督微调的水下立体深度估计框架，并结合多传感器融合实现实时水下SLAM，同时发布大规模真实水下数据集。


<details>
  <summary>Details</summary>
Motivation: 水下图像因光衰减、纹理缺失和动态光照导致立体深度估计困难，现有陆地训练模型无法直接迁移，且缺乏真实水下标注数据。

Method: 采用仿真数据进行初始训练，结合自监督微调提升模型泛化能力；构建融合IMU、气压计和DVL的SLAM系统agname；采集包含24,000对立体图像的真实水下沉船数据集。

Result: 在真实水下数据上显著提升深度估计精度，实现高精度轨迹估计与复杂沉船结构的3D重建。

Conclusion: 仿真+自监督训练范式有效解决水下立体估计的域迁移问题，所提框架与数据集为水下机器人感知提供了新基准。

Abstract: Localization and mapping are core perceptual capabilities for underwater robots. Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks. However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging. In underwater environments, images are degraded by light attenuation, visual artifacts, and dynamic lighting conditions. Furthermore, real-world underwater scenes frequently lack rich texture useful for stereo depth estimation and 3D reconstruction. As a result, stereo estimation networks trained on in-air data cannot transfer directly to the underwater domain. In addition, there is a lack of real-world underwater stereo datasets for supervised training of neural networks. Poor underwater depth estimation is compounded in stereo-based Simultaneous Localization and Mapping (SLAM) algorithms, making it a fundamental challenge for underwater robot perception. To address these challenges, we propose a novel framework that enables sim-to-real training of underwater stereo disparity estimation networks using simulated data and self-supervised finetuning. We leverage our learned depth predictions to develop \algname, a novel framework for real-time underwater SLAM that fuses stereo cameras with IMU, barometric, and Doppler Velocity Log (DVL) measurements. Lastly, we collect a challenging real-world dataset of shipwreck surveys using an underwater robot. Our dataset features over 24,000 stereo pairs, along with high-quality, dense photogrammetry models and reference trajectories for evaluation. Through extensive experiments, we demonstrate the advantages of the proposed training approach on real-world data for improving stereo estimation in the underwater domain and for enabling accurate trajectory estimation and 3D reconstruction of complex shipwreck sites.

</details>


### [58] [Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets](https://arxiv.org/abs/2601.10827)
*Simin Liu,Tong Zhao,Bernhard Paus Graesdal,Peter Werner,Jiuguang Wang,John Dolan,Changliu Liu,Tao Pang*

Main category: cs.RO

TL;DR: 提出一种新的接触丰富操作规划方法，实现近似最优解，显著提升效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的规划器仅关注可行性，未能充分利用接触丰富操作的潜力。

Method: 离线构建互可达集图，在线基于图规划全局最优运动序列。

Result: 在挑战性任务中，任务成本降低61%，成功率91%，查询时间低于一分钟。

Conclusion: 全局优化的接触丰富操作已具备实际应用可行性。

Abstract: If we consider human manipulation, it is clear that contact-rich manipulation (CRM)-the ability to use any surface of the manipulator to make contact with objects-can be far more efficient and natural than relying solely on end-effectors (i.e., fingertips). However, state-of-the-art model-based planners for CRM are still focused on feasibility rather than optimality, limiting their ability to fully exploit CRM's advantages. We introduce a new paradigm that computes approximately optimal manipulator plans. This approach has two phases. Offline, we construct a graph of mutual reachable sets, where each set contains all object orientations reachable from a starting object orientation and grasp. Online, we plan over this graph, effectively computing and sequencing local plans for globally optimized motion. On a challenging, representative contact-rich task, our approach outperforms a leading planner, reducing task cost by 61%. It also achieves a 91% success rate across 250 queries and maintains sub-minute query times, ultimately demonstrating that globally optimized contact-rich manipulation is now practical for real-world tasks.

</details>


### [59] [IMU-based Real-Time Crutch Gait Phase and Step Detections in Lower-Limb Exoskeletons](https://arxiv.org/abs/2601.10832)
*Anis R. Shakkour,David Hexner,Yehuda Bitton,Avishai Sintov*

Main category: cs.RO

TL;DR: 使用单个低成本IMU实现高精度步态相位检测，无需机械改造，TCN模型表现最优，成功率达94%。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂力传感器，存在控制延迟，需更轻量、实时的解决方案。

Method: 采用单IMU集成于手杖握把，提出五阶段分类模型，结合FSM增强生物力学一致性，对比三种深度学习架构。

Result: TCN在PC和嵌入式系统上表现最佳，步态检测成功率达94%，并能泛化至瘫痪用户。

Conclusion: 该框架为外骨骼控制提供低成本、高性能、实时的解决方案，具备临床应用潜力。

Abstract: Lower limb exoskeletons and prostheses require precise, real time gait phase and step detections to ensure synchronized motion and user safety. Conventional methods often rely on complex force sensing hardware that introduces control latency. This paper presents a minimalist framework utilizing a single, low cost Inertial-Measurement Unit (IMU) integrated into the crutch hand grip, eliminating the need for mechanical modifications. We propose a five phase classification system, including standard gait phases and a non locomotor auxiliary state, to prevent undesired motion. Three deep learning architectures were benchmarked on both a PC and an embedded system. To improve performance under data constrained conditions, models were augmented with a Finite State Machine (FSM) to enforce biomechanical consistency. The Temporal Convolutional Network (TCN) emerged as the superior architecture, yielding the highest success rates and lowest latency. Notably, the model generalized to a paralyzed user despite being trained exclusively on healthy participants. Achieving a 94% success rate in detecting crutch steps, this system provides a high performance, cost effective solution for real time exoskeleton control.

</details>


### [60] [Is open robotics innovation a threat to international peace and security?](https://arxiv.org/abs/2601.10877)
*Ludovic Righetti,Vincent Boulanin*

Main category: cs.RO

TL;DR: 机器人领域缺乏针对双重用途风险的专门监管，呼吁建立行业特定的负责任创新路线图


<details>
  <summary>Details</summary>
Motivation: 开放性虽促进机器人发展，但也降低了军事和有害用途的门槛，而当前缺乏像核、生化或AI领域那样的具体监管指导

Method: 提出四项实践路径：负责任机器人教育、激励风险评估、控制高风险材料传播、制定红线

Result: 为机器人领域构建专属的负责任研究与创新框架

Conclusion: 机器人社区亟需制定自身特有的监管与引导规范，以平衡开放创新与安全风险

Abstract: Open access to publication, software and hardware is central to robotics: it lowers barriers to entry, supports reproducible science and accelerates reliable system development. However, openness also exacerbates the inherent dual-use risks associated with research and innovation in robotics. It lowers barriers for states and non-state actors to develop and deploy robotics systems for military use and harmful purposes. Compared to other fields of engineering where dual-use risks are present - e.g., those that underlie the development of weapons of mass destruction (chemical, biological, radiological, and nuclear weapons) and even the field of AI, robotics offers no specific regulation and little guidance as to how research and innovation may be conducted and disseminated responsibly. While other fields can be used for guidance, robotics has its own needs and specificities which have to be taken into account. The robotics community should therefore work toward its own set of sector-specific guidance and possibly regulation. To that end, we propose a roadmap focusing on four practices: a) education in responsible robotics; b) incentivizing risk assessment; c) moderating the diffusion of high-risk material; and d) developing red lines.

</details>


### [61] [Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation](https://arxiv.org/abs/2601.10930)
*Zhixian Xie,Yu Xiang,Michael Posa,Wanxin Jin*

Main category: cs.RO

TL;DR: 提出一种分层RL-MPC框架，通过高层接触意图与低层动力学规划，实现高效、低数据需求的灵巧操作，实现近100%成功率与零样本仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视觉运动策略数据需求大、仿真到现实迁移差、泛化能力弱，难以处理接触丰富的灵巧操作。

Method: 采用分层RL-MPC框架：高层RL预测接触意图（接触位置与目标位姿），低层接触隐式MPC优化接触模式并生成动作。

Result: 在非抓取任务中实现近100%成功率，数据需求减少10倍，具备强鲁棒性和零样本仿真到现实迁移能力。

Conclusion: 利用任务的层次结构可显著提升灵巧操作的效率、泛化性与现实可部署性。

Abstract: A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.

</details>


### [62] [Crane Lowering Guidance Using a Attachable Camera Module for Driver Vision Support](https://arxiv.org/abs/2601.11026)
*HyoJae Kang,SunWoo Ahn,InGyu Choi,GeonYeong Go,KunWoo Son,Min-Sung Kang*

Main category: cs.RO

TL;DR: 提出一种可吸附在负载上的摄像头模块，实时传输下方着陆区图像，提升起重机操作安全性。


<details>
  <summary>Details</summary>
Motivation: 起重机操作中负载遮挡视线，依赖地面人员指挥存在安全隐患。

Method: 设计附带单板计算机、电池和摄像头的吸附式模块，实时采集并传输负载下方图像，生成安装引导。

Result: 初步实验验证了图像实时采集与传输的可行性。

Conclusion: 该系统有望显著提升建筑工地安全性，为操作员提供直观的视觉参考。

Abstract: Cranes have long been essential equipment for lifting and placing heavy loads in construction projects. This study focuses on the lowering phase of crane operation, the stage in which the load is moved to the desired location. During this phase, a constant challenge exists: the load obstructs the operator's view of the landing point. As a result, operators traditionally have to rely on verbal or gestural instructions from ground personnel, which significantly impacts site safety. To alleviate this constraint, the proposed system incorporates a attachable camera module designed to be attached directly to the load via a suction cup. This module houses a single-board computer, battery, and compact camera. After installation, it streams and processes images of the ground directly below the load in real time to generate installation guidance. Simultaneously, this guidance is transmitted to and monitored by a host computer. Preliminary experiments were conducted by attaching this module to a test object, confirming the feasibility of real-time image acquisition and transmission. This approach has the potential to significantly improve safety on construction sites by providing crane operators with an instant visual reference of hidden landing zones.

</details>


### [63] [H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning](https://arxiv.org/abs/2601.11063)
*Haishan Zeng,Peng Li*

Main category: cs.RO

TL;DR: 提出H-AIM框架，通过LLM与经典规划器结合，提升异构机器人团队执行长时程任务的成功率


<details>
  <summary>Details</summary>
Motivation: 现有LLM在长时程推理和动态多机器人协调中表现不足，亟需更有效的多机器人任务规划框架

Method: 采用三阶段级联架构：1）LLM解析指令生成PDDL问题；2）LLM语义推理与经典规划器结合优化动作序列；3）将计划编译为行为树实现反应式控制，并通过共享黑板机制支持异构机器人协同

Result: 在新构建的MACE-THOR基准上，H-AIM将任务成功率从12%提升至55%，目标条件召回率从32%提升至72%，显著优于基线LaMMA-P

Conclusion: H-AIM有效融合LLM与传统规划方法，实现了高效、动态、可扩展的异构机器人团队任务规划

Abstract: In embodied artificial intelligence, enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions remains a critical challenge. While large language models (LLMs) show promise in instruction parsing and preliminary planning, they exhibit limitations in long-term reasoning and dynamic multi-robot coordination. We propose Hierarchical Autonomous Intelligent Multi-Robot Planning(H-AIM), a novel embodied multi-robot task planning framework that addresses these issues through a three-stage cascaded architecture: 1) It leverages an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, thereby transforming commands into formal planning problems; 2) It combines the semantic reasoning of LLMs with the search capabilities of a classical planner to produce optimized action sequences; 3) It compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization. To validate our approach, we introduce the MACE-THOR benchmark dataset, comprising 42 complex tasks across 8 distinct household layouts. Experimental results demonstrate that H-AIM achieves a remarkable performance improvement, elevating the task success rate from 12% to 55% and boosting the goal condition recall from 32% to 72% against the strongest baseline, LaMMA-P.

</details>


### [64] [A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation](https://arxiv.org/abs/2601.11076)
*Jiaqi Liang,Yue Chen,Qize Yu,Yan Shen,Haipeng Zhang,Hao Dong,Ruihai Wu*

Main category: cs.RO

TL;DR: 提出A3D框架，通过自适应支撑策略实现机器人双臂协同组装家具，泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 家具组装对机器人而言具有挑战性，需精确的双臂协调与动态支撑策略，现有方法难以适应多样几何形状和装配过程中的状态变化。

Method: A3D框架利用密集点云几何表示建模部件交互模式，并引入自适应模块，基于交互反馈动态调整支撑策略。

Result: 在包含50个部件、8类家具的仿真与真实环境中，A3D在多样几何形状和家具类型上均表现出良好泛化能力。

Conclusion: A3D有效解决了机器人家具组装中的自适应支撑与几何泛化问题，为复杂双臂协作任务提供了新方案。

Abstract: Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries. We propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions. We establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.

</details>


### [65] [Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments](https://arxiv.org/abs/2601.11078)
*Jiaohong Yao,Linfeng Liang,Yao Deng,Xi Zheng,Richard Han,Yuankai Qi*

Main category: cs.RO

TL;DR: 通过AirSim仿真评估不同城市环境下基于标记的无人机着陆系统，比较启发式策略与强化学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设理想观测条件，缺乏在复杂城市环境中的鲁棒性评估。

Method: 在AirSim平台上构建多样化城市布局、光照和天气条件，使用RGB相机检测标记、深度相机避障，对比两种启发式覆盖模式和一个强化学习智能体。

Result: 结果表明，探索策略和场景复杂度显著影响着陆成功率、路径效率和鲁棒性，强调需在真实传感器条件下评估系统。

Conclusion: 应基于真实传感器受限环境设计和评估无人机自主着陆系统，以提升城市环境下的可靠性。

Abstract: Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. We present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity. Using onboard camera sensors (RGB for marker detection and depth for obstacle avoidance), we benchmark two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness. Results underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.

</details>


### [66] [Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model](https://arxiv.org/abs/2601.11143)
*Minho Lee,Hyeonseok Kim,Jin Tak Kim,Sangshin Park,Jeong Hyun Lee,Jungsan Cho,Jemin Hwangbo*

Main category: cs.RO

TL;DR: 提出一种基于液压动力学的解析执行器模型，实现重载液压四足机器人在强化学习中的高效仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 液压机器人因复杂流体动力学和慢速响应，难以进行精确仿真，限制了强化学习的应用。

Method: 构建一种解析执行器模型，可在1微秒内预测12个执行器的关节力矩，替代神经网络模型。

Result: 在重达300公斤的液压四足机器人上成功实现稳定鲁棒的命令跟踪运动，为首个此类成功案例。

Conclusion: 该模型在数据有限场景下优于神经网络模型，显著提升了sim-to-real迁移的效率与可靠性。

Abstract: The simulation-to-reality (sim-to-real) transfer of large-scale hydraulic robots presents a significant challenge in robotics because of the inherent slow control response and complex fluid dynamics. The complex dynamics result from the multiple interconnected cylinder structure and the difference in fluid rates of the cylinders. These characteristics complicate detailed simulation for all joints, making it unsuitable for reinforcement learning (RL) applications. In this work, we propose an analytical actuator model driven by hydraulic dynamics to represent the complicated actuators. The model predicts joint torques for all 12 actuators in under 1 microsecond, allowing rapid processing in RL environments. We compare our model with neural network-based actuator models and demonstrate the advantages of our model in data-limited scenarios. The locomotion policy trained in RL with our model is deployed on a hydraulic quadruped robot, which is over 300 kg. This work is the first demonstration of a successful transfer of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot, demonstrating advanced sim-to-real transferability.

</details>


### [67] [Adaptive Monitoring of Stochastic Fire Front Processes via Information-seeking Predictive Control](https://arxiv.org/abs/2601.11231)
*Savvas Papaioannou,Panayiotis Kolios,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.RO

TL;DR: 提出一种集成感知、估计与控制的随机最优控制框架，用于无人机自适应监测野火前沿，理论上保证了策略收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖线性高斯假设或启发式近似，缺乏性能保障，无法统一处理野火传播的随机非线性特性。

Method: 将监测问题建模为随机最优控制问题，推导出非线性椭圆增长模型的最优递归贝叶斯估计器，并转化为有限时域马尔可夫决策过程，采用基于置信下界的自适应搜索算法设计信息驱动的预测控制律。

Result: 实现了感知-估计-控制的联合优化，理论证明控制策略渐近收敛至最优策略。

Conclusion: 该方法突破了传统分离式处理的局限，为随机非线性环境下的自主监测提供了理论严谨且具有收敛保证的解决方案。

Abstract: We consider the problem of adaptively monitoring a wildfire front using a mobile agent (e.g., a drone), whose trajectory determines where sensor data is collected and thus influences the accuracy of fire propagation estimation. This is a challenging problem, as the stochastic nature of wildfire evolution requires the seamless integration of sensing, estimation, and control, often treated separately in existing methods. State-of-the-art methods either impose linear-Gaussian assumptions to establish optimality or rely on approximations and heuristics, often without providing explicit performance guarantees. To address these limitations, we formulate the fire front monitoring task as a stochastic optimal control problem that integrates sensing, estimation, and control. We derive an optimal recursive Bayesian estimator for a class of stochastic nonlinear elliptical-growth fire front models. Subsequently, we transform the resulting nonlinear stochastic control problem into a finite-horizon Markov decision process and design an information-seeking predictive control law obtained via a lower confidence bound-based adaptive search algorithm with asymptotic convergence to the optimal policy.

</details>


### [68] [VLAgents: A Policy Server for Efficient VLA Inference](https://arxiv.org/abs/2601.11250)
*Tobias Jülg,Khaled Gamal,Nisarga Nilavadi,Pierre Krack,Seongjin Bien,Michael Krawez,Florian Walter,Wolfram Burgard*

Main category: cs.RO

TL;DR: VLAgents是一个模块化策略服务器，通过统一的Gymnasium接口简化视觉-语言-动作模型在机器人中的部署，支持零拷贝共享内存和压缩流传输，显著提升本地与远程通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型部署因接口碎片化和通信延迟而复杂，亟需统一、高效的推理服务架构。

Method: 设计VLAgents作为模块化策略服务器，抽象VLA推理为Gymnasium风格协议，支持零拷贝共享内存（仿真）和压缩流传输（远程硬件）。

Result: 在包含OpenVLA、Pi Zero等7个策略的基准测试中，VLAgents在本地和远程通信场景下均优于OpenVLA、OpenPi和LeRobot的默认服务器。

Conclusion: VLAgents通过统一接口与自适应通信层，有效解决了VLA部署中的延迟与兼容性问题，为机器人系统提供高效、可扩展的推理基础设施。

Abstract: The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents

</details>


### [69] [Skill-Aware Diffusion for Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.11266)
*Aoshen Huang,Jiaming Chen,Jiyu Cheng,Ran Song,Wei Pan,Wei Zhang*

Main category: cs.RO

TL;DR: 提出Skill-Aware Diffusion (SADiff)，利用技能信息提升机器人操作的泛化能力，并发布新数据集IsaacSkill。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩增数据和网络提升泛化，但忽视了任务间的技能层级信息，而同一技能下的任务具有相似运动模式。

Method: SADiff通过可学习的技能令牌构建技能感知编码模块，结合技能约束的扩散模型生成物体中心运动流，并采用技能检索转换策略优化2D到3D动作映射。

Result: 在仿真和真实机器人实验中，SADiff在多种操作任务上表现出优异的性能与泛化能力。

Conclusion: 引入技能层级信息能显著提升机器人操作的泛化性，SADiff结合新数据集IsaacSkill为后续研究提供有效基准。

Abstract: Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.

</details>


### [70] [Distributed Control Barrier Functions for Safe Multi-Vehicle Navigation in Heterogeneous USV Fleets](https://arxiv.org/abs/2601.11335)
*Tyler Paine,Brendan Long,Jeremy Wenger,Michael DeFilippo,James Usevitch,Michael Benjamin*

Main category: cs.RO

TL;DR: 本文提出一种基于控制屏障函数的分布式安全滤波器，用于异构无人船队的避碰，结合COLREGS行为规则，在仿真与实船实验中验证了其安全与效率优势。


<details>
  <summary>Details</summary>
Motivation: 异构无人船队因决策与控制方式不同，且实时共享轨迹与控制值受限，导致避碰困难。

Method: 为每艘无人船添加基于控制屏障函数（CBF）的分布式安全滤波器，假设其他船只（包括有人船）采取最坏行为，并与COLREGS行为规则结合。

Result: 仿真与三类无人船加有人船的实船实验表明，CBF与COLREGS结合方法在安全性和效率上表现最佳。

Conclusion: CBF与COLREGS融合策略能有效应对异构平台与非合作行为，是实现可靠避碰的实用方案。

Abstract: Collision avoidance in heterogeneous fleets of uncrewed vessels is challenging because the decision-making processes and controllers often differ between platforms, and it is further complicated by the limitations on sharing trajectories and control values in real-time. This paper presents a pragmatic approach that addresses these issues by adding a control filter on each autonomous vehicle that assumes worst-case behavior from other contacts, including crewed vessels. This distributed safety control filter is developed using control barrier function (CBF) theory and the application is clearly described to ensure explainability of these safety-critical methods. This work compares the worst-case CBF approach with a Collision Regulations (COLREGS) behavior-based approach in simulated encounters. Real-world experiments with three different uncrewed vessels and a human operated vessel were performed to confirm the approach is effective across a range of platforms and is robust to uncooperative behavior from human operators. Results show that combining both CBF methods and COLREGS behaviors achieves the best safety and efficiency.

</details>


### [71] [The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning](https://arxiv.org/abs/2601.11394)
*Henrik Hose,Paul Brunzema,Devdutt Subhasish,Sebastian Trimpe*

Main category: cs.RO

TL;DR: 发布了一个开源Mini Wheelbot平衡轮独轮车的高精度动力学数据集，支持多种控制算法的基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于专用机器人硬件访问困难，阻碍了不稳定系统学习型控制算法的开发，亟需高质量真实数据集。

Method: 采集了1kHz同步多模态数据（传感器、状态估计、运动捕捉位姿、第三方视频），覆盖多台硬件、多种表面和控制方法（如随机二进制激励、非线性MPC、强化学习）。

Result: 提供了涵盖动力学建模、状态估计和时间序列分类的示例应用，可供研究者基准验证。

Conclusion: 该数据集填补了开放机器人动力学数据的空白，促进不稳定系统控制算法的可复现研究。

Abstract: The development of robust learning-based control algorithms for unstable systems requires high-quality, real-world data, yet access to specialized robotic hardware remains a significant barrier for many researchers. This paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source, quasi-symmetric balancing reaction wheel unicycle. The dataset provides 1 kHz synchronized data encompassing all onboard sensor readings, state estimates, ground-truth poses from a motion capture system, and third-person video logs. To ensure data diversity, we include experiments across multiple hardware instances and surfaces using various control paradigms, including pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents. We include several example applications in dynamics model learning, state estimation, and time-series classification to illustrate common robotics algorithms that can be benchmarked on our dataset.

</details>


### [72] [ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models](https://arxiv.org/abs/2601.11404)
*Linqing Zhong,Yi Liu,Yifei Wei,Ziyu Xiong,Maoqing Yao,Si Liu,Guanghui Ren*

Main category: cs.RO

TL;DR: 提出Action Chain-of-Thought (ACoT)范式，通过显式与隐式动作推理提升视觉-语言-动作模型的执行精度，在多个基准上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型依赖多模态嵌入直接生成动作，中间推理（如子任务或目标图像）信息粒度不足，难以支持精细动作控制。

Method: 提出ACoT-VLA架构，包含显式动作推理器（EAR）生成粗粒度动作轨迹，和隐式动作推理器（IAR）提取动作先验，二者共同引导动作头学习。

Result: 在LIBERO、LIBERO-Plus和VLABench上分别达到98.5%、84.1%和47.4%的成功率。

Conclusion: 在动作空间内进行直接推理能显著提升机器人策略的精准性与泛化能力，ACoT为VLA模型提供了更有效的推理范式。

Abstract: Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.

</details>


### [73] [The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents](https://arxiv.org/abs/2601.11421)
*Ziyu Wang,Chenyuan Liu,Yushun Xiang,Runhao Zhang,Qingbo Hao,Hongliang Lu,Houyu Chen,Zhizhong Feng,Kaiyue Zheng,Dehao Ye,Xianchao Zeng,Xinyu Zhou,Boran Wen,Jiaxin Li,Mingyu Zhang,Kecheng Zheng,Qian Zhu,Ran Cheng,Yong-Lu Li*

Main category: cs.RO

TL;DR: 提出GM-100，一个包含100个系统设计任务的机器人学习基准，用于全面评估机器人代理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和任务设计缺乏系统性，难以真实反映机器人方法的性能差异。

Method: 基于人-物交互原语和物体功能，系统构建100个多样化、长尾行为任务，并收集多平台轨迹数据，评估基线模型。

Result: GM-100任务可执行且能有效区分当前VLA模型的性能差异。

Conclusion: GM-100为机器人学习奥运会奠定基础，推动任务设计的多样性与复杂性。

Abstract: Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.

</details>


### [74] [Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations](https://arxiv.org/abs/2601.11460)
*Franziska Herbert,Vignesh Prasad,Han Liu,Dorothea Koert,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 提出一种语义-几何任务图表示法，结合MPNN与Transformer，从人类演示中学习双手机器人任务的结构化表征，并实现跨任务迁移和在线动作选择。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型难以捕捉双手机器人任务中动作顺序、物体参与和几何关系的高变异性，亟需一种能联合建模语义结构与几何时序演化的表征方法。

Method: 采用语义-几何任务图编码物体身份、关系及时序几何变化，使用MPNN编码器学习场景图表示，Transformer解码器基于动作上下文预测未来动作、物体及运动。

Result: 在人类演示数据集上显著优于序列模型，尤其在高变异性任务中表现优异，并成功迁移至物理双手机器人实现在线动作选择。

Conclusion: 语义-几何任务图是一种可复用的任务抽象，为机器人操作系统的决策系统提供了有效且泛化的结构化表征。

Abstract: Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic-geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic-geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [75] [Analytic Bijections for Smooth and Interpretable Normalizing Flows](https://arxiv.org/abs/2601.10774)
*Mathis Gerdes,Miranda C. N. Cheng*

Main category: cs.LG

TL;DR: 提出三种全局光滑、可解析反演的双射函数（三次有理、sinh、三次多项式），并设计径向流架构，在保持高表达力的同时显著提升训练稳定性和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有归一化流中的标量双射存在表达力、光滑性与可逆性之间的权衡：仿射变换表达力弱、样条分段光滑且限于有界域、残差流需数值求逆。

Method: 引入三种解析可逆的全局光滑双射函数，并提出径向流架构，通过直接参数化径向坐标实现角度不变的变换。

Result: 在1D/2D基准上表现优于样条，在高维φ⁴格点场论中超越仿射基线，参数量减少1000倍仍保持性能。

Conclusion: 所提方法融合了现有方法的优势，实现了表达力、光滑性与可逆性的统一，为归一化流提供了更高效、稳定的构建模块。

Abstract: A key challenge in designing normalizing flows is finding expressive scalar bijections that remain invertible with tractable Jacobians. Existing approaches face trade-offs: affine transformations are smooth and analytically invertible but lack expressivity; monotonic splines offer local control but are only piecewise smooth and act on bounded domains; residual flows achieve smoothness but need numerical inversion. We introduce three families of analytic bijections -- cubic rational, sinh, and cubic polynomial -- that are globally smooth ($C^\infty$), defined on all of $\mathbb{R}$, and analytically invertible in closed form, combining the favorable properties of all prior approaches. These bijections serve as drop-in replacements in coupling flows, matching or exceeding spline performance. Beyond coupling layers, we develop radial flows: a novel architecture using direct parametrization that transforms the radial coordinate while preserving angular direction. Radial flows exhibit exceptional training stability, produce geometrically interpretable transformations, and on targets with radial structure can achieve comparable quality to coupling flows with $1000\times$ fewer parameters. We provide comprehensive evaluation on 1D and 2D benchmarks, and demonstrate applicability to higher-dimensional physics problems through experiments on $φ^4$ lattice field theory, where our bijections outperform affine baselines and enable problem-specific designs that address mode collapse.

</details>


### [76] [Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework](https://arxiv.org/abs/2601.10779)
*Qingyue Zhang,Chang Chu,Haohao Fu,Tianren Peng,Yanru Wu,Guanbo Huang,Yang Li,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 提出UOWQ框架，联合优化源任务权重与转移样本量，在数据稀缺下提升迁移学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅优化权重或样本量，忽视二者联合优化，且均匀迁移易导致负迁移。

Method: 基于KL散度泛化误差的渐近分析，建立UOWQ理论框架，推导出单源闭式解与多源凸优化解，证明充分使用所有源样本在权重调整后最优。

Result: 在DomainNet和Office-Home等基准上显著超越强基线，验证理论与实践有效性。

Conclusion: UOWQ通过联合优化权重与样本量，实现更高效、稳定的多源迁移学习。

Abstract: Transfer learning plays a vital role in improving model performance in data-scarce scenarios. However, naive uniform transfer from multiple source tasks may result in negative transfer, highlighting the need to properly balance the contributions of heterogeneous sources. Moreover, existing transfer learning methods typically focus on optimizing either the source weights or the amount of transferred samples, while largely neglecting the joint consideration of the other. In this work, we propose a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), which formulates multi-source transfer learning as a parameter estimation problem grounded in an asymptotic analysis of a Kullback-Leibler divergence-based generalization error measure. The proposed framework jointly determines the optimal source weights and optimal transfer quantities for each source task. Firstly, we prove that using all available source samples is always optimal once the weights are properly adjusted, and we provide a theoretical explanation for this phenomenon. Moreover, to determine the optimal transfer weights, our analysis yields closed-form solutions in the single-source setting and develops a convex optimization-based numerical procedure for the multi-source case. Building on the theoretical results, we further propose practical algorithms for both multi-source transfer learning and multi-task learning settings. Extensive experiments on real-world benchmarks, including DomainNet and Office-Home, demonstrate that UOWQ consistently outperforms strong baselines. The results validate both the theoretical predictions and the practical effectiveness of our framework.

</details>


### [77] [Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs](https://arxiv.org/abs/2601.10801)
*Alberto Coppi,Ema Puljak,Lorenzo Borella,Daniel Jaschke,Enrique Rico,Maurizio Pierini,Jacopo Pazzini,Andrea Triossi,Simone Montangero*

Main category: cs.LG

TL;DR: 张量网络模型（MPS/TTN）在高能物理实时喷注标记中实现亚微秒延迟，适合FPGA部署


<details>
  <summary>Details</summary>
Motivation: HL-LHC Level-1触发系统对低延迟和资源效率的严格需求，需替代深度神经网络

Method: 使用喷注组分特征构建MPS和TTN模型，结合后训练量化优化硬件实现

Result: 模型性能媲美最优深度学习方法，FPGA综合显示亚微秒延迟与低资源占用

Conclusion: 张量网络是低延迟环境中高效推理的可行且有潜力的方案

Abstract: We present a systematic study of Tensor Network (TN) models $\unicode{x2013}$ Matrix Product States (MPS) and Tree Tensor Networks (TTN) $\unicode{x2013}$ for real-time jet tagging in high-energy physics, with a focus on low-latency deployment on Field Programmable Gate Arrays (FPGAs). Motivated by the strict requirements of the HL-LHC Level-1 trigger system, we explore TNs as compact and interpretable alternatives to deep neural networks. Using low-level jet constituent features, our models achieve competitive performance compared to state-of-the-art deep learning classifiers. We investigate post-training quantization to enable hardware-efficient implementations without degrading classification performance or latency. The best-performing models are synthesized to estimate FPGA resource usage, latency, and memory occupancy, demonstrating sub-microsecond latency and supporting the feasibility of online deployment in real-time trigger systems. Overall, this study highlights the potential of TN-based models for fast and resource-efficient inference in low-latency environments.

</details>


### [78] [Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning -- Towards a Pure Neural Logic Core](https://arxiv.org/abs/2601.10810)
*Mengmeng Peng,Zhenyu Fang,He Sun*

Main category: cs.LG

TL;DR: 提出数字代谢假说，通过RLCP框架实现事实遗忘与逻辑核心分离，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因参数纠缠导致记忆墙与幻觉问题，需解耦逻辑与事实以提升效率。

Method: 引入再生逻辑核心协议（RLCP），采用双流训练与梯度反转使事实依赖线性不可解码。

Result: Qwen2.5-0.5B模型事实保留率<7%，出现结构结晶效应，并自发采用思维链推理。

Conclusion: 为神经CPU+符号RAM架构提供新路径，推动模块化模型设计。

Abstract: Large language models (LLMs) currently suffer from parameter entanglement, where general reasoning capabilities (logic) and specific factual knowledge (facts) exist in a superposition state within shared weights. This coupling leads to the "memory wall," where computational capacity is squandered on simulating retrieval, often resulting in hallucinations. In this paper, we propose "digital metabolism," a thermodynamic hypothesis suggesting that targeted forgetting is necessary for distilling a pure neural logic core. To validate this hypothesis, we introduce the Regenerative Logic-Core Protocol (RLCP), a dual-stream training framework that renders specific factual dependencies linearly undecodable via deep-layer gradient reversal. Applying RLCP to Qwen2.5-0.5B, we observe a distinct phase transition: the model achieves near-zero retention of targeted factual associations (Accuracy < 7%) while exhibiting changes consistent with an emergent "structural crystallization" effect. Empirical analysis on GSM8K reveals that the "metabolized" model spontaneously adopts chain-of-thought (CoT) scaffolding, which we interpret as compensating for the loss of direct associative recall (shifting from $O(1)$ recall to $O(N)$ reasoning). While the causal mechanism underlying this behavioral shift requires further investigation, our findings provide a dynamic weight-level counterpart to architectural innovations like DeepSeek's Engram, paving the way for modular "Neural CPU + Symbolic RAM" architectures.

</details>


### [79] [Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents](https://arxiv.org/abs/2601.10820)
*Himanshu Thakur,Anusha Kamath,Anurag Muthyala,Dhwani Sanmukhani,Smruthi Mukund,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出一种基于规划器的多智能体框架，显著提升代码生成在特征工程中的效率与可靠性，将周期从三周缩短至一天。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成模型在生产级特征工程中面临三大挑战：缺乏真实迭代数据、AI工具难以个性化集成、人机协作反馈不及时。

Method: 采用图结构建模团队环境，通过LLM规划器协调多智能体，实现分步代码生成、上下文感知提示、失败回溯修正及关键步骤人工干预。

Result: 在自有数据集上相较人工和无规划方法分别提升38%和150%指标；实际应用于推荐系统，特征工程周期从三周降至一天。

Conclusion: 该框架有效解决生产级特征工程中的自动化瓶颈，实现高效、可靠且符合团队实践的AI辅助开发。

Abstract: Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.

</details>


### [80] [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)
*Daniel Price,Prabhu Vellaisamy,John Shen,Di Wu*

Main category: cs.LG

TL;DR: 本文提出Mugi架构，通过改进值级并行（VLP）技术显著提升大语言模型的性能、能效与可持续性，最高实现45倍吞吐量和668倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLP方法仅适用于对称激活与权重的低精度GEMM，难以应对LLM中复杂的非线性操作和小批量非对称输入，亟需更通用高效的VLP方案。

Method: 扩展VLP支持非线性逼近（值中心精度分配），优化小批量非对称GEMM（结合权重量化、KV缓存量化与组查询注意力），并设计新型架构Mugi整合上述创新。

Result: Mugi在非线性softmax上实现最高45×吞吐量和668×能效提升，在LLM上实现2.07×吞吐量和3.11×能效提升，同时降低操作碳排放1.45×和隐含碳排放1.48×。

Conclusion: Mugi通过价值中心的VLP革新，首次实现对全LLM工作负载的高效支持，显著提升性能、能效与可持续性，为低功耗大模型部署提供新范式。

Abstract: Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

</details>


### [81] [AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures](https://arxiv.org/abs/2601.10859)
*Dat Quoc Ha,Md Ferdous Alam,Markus J. Buehler,Faez Ahmed,Josephine V. Carstensen*

Main category: cs.LG

TL;DR: 提出一种AI协作者，通过U-Net模型预测用户偏好区域，显著提升拓扑优化效率与设计质量。


<details>
  <summary>Details</summary>
Motivation: 传统拓扑优化计算耗时且黑箱，现有人机交互方法依赖低效的迭代区域选择。

Method: 采用U-Net架构的图像分割模型，基于合成数据训练，预测用户偏好修改区域。

Result: 模型可泛化至非标准问题，预测准确，使设计时间仅增加15秒，但屈曲载荷提升39%。

Conclusion: AI协作者有效增强人机协同拓扑优化，兼顾效率与性能，具备实际应用潜力。

Abstract: Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.

</details>


### [82] [Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting](https://arxiv.org/abs/2601.10863)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: 提出一种新的评估指标forecast AC score，同时考虑预测准确性与时间一致性，显著降低预测波动性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测仅优化准确性，忽视预测结果随预测时间变化的一致性问题。

Method: 引入可微分的forecast AC score，结合用户自定义权重平衡准确性与稳定性，并用于优化季节性ARIMA模型。

Result: 在M4 Hourly数据集上，AC优化模型在保持或提升预测精度的同时，目标时间点的预测波动性降低75%。

Conclusion: forecast AC score能有效提升概率多步预测的质量，兼顾准确性与时间一致性，具有实际应用价值。

Abstract: Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally provides for user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal ARIMA models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Our AC-optimized models achieve a 75\% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

</details>


### [83] [Unit-Consistent (UC) Adjoint for GSD and Backprop in Deep Learning Applications](https://arxiv.org/abs/2601.10873)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: 通过引入单位一致（UC）伴随算子，使梯度下降与神经网络的规范对称性兼容，提升优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降不满足深度神经网络中线性与齐次非线性组件的规范对称性，导致优化轨迹依赖参数化方式。

Method: 在反向传播中用单位一致（UC）伴随替代欧几里得转置，构建规范一致的最速下降与反向传播算法。

Result: 提出了一种统一适用于网络各组件与优化器状态的简洁算子级方法，增强优化对参数化不敏感性。

Conclusion: UC伴随方法为规范不变优化提供了简单有效的理论框架，可推广至多种同质网络结构。

Abstract: Deep neural networks constructed from linear maps and positively homogeneous nonlinearities (e.g., ReLU) possess a fundamental gauge symmetry: the network function is invariant to node-wise diagonal rescalings. However, standard gradient descent is not equivariant to this symmetry, causing optimization trajectories to depend heavily on arbitrary parameterizations. Prior work has proposed rescaling-invariant optimization schemes for positively homogeneous networks (e.g., path-based or path-space updates). Our contribution is complementary: we formulate the invariance requirement at the level of the backward adjoint/optimization geometry, which provides a simple, operator-level recipe that can be applied uniformly across network components and optimizer state. By replacing the Euclidean transpose with a Unit-Consistent (UC) adjoint, we derive UC gauge-consistent steepest descent and backprogation.

</details>


### [84] [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905)
*Rajat Ghosh,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出Action Shapley作为数据选择指标，通过随机动态算法高效计算，显著提升世界模型训练效率与性能。


<details>
  <summary>Details</summary>
Motivation: 世界模型依赖高质量训练数据，但传统数据选择方法biased且计算复杂，需更优的评估与选择机制。

Method: 引入Action Shapley作为数据贡献度度量，设计随机动态算法降低Shapley值计算复杂度。

Result: 在5个真实数据受限场景中，计算效率提升超80%，且基于Action Shapley的选择策略优于人工选择。

Conclusion: Action Shapley为世界模型提供高效、无偏的数据选择方法，显著提升模型训练效果。

Abstract: Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

</details>


### [85] [Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation](https://arxiv.org/abs/2601.10911)
*Zhang Xiaocai,Xiao Zhe,Liang Maohan,Liu Tao,Li Haijiang,Zhang Wenbin*

Main category: cs.LG

TL;DR: 提出一种结合课程强化学习与数据驱动模拟的框架，实现船舶航行的安全与低碳导航


<details>
  <summary>Details</summary>
Motivation: 传统船舶导航依赖人工经验，缺乏自主性和排放意识，易引发安全风险与高排放问题

Method: 构建基于真实航行数据与扩散模型的仿真环境，结合机器学习预测油耗，设计轻量级课程强化学习代理，奖励函数综合考虑安全、排放、准时与目标达成

Result: 在印度洋区域验证了该框架在提升航行安全性与降低排放方面的有效性

Conclusion: 所提框架能有效支持复杂环境下的可持续船舶导航，具有实际应用潜力

Abstract: Sustainability is becoming increasingly critical in the maritime transport, encompassing both environmental and social impacts, such as Greenhouse Gas (GHG) emissions and navigational safety. Traditional vessel navigation heavily relies on human experience, often lacking autonomy and emission awareness, and is prone to human errors that may compromise safety. In this paper, we propose a Curriculum Reinforcement Learning (CRL) framework integrated with a realistic, data-driven marine simulation environment and a machine learning-based fuel consumption prediction module. The simulation environment is constructed using real-world vessel movement data and enhanced with a Diffusion Model to simulate dynamic maritime conditions. Vessel fuel consumption is estimated using historical operational data and learning-based regression. The surrounding environment is represented as image-based inputs to capture spatial complexity. We design a lightweight, policy-based CRL agent with a comprehensive reward mechanism that considers safety, emissions, timeliness, and goal completion. This framework effectively handles complex tasks progressively while ensuring stable and efficient learning in continuous action spaces. We validate the proposed approach in a sea area of the Indian Ocean, demonstrating its efficacy in enabling sustainable and safe vessel navigation.

</details>


### [86] [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)
*Francis Ndikum Nji,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出FAConvLSTM，一种更高效、可解释且能捕捉长程气候关联的ConvLSTM2D替代方法，显著降低计算开销并提升表示质量。


<details>
  <summary>Details</summary>
Motivation: 传统ConvLSTM2D计算开销大、局部感受野限制了长程空间结构和气候动力学建模。

Method: FAConvLSTM通过1x1瓶颈、共享深度卷积、多尺度膨胀分支、挤压激励机制、窥孔连接和稀疏轴向注意力，结合子空间头与季节性位置编码增强时空建模能力。

Result: 在多变量气候数据上表现优于ConvLSTM2D，生成更稳定、可解释、鲁棒的潜在表示，同时大幅降低计算成本。

Conclusion: FAConvLSTM在效率、空间表达力与物理可解释性上全面超越ConvLSTM2D，适合高分辨率地球观测数据建模。

Abstract: Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convolutional gating incurs high computational cost and its strictly local receptive fields limit the modeling of long-range spatial structure and disentangled climate dynamics. To address these limitations, we propose FAConvLSTM, a Factorized-Attention ConvLSTM layer designed as a drop-in replacement for ConvLSTM2D that simultaneously improves efficiency, spatial expressiveness, and physical interpretability. FAConvLSTM factorizes recurrent gate computations using lightweight [1 times 1] bottlenecks and shared depthwise spatial mixing, substantially reducing channel complexity while preserving recurrent dynamics. Multi-scale dilated depthwise branches and squeeze-and-excitation recalibration enable efficient modeling of interacting physical processes across spatial scales, while peephole connections enhance temporal precision. To capture teleconnection-scale dependencies without incurring global attention cost, FAConvLSTM incorporates a lightweight axial spatial attention mechanism applied sparsely in time. A dedicated subspace head further produces compact per timestep embeddings refined through temporal self-attention with fixed seasonal positional encoding. Experiments on multivariate spatiotemporal climate data shows superiority demonstrating that FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

</details>


### [87] [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)
*Aakriti,Zhe Li,Dandan Liang,Chao Huang,Rui Li,Haibo Yang*

Main category: cs.LG

TL;DR: HOSL通过在客户端使用零阶优化、服务器端使用一阶优化，显著降低边缘设备内存开销，同时保持高精度与快速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有分裂学习依赖一阶优化，导致客户端内存开销大；零阶优化虽省内存但收敛慢，亟需平衡内存效率与优化性能。

Method: 提出HOSL框架，客户端采用零阶梯度估计消除反向传播与激活存储，服务器端保留一阶优化以保证收敛速度与精度。

Result: 在OPT模型上，客户端GPU内存降低达3.7倍，精度仅比一阶方法低0.20%-4.23%，且比纯零阶方法提升最高达15.55%。

Conclusion: HOSL有效缓解了边缘设备内存瓶颈，实现了内存效率与训练性能的最优权衡，为资源受限环境下的大模型协作训练提供了新范式。

Abstract: Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which requires clients to store intermediate quantities such as activations for backpropagation. This results in substantial memory overhead, largely negating benefits of model partitioning. In contrast, zeroth-order (ZO) optimization eliminates backpropagation and significantly reduces memory usage, but often suffers from slow convergence and degraded performance. In this work, we propose HOSL, a novel Hybrid-Order Split Learning framework that addresses this fundamental trade-off between memory efficiency and optimization effectiveness by strategically integrating ZO optimization on the client side with FO optimization on the server side. By employing memory-efficient ZO gradient estimation at the client, HOSL eliminates backpropagation and activation storage, reducing client memory consumption. Meanwhile, server-side FO optimization ensures fast convergence and competitive performance. Theoretically, we show that HOSL achieves a $\mathcal{O}(\sqrt{d_c/TQ})$ rate, which depends on client-side model dimension $d_c$ rather than the full model dimension $d$, demonstrating that convergence improves as more computation is offloaded to the server. Extensive experiments on OPT models (125M and 1.3B parameters) across 6 tasks demonstrate that HOSL reduces client GPU memory by up to 3.7$\times$ compared to the FO method while achieving accuracy within 0.20%-4.23% of this baseline. Furthermore, HOSL outperforms the ZO baseline by up to 15.55%, validating the effectiveness of our hybrid strategy for memory-efficient training on edge devices.

</details>


### [88] [Multivariate LSTM-Based Forecasting for Renewable Energy: Enhancing Climate Change Mitigation](https://arxiv.org/abs/2601.10961)
*Farshid Kamrani,Kristen Schell*

Main category: cs.LG

TL;DR: 本文提出一种基于多变量LSTM的可再生能源发电预测模型，利用历史数据提升预测精度，降低碳排放并增强供电可靠性。


<details>
  <summary>Details</summary>
Motivation: 可再生能源发电具有强波动性，传统方法如K均值聚类生成场景难以捕捉其时序依赖与非线性模式，亟需更精准的预测方法。

Method: 构建多变量LSTM网络，融合本地及邻区历史数据，建模多种可再生能源间的长期依赖与交互关系。

Result: 实验表明，该模型显著提升预测准确性，降低CO2排放，并提高电力负荷供应的可靠性。

Conclusion: 基于多变量LSTM的预测方法优于传统方法，为高比例可再生能源电网提供了更高效、低碳的运行支持。

Abstract: The increasing integration of renewable energy sources (RESs) into modern power systems presents significant opportunities but also notable challenges, primarily due to the inherent variability of RES generation. Accurate forecasting of RES generation is crucial for maintaining the reliability, stability, and economic efficiency of power system operations. Traditional approaches, such as deterministic methods and stochastic programming, frequently depend on representative scenarios generated through clustering techniques like K-means. However, these methods may fail to fully capture the complex temporal dependencies and non-linear patterns within RES data. This paper introduces a multivariate Long Short-Term Memory (LSTM)-based network designed to forecast RESs generation using their real-world historical data. The proposed model effectively captures long-term dependencies and interactions between different RESs, utilizing historical data from both local and neighboring areas to enhance predictive accuracy. In the case study, we showed that the proposed forecasting approach results in lower CO2 emissions, and a more reliable supply of electric loads.

</details>


### [89] [Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent](https://arxiv.org/abs/2601.10962)
*Ning Yang,Yikuan Zhang,Qi Ouyang,Chao Tang,Yuhai Tu*

Main category: cs.LG

TL;DR: SGD偏好平坦解的动态机制被揭示，噪声延迟冻结过程从而提升泛化性。


<details>
  <summary>Details</summary>
Motivation: SGD为何偏好平坦且泛化性更好的解，其动力学起源尚不明确。

Method: 通过分析SGD动力学与数值实验，结合物理模型构建有效势能，发现瞬态冻结机制。

Result: SGD在训练初期探索多个谷区，随后因能量壁垒增大而冻结于平坦区域；增加噪声可延迟冻结，增强收敛到平坦极小点。

Conclusion: 提出统一的物理框架，连接学习动力学、损失几何与泛化，为优化算法设计提供新原则。

Abstract: Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.

</details>


### [90] [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)
*Zain ul Abdeen,Waris Gill,Ming Jin*

Main category: cs.LG

TL;DR: 提出一种元引导无梯度强化学习框架（MGF-RL），在可再生能源不确定性下实现配电网灾后负荷恢复的快速自适应控制。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在新型断电场景和可再生能源波动下泛化能力差、重训练成本高，难以满足实时恢复需求。

Method: 结合一阶元学习与进化策略，构建无梯度元学习框架，从历史停电经验中学习可迁移的策略初始化，实现少量微调下的快速适应。

Result: 在IEEE 13和123节点系统上，MGF-RL在可靠性、恢复速度和适应效率上优于标准RL、MAML和MPC，且对未见场景具有强泛化能力。

Conclusion: MGF-RL通过理论子线性遗憾界验证了其适应效率与任务相似性的关系，适合可再生能源丰富的配电网实时恢复应用。

Abstract: Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.

</details>


### [91] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 使用轻量级符号推理监督提升小型程序修复模型的修复类型分类性能，无需增加模型规模。


<details>
  <summary>Details</summary>
Motivation: 小型代码模型在资源受限场景中虽具吸引力，但通常仅输出单一预测，难以判断其是否学习到程序结构或依赖浅层相关性。

Method: 提出一种推理蒸馏方法，由大模型提供结构化符号推理标签（非自由文本），与修复类型标签一同训练基于CodeT5的小模型。

Result: 符号推理监督在IntroClass基准上一致提升宏观平均性能，尤其改善低频错误类别；推理轨迹正确与预测正确强相关但不完全决定。

Conclusion: 符号推理蒸馏是提升轻量级程序修复模型可解释性与鲁棒性的实用方法。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [92] [Constant Metric Scaling in Riemannian Computation](https://arxiv.org/abs/2601.10992)
*Kisung You*

Main category: cs.LG

TL;DR: 本文澄清了黎曼度量的全局尺度缩放对几何量的影响，区分了变化与不变量，并指出其在优化中仅改变步长而非几何结构。


<details>
  <summary>Details</summary>
Motivation: 实践中常引入全局尺度参数缩放黎曼度量，但其对几何量的影响常被混淆，需明确区分变化与不变量。

Method: 通过自包含的分析，系统区分常数尺度缩放下变化的量（如范数、距离）与不变的几何对象（如联络、测地线）。

Result: 明确指出尺度缩放不影响黎曼几何核心结构（如连接、测地线），仅改变数值量，优化中等价于全局步长调整。

Conclusion: 该缩放操作是纯数值调整，不应误认为改变流形几何，为黎曼计算提供清晰的理论基础。

Abstract: Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.

</details>


### [93] [Backdoor Attacks on Multi-modal Contrastive Learning](https://arxiv.org/abs/2601.11006)
*Simi D Kuniyilh,Rita Machacy*

Main category: cs.LG

TL;DR: 本文系统综述了对比学习中的后门攻击，分析了威胁模型、攻击方法、目标领域及防御策略，并指出其安全挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 对比学习虽在表示学习中表现优异，但易受后门和数据投毒攻击，亟需系统性安全评估。

Method: 对对比学习中的后门攻击进行全面综述，分类整理威胁模型、攻击方法、应用场景与防御机制。

Result: 揭示了对比学习特有的脆弱性，总结了当前攻击与防御技术的进展，明确了研究空白。

Conclusion: 为工业与分布式环境中对比学习系统的安全部署提供了关键见解，并指明未来研究方向。

Abstract: Contrastive learning has become a leading self- supervised approach to representation learning across domains, including vision, multimodal settings, graphs, and federated learning. However, recent studies have shown that contrastive learning is susceptible to backdoor and data poisoning attacks. In these attacks, adversaries can manipulate pretraining data or model updates to insert hidden malicious behavior. This paper offers a thorough and comparative review of backdoor attacks in contrastive learning. It analyzes threat models, attack methods, target domains, and available defenses. We summarize recent advancements in this area, underline the specific vulnerabilities inherent to contrastive learning, and discuss the challenges and future research directions. Our findings have significant implications for the secure deployment of systems in industrial and distributed environments.

</details>


### [94] [Combating Spurious Correlations in Graph Interpretability via Self-Reflection](https://arxiv.org/abs/2601.11021)
*Kecheng Cai,Chenyang Xu,Chao Peng*

Main category: cs.LG

TL;DR: 本文提出一种自省框架，通过迭代反馈提升图学习模型在强虚假相关性数据集（如Spurious-Motif）上的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有图可解释方法在Spurious-Motif基准上表现差，因模型难以区分真实结构与虚假相关性。

Method: 引入自省框架，将节点/边重要性评分反馈回原模型进行二次评估，并基于此提出微调训练方法。

Result: 该框架能有效提升模型在具有强虚假相关性数据上的解释准确性。

Conclusion: 自省机制可迁移至图学习领域，为提升可解释性提供新思路，且微调方法进一步增强效果。

Abstract: Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.

</details>


### [95] [Matching High-Dimensional Geometric Quantiles for Test-Time Adaptation of Transformers and Convolutional Networks Alike](https://arxiv.org/abs/2601.11022)
*Sravan Danda,Aditya Challa,Shlok Mehendale,Snehanshu Saha*

Main category: cs.LG

TL;DR: 提出一种与架构无关的测试时自适应方法，通过引入适配器网络和分位数损失来校正分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法高度依赖模型架构，难以推广到通用架构，亟需一种架构无关的解决方案。

Method: 引入一个预处理输入图像的适配器网络，使用分位数损失训练，通过匹配高维几何分位数来校正分布偏移，并理论证明其最优性。

Result: 在CIFAR10-C、CIFAR100-C和TinyImageNet-C上验证，适用于卷积网络和Transformer架构，性能优越。

Conclusion: 所提方法实现了架构无关的测试时自适应，理论可靠且实验有效，为通用模型部署提供了新思路。

Abstract: Test-time adaptation (TTA) refers to adapting a classifier for the test data when the probability distribution of the test data slightly differs from that of the training data of the model. To the best of our knowledge, most of the existing TTA approaches modify the weights of the classifier relying heavily on the architecture. It is unclear as to how these approaches are extendable to generic architectures. In this article, we propose an architecture-agnostic approach to TTA by adding an adapter network pre-processing the input images suitable to the classifier. This adapter is trained using the proposed quantile loss. Unlike existing approaches, we correct for the distribution shift by matching high-dimensional geometric quantiles. We prove theoretically that under suitable conditions minimizing quantile loss can learn the optimal adapter. We validate our approach on CIFAR10-C, CIFAR100-C and TinyImageNet-C by training both classic convolutional and transformer networks on CIFAR10, CIFAR100 and TinyImageNet datasets.

</details>


### [96] [AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2601.11028)
*Xinru Wen,Weizhong Lin,zi liu,Xuan Xiao*

Main category: cs.LG

TL;DR: AVP-Pro是一种两阶段预测框架，结合自适应特征融合与对比学习，显著提升抗病毒肽的识别准确率和亚型分类能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉肽序列复杂依赖关系和区分高相似样本方面存在局限，影响抗病毒肽的精准识别。

Method: 构建包含10种描述符的全景特征空间，采用基于自注意力与自适应门控的层次融合架构整合CNN与BiLSTM特征，并引入OHEM驱动的对比学习（增强BLOSUM62）以强化判别力。

Result: 第一阶段识别准确率0.9531、MCC 0.9064，优于SOTA；第二阶段在小样本下实现6种病毒家族和8种特定病毒的准确分类。

Conclusion: AVP-Pro提供高效、可解释的抗病毒药物高通量筛选工具，并已发布用户友好型Web平台。

Abstract: The accurate identification of antiviral peptides (AVPs) is crucial for novel drug development. However, existing methods still have limitations in capturing complex sequence dependencies and distinguishing confusing samples with high similarity. To address these challenges, we propose AVP-Pro, a novel two-stage predictive framework that integrates adaptive feature fusion and contrastive learning. To comprehensively capture the physicochemical properties and deep-seated patterns of peptide sequences, we constructed a panoramic feature space encompassing 10 distinct descriptors and designed a hierarchical fusion architecture. This architecture integrates self-attention and adaptive gating mechanisms to dynamically modulate the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Targeting the blurred decision boundary caused by the high similarity between positive and negative sample sequences, we adopted an Online Hard Example Mining (OHEM)-driven contrastive learning strategy enhanced by BLOSUM62. This approach significantly sharpened the model's discriminative power. Model evaluation results show that in the first stage of general AVP identification, the model achieved an accuracy of 0.9531 and an MCC of 0.9064, outperforming existing state-of-the-art (SOTA) methods. In the second stage of functional subtype prediction, combined with a transfer learning strategy, the model realized accurate classification of 6 viral families and 8 specific viruses under small-sample conditions. AVP-Pro provides a powerful and interpretable new tool for the high-throughput screening of antiviral drugs. To further enhance accessibility for users, we have developed a user-friendly web interface, which is available at https://wwwy1031-avp-pro.hf.space.

</details>


### [97] [Self-Augmented Mixture-of-Experts for QoS Prediction](https://arxiv.org/abs/2601.11036)
*Kecheng Cai,Chao Peng,Chenyang Xu,Xia Chen*

Main category: cs.LG

TL;DR: 提出一种自增强的混合专家模型，通过迭代预测和反馈提升稀疏场景下的服务质量预测性能。


<details>
  <summary>Details</summary>
Motivation: 服务质量预测面临用户-服务交互数据稀疏的挑战，传统方法难以充分利用有限观测数据。

Method: 设计自增强混合专家模型，通过部分遮蔽预测值并反馈给模型进行多轮迭代优化，实现专家间协作与信息互补。

Result: 在基准数据集上表现优于现有基线方法，预测精度显著提升。

Conclusion: 自增强策略能有效缓解数据稀疏问题，迭代反馈机制与混合专家架构天然契合，为QoS预测提供了新思路。

Abstract: Quality of Service (QoS) prediction is one of the most fundamental problems in service computing and personalized recommendation. In the problem, there is a set of users and services, each associated with a set of descriptive features. Interactions between users and services produce feedback values, typically represented as numerical QoS metrics such as response time or availability. Given the observed feedback for a subset of user-service pairs, the goal is to predict the QoS values for the remaining pairs.
  A key challenge in QoS prediction is the inherent sparsity of user-service interactions, as only a small subset of feedback values is typically observed. To address this, we propose a self-augmented strategy that leverages a model's own predictions for iterative refinement. In particular, we partially mask the predicted values and feed them back into the model to predict again. Building on this idea, we design a self-augmented mixture-of-experts model, where multiple expert networks iteratively and collaboratively estimate QoS values. We find that the iterative augmentation process naturally aligns with the MoE architecture by enabling inter-expert communication: in the second round, each expert receives the first-round predictions and refines its output accordingly. Experiments on benchmark datasets show that our method outperforms existing baselines and achieves competitive results.

</details>


### [98] [OpFML: Pipeline for ML-based Operational Forecasting](https://arxiv.org/abs/2601.11046)
*Shahbaz Alvi,Giusy Fedele,Gabriele Accarino,Italo Epicoco,Ilenia Manco,Pasquale Schiano*

Main category: cs.LG

TL;DR: OpFML是一个可配置的机器学习管道，用于气候与地球科学中的周期性预测，如火灾危险指数预报。


<details>
  <summary>Details</summary>
Motivation: 传统方法常高估野火风险，亟需更精确的机器学习方法进行火灾危险评估。

Method: 提出OpFML，一个可配置且适应性强的机器学习预测管道，应用于每日火灾危险指数预报。

Result: 成功演示了OpFML在火灾危险指数预测中的能力，并展示了其多种功能。

Conclusion: OpFML为气候科学中的机器学习周期性预测提供了一个灵活、实用的解决方案。

Abstract: Machine learning is finding its application in a multitude of areas in science and research, and Climate and Earth Sciences is no exception to this trend. Operational forecasting systems based on data-driven approaches and machine learning methods deploy models for periodic forecasting. Wildfire danger assessment using machine learning has garnered significant interest in the last decade, as conventional methods often overestimate the risk of wildfires. In this work, we present the code OpFML: Operational Forecasting with Machine Learning. OpFML is a configurable and adaptable pipeline that can be utilized to serve a machine learning model for periodic forecasting. We further demonstrate the capabilities of the pipeline through its application to daily Fire Danger Index forecasting and outline its various features.

</details>


### [99] [Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs](https://arxiv.org/abs/2601.11061)
*Lecheng Yan,Ruizhe Li,Guanhua Chen,Qing Li,Jiahui Geng,Wenxi Li,Vincent Wang,Chris Lee*

Main category: cs.LG

TL;DR: 研究发现RLVR中虚假奖励导致模型通过记忆而非推理获取答案，揭示了困惑度悖论与Anchor-Adapter机制。


<details>
  <summary>Details</summary>
Motivation: 尽管虚假奖励能提升模型表现，但其机制不明，亟需解释模型如何绕过推理依赖记忆。

Method: 采用Path Patching、Logit Lens、JSD分析和神经微分方程，定位Anchor-Adapter电路。

Result: 发现中层Functional Anchor触发记忆检索，后层Structural Adapters适配捷径信号，且可通过调控MLP键实现因果干预。

Conclusion: 该研究揭示了RLVR中记忆捷径的神经机制，为检测与缓解数据污染提供了可操作路径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a "Perplexity Paradox": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.

</details>


### [100] [Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud](https://arxiv.org/abs/2601.11073)
*Rongkun Cui,Nana Zhang,Kun Zhu,Qi Zhang*

Main category: cs.LG

TL;DR: 提出HIMVH模型，借鉴海马体机制提升金融欺诈检测性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线金融欺诈危害用户信任与社会公平，现有GNN方法难以应对欺诈伪装和长尾数据问题。

Method: HIMVH模型包含跨视图不一致性感知模块和新颖性感知超图学习模块，分别模拟海马体的场景冲突监测与CA1区的新颖性检测机制。

Result: 在6个真实数据集上，HIMVH平均AUC提升6.42%，F1提升9.74%，AP提升39.14%，优于15个SOTA模型。

Conclusion: 生物启发的多视图超图学习有效解决金融欺诈检测中的伪装与长尾挑战，具有实际应用价值。

Abstract: Online financial services constitute an essential component of contemporary web ecosystems, yet their openness introduces substantial exposure to fraud that harms vulnerable users and weakens trust in digital finance. Such threats have become a significant web harm that erodes societal fairness and affects the well being of online communities. However, existing detection methods based on graph neural networks (GNNs) struggle with two persistent challenges: (1) fraud camouflage, where malicious transactions mimic benign behaviors to evade detection, and (2) long-tailed data distributions, which obscure rare but critical fraudulent cases. To fill these gaps, we propose HIMVH, a Hippocampus-Inspired Multi-View Hypergraph learning model for web finance fraud detection. Specifically, drawing inspiration from the scene conflict monitoring role of the hippocampus, we design a cross-view inconsistency perception module that captures subtle discrepancies and behavioral heterogeneity across multiple transaction views. This module enables the model to identify subtle cross-view conflicts for detecting online camouflaged fraudulent behaviors. Furthermore, inspired by the match-mismatch novelty detection mechanism of the CA1 region, we introduce a novelty-aware hypergraph learning module that measures feature deviations from neighborhood expectations and adaptively reweights messages, thereby enhancing sensitivity to online rare fraud patterns in the long-tailed settings. Extensive experiments on six web-based financial fraud datasets demonstrate that HIMVH achieves 6.42\% improvement in AUC, 9.74\% in F1 and 39.14\% in AP on average over 15 SOTA models.

</details>


### [101] [Soft Bayesian Context Tree Models for Real-Valued Time Series](https://arxiv.org/abs/2601.11079)
*Shota Saito,Yuta Nakahara,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 提出一种用于实值时间序列的软贝叶斯上下文树模型（Soft-BCT），通过概率分割提升性能


<details>
  <summary>Details</summary>
Motivation: 传统BCT模型使用硬分割处理实值时间序列，缺乏灵活性，难以建模复杂分布

Method: 引入软分割机制，基于变分推断设计学习算法

Result: 在多个真实数据集上，Soft-BCT性能与传统BCT相当或更优

Conclusion: Soft-BCT通过概率化上下文分割有效提升了实值时间序列建模能力

Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.

</details>


### [102] [Differentially Private Subspace Fine-Tuning for Large Language Models](https://arxiv.org/abs/2601.11113)
*Lele Zheng,Xiang Wang,Tao Zhang,Yang Cao,Ke Cheng,Yulong Shen*

Main category: cs.LG

TL;DR: DP-SFT通过在低维子空间注入噪声，在保证差分隐私的同时显著提升微调性能。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私微调在高维参数空间注入噪声导致性能下降和训练不稳定，亟需更高效的隐私保护方法。

Method: 提出两阶段子空间微调方法：第一阶段通过主梯度方向识别任务相关低维子空间；第二阶段仅在该子空间内注入噪声并投影回原参数空间更新模型。

Result: 在多个数据集上，DP-SFT在严格隐私约束下显著提升准确率、稳定性和收敛速度，优于基线方法。

Conclusion: 在低维任务子空间中应用差分隐私是高效保护模型隐私并维持性能的有效策略。

Abstract: Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.

</details>


### [103] [Optimized Algorithms for Text Clustering with LLM-Generated Constraints](https://arxiv.org/abs/2601.11118)
*Chaoqi Jia,Weihong Wu,Longkun Guo,Zhigang Lu,Chao Chen,Kok-Leong Ong*

Main category: cs.LG

TL;DR: 提出一种基于LLM的约束生成方法，通过生成约束集而非成对约束，显著减少LLM调用次数并提升聚类效率与精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于成对约束的LLM聚类方法资源消耗大，亟需高效且准确的约束生成策略。

Method: 引入约束集生成机制，结合置信阈值与惩罚机制的约束聚类算法，优化LLM生成约束的质量与效率。

Result: 在五个文本数据集上，聚类精度与SOTA相当，LLM查询次数减少20倍以上。

Conclusion: 该方法在保持高聚类准确率的同时，大幅降低计算成本，为LLM驱动的聚类提供了高效新范式。

Abstract: Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.

</details>


### [104] [Shape-morphing programming of soft materials on complex geometries via neural operator](https://arxiv.org/abs/2601.11126)
*Lu Chen,Gengxiang Chen,Xu Liu,Jingyan Su,Xuhao Lyu,Lihui Wang,Yingguang Li*

Main category: cs.LG

TL;DR: 提出S2NO神经算子，实现复杂几何体上高精度形状形变预测与材料分布优化


<details>
  <summary>Details</summary>
Motivation: 现有方法在简单几何上有效，但难以实现复杂几何（如植入物、气动形变）的精准形变设计

Method: 结合拉普拉斯本征函数编码与空间卷积的S2NO神经算子，联合进化算法进行voxel级材料分布优化

Result: 在不规则边界、多孔和薄壁结构上实现高保真形变预测与超分辨率材料设计

Conclusion: S2NO显著提升复杂形状形变编程的效率与能力，拓展了软材料形变设计的多样性与复杂性

Abstract: Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.

</details>


### [105] [FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling](https://arxiv.org/abs/2601.11134)
*Sultan Amed,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: 提出一种联邦生存学习框架FSL-BDP，在保护隐私的同时准确预测违约时间，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统信用风险模型因中央训练违反数据隐私法规（如GDPR），且二元分类忽略违约时间信息，无法区分早/晚违约的损失差异。

Method: 采用联邦学习框架结合贝叶斯差分隐私（BDP），建模违约时间轨迹，无需集中敏感数据，实现跨机构联合学习。

Result: 在LendingClub、SBA、Bondora三个真实数据集上，FSL-BDP在联邦环境下比经典差分隐私提升7.0%，接近非隐私模型性能，并在多数机构中表现更优。

Conclusion: 隐私机制选择应基于实际部署架构（如联邦环境）评估，而非传统中央基准，为受监管的多机构系统提供实用指导。

Abstract: Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\% vs +1.4\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.

</details>


### [106] [Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction](https://arxiv.org/abs/2601.11135)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: CaMol通过因果推断框架提升少样本分子属性预测的准确性与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的方法难以利用功能基团的先验知识和识别与属性直接相关的亚结构

Method: 提出CaMol框架，包含上下文图编码化学知识、可学习原子掩码分离因果亚结构、分布干预者应用后门调整以解耦因果效应

Result: 在多个分子数据集上显著提升少样本预测精度与样本效率，发现的因果亚结构与化学知识高度一致

Conclusion: CaMol通过因果视角有效提升少样本分子预测的性能与可解释性，具备良好泛化能力

Abstract: Molecular property prediction is becoming one of the major applications of graph learning in Web-based services, e.g., online protein structure prediction and drug discovery. A key challenge arises in few-shot scenarios, where only a few labeled molecules are available for predicting unseen properties. Recently, several studies have used in-context learning to capture relationships among molecules and properties, but they face two limitations in: (1) exploiting prior knowledge of functional groups that are causally linked to properties and (2) identifying key substructures directly correlated with properties. We propose CaMol, a context-aware graph causality inference framework, to address these challenges by using a causal inference perspective, assuming that each molecule consists of a latent causal structure that determines a specific property. First, we introduce a context graph that encodes chemical knowledge by linking functional groups, molecules, and properties to guide the discovery of causal substructures. Second, we propose a learnable atom masking strategy to disentangle causal substructures from confounding ones. Third, we introduce a distribution intervener that applies backdoor adjustment by combining causal substructures with chemically grounded confounders, disentangling causal effects from real-world chemical variations. Experiments on diverse molecular datasets showed that CaMol achieved superior accuracy and sample efficiency in few-shot tasks, showing its generalizability to unseen properties. Also, the discovered causal substructures were strongly aligned with chemical knowledge about functional groups, supporting the model interpretability.

</details>


### [107] [Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines](https://arxiv.org/abs/2601.11154)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 比较监督分类与无监督自编码器在直升机发动机预测性维护中的效果，发现无监督方法在故障数据稀缺时更具实用性。


<details>
  <summary>Details</summary>
Motivation: 直升机发动机突发故障带来安全与经济风险，亟需高效预测性维护方法。

Method: 采用监督分类与基于自编码器的无监督异常检测，基于真实发动机遥测数据进行评估。

Result: 监督方法在有标签数据时性能优越，自编码器无需故障标签即可有效检测异常，适合数据不完整场景。

Conclusion: 无监督学习在航空领域故障早期检测中具备实际部署潜力，尤其适用于故障样本稀缺的情况。

Abstract: Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.

</details>


### [108] [Theoretically and Practically Efficient Resistance Distance Computation on Large Graphs](https://arxiv.org/abs/2601.11159)
*Yichun Yang,Longlong Lin,Rong-Hua Li,Meihao Liao,Guoren Wang*

Main category: cs.LG

TL;DR: 提出两种基于Lanczos方法的高效算法，显著提升电阻距离计算速度，超越现有全局与局部方法。


<details>
  <summary>Details</summary>
Motivation: 现有电阻距离算法在图拉普拉斯矩阵条件数κ较大时收敛慢，缺乏高效计算方法。

Method: 提出Lanczos Iteration（全局，时间复杂度$	ilde{O}(rac{m}{	ext{κ}})$）和Lanczos Push（局部，$	ilde{O}(κ^{2.75})$），降低对κ的依赖。

Result: Lanczos Iteration比现有全局方法快$rac{1}{	ext{κ}}$，Lanczos Push比现有局部方法快$κ^{0.25}$，实验在8个真实数据集上验证了高效性和准确性。

Conclusion: 所提算法大幅提升了电阻距离计算效率，适用于大规模图分析任务。

Abstract: The computation of resistance distance is pivotal in a wide range of graph analysis applications, including graph clustering, link prediction, and graph neural networks. Despite its foundational importance, efficient algorithms for computing resistance distances on large graphs are still lacking. Existing state-of-the-art (SOTA) methods, including power iteration-based algorithms and random walk-based local approaches, often struggle with slow convergence rates, particularly when the condition number of the graph Laplacian matrix, denoted by $κ$, is large. To tackle this challenge, we propose two novel and efficient algorithms inspired by the classic Lanczos method: Lanczos Iteration and Lanczos Push, both designed to reduce dependence on $κ$. Among them, Lanczos Iteration is a near-linear time global algorithm, whereas Lanczos Push is a local algorithm with a time complexity independent of the size of the graph. More specifically, we prove that the time complexity of Lanczos Iteration is $\tilde{O}(\sqrtκ m)$ ($m$ is the number of edges of the graph and $\tilde{O}$ means the complexity omitting the $\log$ terms) which achieves a speedup of $\sqrtκ$ compared to previous power iteration-based global methods. For Lanczos Push, we demonstrate that its time complexity is $\tilde{O}(κ^{2.75})$ under certain mild and frequently established assumptions, which represents a significant improvement of $κ^{0.25}$ over the SOTA random walk-based local algorithms. We validate our algorithms through extensive experiments on eight real-world datasets of varying sizes and statistical properties, demonstrating that Lanczos Iteration and Lanczos Push significantly outperform SOTA methods in terms of both efficiency and accuracy.

</details>


### [109] [Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026](https://arxiv.org/abs/2601.11160)
*Claudia Plant,Lena G. M. Bauer,Christian Böhm*

Main category: cs.LG

TL;DR: 本文探讨聚类中抽象与表征的平衡，分析传统与深度聚类方法如何权衡二者，并提出未来方向是自适应平衡以提升性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法如K-means抽象过高、表征过简，难以处理高维复杂数据，亟需更优的抽象与表征平衡机制。

Method: 通过子空间聚类和深度聚类引入多隐空间，利用质心损失和密度损失显式约束抽象，分离聚类相关与无关信息。

Result: 深度聚类方法能在保留丰富表征的同时，通过显式抽象损失实现有效聚类，子空间方法提升信息分离能力。

Conclusion: 未来聚类方法应自适应调节抽象与表征的平衡，借鉴人类大脑的高效聚类机制，提升性能、能效与可解释性。

Abstract: How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.
  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.
  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.

</details>


### [110] [GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling](https://arxiv.org/abs/2601.11161)
*Pascal Schlachter,Bin Yang*

Main category: cs.LG

TL;DR: 提出首个持续源自由通用域适应（continual SF-UniDA）方法GMM-COMET，在多目标域序列中持续适应，显著超越源仅模型。


<details>
  <summary>Details</summary>
Motivation: 现有SF-UniDA方法仅处理单次源到目标的域偏移，无法应对现实中的多阶段、连续域变化场景。

Method: 基于高斯混合模型的伪标签与均值教师框架结合，并引入一致性损失，提升长期适应的稳定性。

Result: GMM-COMET是唯一在所有实验场景中持续优于源仅模型的方法，为持续SF-UniDA提供首个强基线。

Conclusion: 该工作首次系统探索持续SF-UniDA问题，提出有效解决方案，推动更贴近实际的无监督域适应研究。

Abstract: Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the target domain may differ from the source label space. This setting, known as source-free universal domain adaptation (SF-UniDA), has recently gained attention, but all existing approaches only assume a single domain shift from source to target. In this work, we present the first study on continual SF-UniDA, where the model must adapt sequentially to a stream of multiple different unlabeled target domains. Building upon our previous methods for online SF-UniDA, we combine their key ideas by integrating Gaussian mixture model-based pseudo-labeling within a mean teacher framework for improved stability over long adaptation sequences. Additionally, we introduce consistency losses for further robustness. The resulting method GMM-COMET provides a strong first baseline for continual SF-UniDA and is the only approach in our experiments to consistently improve upon the source-only model across all evaluated scenarios. Our code is available at https://github.com/pascalschlachter/GMM-COMET.

</details>


### [111] [LSTM VS. Feed-Forward Autoencoders for Unsupervised Fault Detection in Hydraulic Pumps](https://arxiv.org/abs/2601.11163)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 使用无监督自编码器在仅用健康数据训练的情况下，实现液压泵故障的早期检测


<details>
  <summary>Details</summary>
Motivation: 工业液压泵的非计划故障会导致生产中断和高昂成本，亟需无需故障样本的早期检测方法

Method: 采用两种无监督自编码器：前馈模型分析单个传感器快照，LSTM模型捕捉短时序窗口，均仅用健康数据训练

Result: 在未见故障样本的训练下，两种模型在包含7个标注故障区间的数据集上实现了高可靠性

Conclusion: 无监督自编码器能有效实现仅基于健康数据的液压泵故障早期检测，具有实际应用价值

Abstract: Unplanned failures in industrial hydraulic pumps can halt production and incur substantial costs. We explore two unsupervised autoencoder (AE) schemes for early fault detection: a feed-forward model that analyses individual sensor snapshots and a Long Short-Term Memory (LSTM) model that captures short temporal windows. Both networks are trained only on healthy data drawn from a minute-level log of 52 sensor channels; evaluation uses a separate set that contains seven annotated fault intervals. Despite the absence of fault samples during training, the models achieve high reliability.

</details>


### [112] [TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation](https://arxiv.org/abs/2601.11184)
*Xiangyu Xu,Qingsong Zhong,Jilin Hu*

Main category: cs.LG

TL;DR: 提出一种结构解耦的多尺度时间序列生成框架，在保持层次依赖的同时有效处理多尺度与异构结构问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分应对时间序列的多尺度时序模式和异构成分带来的生成挑战。

Method: 采用多分辨率离散编码与自回归生成，结合双路径VQ-VAE解耦趋势与季节成分，并引入引导重建策略利用粗粒度季节信号指导细粒度重建。

Result: 在六个数据集上优于现有方法，参数更少，长序列生成质量更高。

Conclusion: 该框架有效解决了时间序列生成中的结构复杂性问题，具备高效与高质量生成能力。

Abstract: Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.

</details>


### [113] [FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization](https://arxiv.org/abs/2601.11200)
*Haiyang Xiao,Weiqing Li,Jinyue Guo,Guochao Jiang,Guohua Liu,Yuewei Zhang*

Main category: cs.LG

TL;DR: FAQ通过利用同家族大模型生成高保真校准数据，显著降低量化精度损失，最高提升28.5%


<details>
  <summary>Details</summary>
Motivation: 传统PTQ依赖有限校准数据，难以准确捕捉推理阶段的激活分布，导致量化参数偏差

Method: FAQ框架利用同家族更大模型生成高保真校准数据，结合思维链推理与专家引导的竞争机制筛选最优样本，并进行重归一化

Result: 在Qwen3-8B等多模型系列上，FAQ相比基线降低量化精度损失高达28.5%

Conclusion: FAQ有效提升PTQ的泛化性与准确性，为资源受限设备部署LLM提供新思路

Abstract: Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.

</details>


### [114] [SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients](https://arxiv.org/abs/2601.11219)
*Zhikang Shen,Jianrong Lu,Haiyuan Wan,Jianhai Chen*

Main category: cs.LG

TL;DR: SDFLoRA通过分离全局与本地LoRA模块，解决联邦学习中低秩异构问题，提升隐私保护与个性化性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法因强制统一低秩或对齐异构更新，过度约束客户端语义，限制个性化，并削弱差分隐私下的本地信息保护。

Method: 提出SDFLoRA，将每个客户端适配器分解为全局模块（跨客户端选择性对齐聚合）和本地模块（私有保留），仅在全局模块注入差分隐私噪声。

Result: 在GLUE基准上，SDFLoRA优于现有联邦LoRA方法，实现更好的效用-隐私权衡。

Conclusion: SDFLoRA有效应对低秩异构，兼顾个性化与隐私保护，为联邦LLM适应提供新范式。

Abstract: Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.

</details>


### [115] [Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques](https://arxiv.org/abs/2601.11222)
*Haochen Wu,Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 提出一种仅依赖边界数据的数学人工数据边界神经算子（MAD-BNO），通过基础解合成训练数据，无需数值模拟，高效求解偏微分方程


<details>
  <summary>Details</summary>
Motivation: 传统神经算子依赖全域采样数据，成本高；本工作旨在仅用边界数据实现高精度、低训练成本的求解框架

Method: 利用数学人工数据（MAD）方法，基于已知基本解生成Dirichlet-Neumann边界数据对，训练边界到边界的神经算子，通过边界积分公式重建域内解

Result: 在二维Laplace、Poisson和Helmholtz方程上验证，精度与现有方法相当，训练时间显著减少，支持多种边界条件和源项

Conclusion: MAD-BNO是一种高效、可扩展的边界数据驱动方法，适用于二维和三维复杂几何问题，无需外部测量或数值仿真

Abstract: For linear partial differential equations with known fundamental solutions, this work introduces a novel operator learning framework that relies exclusively on domain boundary data, including solution values and normal derivatives, rather than full-domain sampling. By integrating the previously developed Mathematical Artificial Data (MAD) method, which enforces physical consistency, all training data are synthesized directly from the fundamental solutions of the target problems, resulting in a fully data-driven pipeline without the need for external measurements or numerical simulations. We refer to this approach as the Mathematical Artificial Data Boundary Neural Operator (MAD-BNO), which learns boundary-to-boundary mappings using MAD-generated Dirichlet-Neumann data pairs. Once trained, the interior solution at arbitrary locations can be efficiently recovered through boundary integral formulations, supporting Dirichlet, Neumann, and mixed boundary conditions as well as general source terms. The proposed method is validated on benchmark operator learning tasks for two-dimensional Laplace, Poisson, and Helmholtz equations, where it achieves accuracy comparable to or better than existing neural operator approaches while significantly reducing training time. The framework is naturally extensible to three-dimensional problems and complex geometries.

</details>


### [116] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出PaST框架，通过正交参数更新高效迁移知识操作技能，提升LLMs在新知识上的问答与工具使用能力


<details>
  <summary>Details</summary>
Motivation: 传统SFT更新知识但不提升使用能力，RL虽有效但成本过高，需一种高效且有效的在线适应方法

Method: 利用SFT与RL参数更新正交性，提取领域无关的Skill Vector，轻量SFT后线性注入目标模型

Result: 在SQuAD上超越SOTA基线9.9分，LooGLE提升8.0分，ToolBench平均提升10.3分，跨域迁移性强

Conclusion: PaST实现模块化技能迁移，兼顾效率与效果，为LLMs在线知识更新提供新范式

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [117] [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)
*Lorenzo Tomada,Federico Pichi,Gianluigi Rozza*

Main category: cs.LG

TL;DR: 提出LD-GCN，一种无编码器的图神经网络，用于参数化PDE的高效降阶建模，支持时间外推与零样本预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以融合几何诱导偏差与可解释的潜在行为，忽略动力学特征或空间信息。

Method: LD-GCN通过潜在空间时间步进建模动态，用GNN解码到几何参数化域，无需编码器，支持插值与外推。

Result: 数学上验证了无编码器架构的通用近似性，数值实验成功应用于Navier-Stokes方程的分岔检测等复杂问题。

Conclusion: LD-GCN在保持几何结构的同时实现了高可解释性与强泛化能力，为参数化PDE降阶提供了新范式。

Abstract: Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom

</details>


### [118] [Sample-Near-Optimal Agnostic Boosting with Improved Running Time](https://arxiv.org/abs/2601.11265)
*Arthur da Cunha,Miakel Møller Høgsgaard,Andrea Paudice*

Main category: cs.LG

TL;DR: 提出首个多项式时间的可证近优样本复杂度的无假设提升算法


<details>
  <summary>Details</summary>
Motivation: 现有无假设提升算法虽样本复杂度近优，但运行时间指数级，亟需高效算法

Method: 设计一种多项式时间的无假设提升算法，优化样本复杂度与运行效率的平衡

Result: 实现近最优样本复杂度，且在固定其他参数时运行时间为样本规模的多项式时间

Conclusion: 该算法填补了高效无假设提升方法的空白，推动理论与实践应用进展

Abstract: Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.

</details>


### [119] [Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning](https://arxiv.org/abs/2601.11283)
*Nabil Belacel,Mohamed Rachid Boulassel*

Main category: cs.LG

TL;DR: 利用尿液代谢组学和可解释机器学习构建ADHD客观诊断模型，AUC>0.97，识别出14个关键代谢物。


<details>
  <summary>Details</summary>
Motivation: ADHD缺乏客观诊断工具，亟需基于生物学的精准诊断框架。

Method: 采用Closest Resemblance分类器对52例ADHD和46例对照的靶向代谢组数据进行分析，结合特征选择。

Result: CR模型性能优于随机森林和KNN，AUC>0.97，识别出与多巴胺能神经传递和氨基酸代谢相关的14种代谢物。

Conclusion: 该框架结合代谢组学与可解释机器学习，为ADHD提供可转化的客观生物标志物诊断策略。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.

</details>


### [120] [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)
*Zhihan Yang,Jiaqi Wei,Xiang Zhang,Haoyu Dong,Yiwen Wang,Xiaoke Guo,Pengkun Zhang,Yiwei Xu,Chenyu You*

Main category: cs.LG

TL;DR: FORESTLLM结合决策树与大语言模型，在少样本场景下提升表格数据预测性能，训练时使用LLM设计模型，测试时无需LLM。


<details>
  <summary>Details</summary>
Motivation: 传统树方法在少样本下易过拟合，LLM直接应用忽略表格结构，导致性能不佳。

Method: 提出FORESTLLM框架：1）用LLM设计语义分裂准则，优化树结构；2）用一次性上下文推理稳定叶节点预测。

Result: 在多个少样本分类与回归基准上达到SOTA性能。

Conclusion: FORESTLLM有效融合结构偏置与语义推理，实现高效、可解释、少样本下的表格数据建模。

Abstract: Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.

</details>


### [121] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://arxiv.org/abs/2601.11342)
*Chuanyue Yu,Jiahui Wang,Yuhan Li,Heng Chang,Ge Lan,Qingyun Sun,Jia Li,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 提出SPREAD框架，通过语义保持的去噪策略解决扩散语言模型在检索增强生成中的语义漂移问题，显著提升生成精度。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）在检索增强生成（RAG）中的潜力未被充分探索，因DLM与LLM解码机制不同，导致生成语义漂移。

Method: 提出SPREAD框架，引入查询相关性引导的去噪策略，确保生成过程始终锚定查询语义。

Result: SPREAD有效抑制语义漂移（RSD），显著提升生成答案的精确度。

Conclusion: DLMs结合RAG具有潜力，但需语义保持机制；SPREAD为DLM-RAG提供了有效解决方案。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.

</details>


### [122] [FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting](https://arxiv.org/abs/2601.11350)
*Jaehoon Lee,Seungwoo Lee,Younghwi Kim,Dohee Kim,Sunghyun Sim*

Main category: cs.LG

TL;DR: FEATHer是一个仅用400个参数的轻量级时间序列预测模型，能在边缘设备上实现高精度长期预测。


<details>
  <summary>Details</summary>
Motivation: 工业自动化系统要求模型在内存和延迟受限的边缘设备（如PLC）上运行，传统深度模型参数过多不适用。

Method: FEATHer采用四部分创新：多尺度频域分解、共享密集时间核（无循环或注意力）、频率感知分支门控、稀疏周期核通过周期下采样捕捉季节性。

Result: 在8个基准上取得最优排名，60次第一，平均排名2.05，参数低至400。

Conclusion: FEATHer证明了在严苛边缘硬件上实现可靠长期预测的可行性，为工业实时推理提供实用方案。

Abstract: Time-series forecasting is fundamental in industrial domains like manufacturing and smart factories. As systems evolve toward automation, models must operate on edge devices (e.g., PLCs, microcontrollers) with strict constraints on latency and memory, limiting parameters to a few thousand. Conventional deep architectures are often impractical here. We propose the Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) for accurate long-term forecasting under severe limits. FEATHer introduces: (i) ultra-lightweight multiscale decomposition into frequency pathways; (ii) a shared Dense Temporal Kernel using projection-depthwise convolution-projection without recurrence or attention; (iii) frequency-aware branch gating that adaptively fuses representations based on spectral characteristics; and (iv) a Sparse Period Kernel reconstructing outputs via period-wise downsampling to capture seasonality. FEATHer maintains a compact architecture (as few as 400 parameters) while outperforming baselines. Across eight benchmarks, it achieves the best ranking, recording 60 first-place results with an average rank of 2.05. These results demonstrate that reliable long-range forecasting is achievable on constrained edge hardware, offering a practical direction for industrial real-time inference.

</details>


### [123] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: 使用离线强化学习设计CPU功耗控制器，在不显著影响性能的前提下降低并行应用的能耗。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在实时系统中训练存在环境建模困难、噪音和可靠性问题，因此需要更安全的离线训练方法。

Method: 采用离线强化学习，结合应用无关的性能数据（如心跳）和硬件性能计数器，通过Intel的RAPL动态控制功耗。

Result: 在多种计算密集和内存密集基准上验证，离线训练的代理能显著降低能耗，且性能下降在可接受范围内。

Conclusion: 离线强化学习是实现高效、安全、实时CPU功耗调控的有效途径。

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [124] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: 提出一种基于配对自编码器的潜在空间推断框架，用于处理逆问题中的观测不一致性，显著提升参数估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统逆问题求解方法在面对观测数据缺失、噪声或分布外情况时表现不佳，缺乏对物理模型的一致性保障。

Method: 使用两个分别编码参数空间与观测空间的自编码器，通过学习其潜在空间之间的映射关系，构建低维、信息丰富的代理逆模型，实现数据重建与参数估计的联合优化。

Result: 在医学断层成像与地球物理地震波形反演中，该方法优于配对自编码器和端到端编码器-解码器，尤其在数据不一致情况下表现更优。

Conclusion: 该框架具有通用性，可扩展至多种科学与工程领域的逆问题，为数据不完整场景提供鲁棒且物理一致的解决方案。

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


### [125] [Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.11401)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 提出扩散价值函数DVF，通过图扩散机制改进多智能体强化学习中的信用分配，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有全局或局部价值函数在结构化多智能体系统中信号弱或估计困难，尤其在无限时域下表现不佳。

Method: 引入扩散价值函数（DVF），基于影响图对奖励进行时间折扣与空间衰减的扩散，结合GNN可扩展估计，并构建DA2C算法与LD-GNN_actor。

Result: 在灭火基准和三个分布式任务中，DA2C相比基线平均奖励提升高达11%，表现更优。

Conclusion: DVF有效解决多智能体信用分配问题，为结构化MARL提供可扩展、稳定且高性能的解决方案。

Abstract: Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.

</details>


### [126] [Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families](https://arxiv.org/abs/2601.11428)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 系统性压力测试揭示傅里叶神经算子在分布偏移、分辨率外推和迭代预测中的脆弱性，误差可放大十倍以上。


<details>
  <summary>Details</summary>
Motivation: FNOs在PDE求解中表现优异，但其在分布偏移、长时预测和结构扰动下的鲁棒性尚不明确，亟需系统评估。

Method: 设计五类PDE系统的控制压力测试，包括参数偏移、边界条件变化、分辨率外推与频谱分析、迭代预测，评估1000个训练模型。

Result: 参数或边界条件偏移使误差放大十倍以上；分辨率变化主要影响高频模式误差；输入扰动通常不放大误差，但局部Poisson扰动仍具挑战。

Conclusion: 本研究构建了FNOs的失败模式图谱，为提升算子学习鲁棒性提供可操作指导。

Abstract: Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests--including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts--to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1{,}000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.

</details>


### [127] [Inter-patient ECG Arrhythmia Classification with LGNs and LUTNs](https://arxiv.org/abs/2601.11433)
*Wout Mommen,Lars Keuninckx,Paul Detterer,Achiel Colpaert,Piet Wambacq*

Main category: cs.LG

TL;DR: 提出低功耗的可微逻辑门网络和查找表网络，用于心电图分类，在MIT-BIH数据集上实现94.28%准确率，功耗低至5-7mW。


<details>
  <summary>Details</summary>
Motivation: 现有心电图分类模型计算开销大，难以部署于可穿戴或植入式设备，需开发极低功耗、高效率的模型。

Method: 采用可微逻辑门网络（LGNs）和查找表网络（LUTNs），引入新型预处理方法、基于多路复用器布尔方程的LUT训练法及速率编码技术。

Result: 在四分类任务中准确率达94.28%，κ指数0.683，FLOPs仅为2.89k–6.17k，功耗5–7mW，显著优于现有方法。

Conclusion: LGNs和LUTNs具备在低功耗设备上实现高精度心律失常检测的潜力，尤其适用于未参与训练的患者。

Abstract: Deep Differentiable Logic Gate Networks (LGNs) and Lookup Table Networks (LUTNs) are demonstrated to be suitable for the automatic classification of electrocardiograms (ECGs) using the inter-patient paradigm. The methods are benchmarked using the MIT-BIH arrhythmia data set, achieving up to 94.28% accuracy and a $jκ$ index of 0.683 on a four-class classification problem. Our models use between 2.89k and 6.17k FLOPs, including preprocessing and readout, which is three to six orders of magnitude less compared to SOTA methods. A novel preprocessing method is utilized that attains superior performance compared to existing methods for both the mixed-patient and inter-patient paradigms. In addition, a novel method for training the Lookup Tables (LUTs) in LUTNs is devised that uses the Boolean equation of a multiplexer (MUX). Additionally, rate coding was utilized for the first time in these LGNs and LUTNs, enhancing the performance of LGNs. Furthermore, it is the first time that LGNs and LUTNs have been benchmarked on the MIT-BIH arrhythmia dataset using the inter-patient paradigm. Using an Artix 7 FPGA, between 2000 and 2990 LUTs were needed, and between 5 to 7 mW (i.e. 50 pJ to 70 pJ per inference) was estimated for running these models. The performance in terms of both accuracy and $jκ$-index is significantly higher compared to previous LGN results. These positive results suggest that one can utilize LGNs and LUTNs for the detection of arrhythmias at extremely low power and high speeds in heart implants or wearable devices, even for patients not included in the training set.

</details>


### [128] [GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2601.11440)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Ricardo Vinuesa,Soledad Le Clainche*

Main category: cs.LG

TL;DR: GenDA是一种基于生成式数据同化的框架，利用稀疏传感器数据重建高分辨率城市风场，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 城市风场重建对空气质量、热扩散和行人舒适度至关重要，但稀疏传感器数据下难以准确重构。

Method: 采用多尺度图结构扩散架构，结合无条件分支学习几何感知流先验，条件分支注入观测约束，实现无需重训练的障碍感知重建。

Result: 相比GNN和经典方法，RRMSE降低25-57%，SSIM提升23-33%，在真实城市场景中表现优异。

Conclusion: GenDA为复杂环境中的生成式、几何感知数据同化提供了可扩展解决方案。

Abstract: Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\mathrm{Re}\approx2\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.

</details>


### [129] [When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models](https://arxiv.org/abs/2601.11444)
*Raphaël Razafindralambo,Rémy Sun,Frédéric Precioso,Damien Garreau,Pierre-Alexandre Mattei*

Main category: cs.LG

TL;DR: 尽管集成分数可以改善得分匹配损失和模型似然，但在图像数据集上并未一致提升感知质量（如FID），但在表格数据中发现一种聚合策略更优。


<details>
  <summary>Details</summary>
Motivation: 探索集成方法在无条件基于分数的扩散模型中的应用效果，弥补该领域研究空白。

Method: 在CIFAR-10和FFHQ上测试多种集成策略（如Deep Ensembles、Monte Carlo Dropout），并分析得分估计与图像质量的关系，同时在表格数据上使用随机森林验证。

Result: 集成提升得分匹配与似然，但FID等感知指标无显著改善；表格数据中一种策略表现最佳。

Conclusion: 集成对扩散模型的理论和实践有重要启示，尤其在模型组合（如引导）方面具有潜在价值。

Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).

</details>


### [130] [Low-Rank Key Value Attention](https://arxiv.org/abs/2601.11471)
*James O'Neill,Robert Clancy,Mariia Matskevichus,Fergal Reid*

Main category: cs.LG

TL;DR: LRKV通过低秩适配减少KV缓存内存占用，同时保持注意力头多样性，在大规模预训练中显著降低计算开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer预训练受限于内存和计算资源，KV缓存成为主要瓶颈，现有方法如MQA/GQA或MLA在压缩KV时牺牲了注意力多样性或效果。

Method: 提出LRKV，通过共享全秩KV投影并添加头特异性低秩残差，在保持token级分辨率的同时实现KV缓存压缩。

Result: 在2.5B模型上，LRKV以一半KV缓存实现优于标准注意力、MQA/GQA和MLA的性能，训练计算节省20-25%，且保留更多功能头多样性。

Conclusion: LRKV是一种实用高效的注意力机制，能在内存与计算受限条件下显著提升Transformer预训练效率。

Abstract: Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.

</details>


### [131] [Extractive summarization on a CMOS Ising machine](https://arxiv.org/abs/2601.11491)
*Ziqing Zeng,Abhimanyu Kumar,Chris H. Kim,Ulya R. Karpuzcu,Sachin S. Sapatnekar*

Main category: cs.LG

TL;DR: 本文提出在低功耗CMOS耦合振荡器Ising机器上实现提取式摘要，实现高速低能耗的边缘部署。


<details>
  <summary>Details</summary>
Motivation: 传统神经模型提取式摘要能耗高，不适合资源受限的实时场景，亟需低功耗硬件解决方案。

Method: 设计硬件感知的Ising模型，引入随机舍入与迭代优化、问题分解策略，适配COBI硬件的整数耦合与低精度限制。

Result: 在CNN/DailyMail数据集上，COBI实现3-4.5倍速度提升，能耗降低两个数量级，摘要质量与软件方法相当。

Conclusion: CMOS Ising求解器在边缘设备上实现高效、低功耗实时摘要具有显著潜力。

Abstract: Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports integer-valued, all-to-all spin couplings. We first propose a hardware-aware Ising formulation that reduces the scale imbalance between local fields and coupling terms, thereby improving robustness to coefficient quantization: this method can be applied to any problem formulation that requires k of n variables to be chosen. We then develop a complete ES pipeline including (i) stochastic rounding and iterative refinement to compensate for precision loss, and (ii) a decomposition strategy that partitions a large ES problem into smaller Ising subproblems that can be efficiently solved on COBI and later combined. Experimental results on the CNN/DailyMail dataset show that our pipeline can produce high-quality summaries using only integer-coupled Ising hardware with limited precision. COBI achieves 3-4.5x runtime speedups compared to a brute-force method, which is comparable to software Tabu search, and two to three orders of magnitude reductions in energy, while maintaining competitive summary quality. These results highlight the potential of deploying CMOS Ising solvers for real-time, low-energy text summarization on edge devices.

</details>


### [132] [QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid](https://arxiv.org/abs/2601.11500)
*Hoang M. Ngo,Tre' R. Jeter,Jung Taek Seo,My T. Thai*

Main category: cs.LG

TL;DR: 提出QUPID和R-QUPID两种量子机器学习模型，显著提升智能电网异常检测的性能与鲁棒性，且支持差分隐私与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型难以应对智能电网的高维复杂性与对抗攻击，亟需更鲁棒的解决方案。

Method: 提出分区量子神经网络（PQNN）架构QUPID，并扩展为支持差分隐私的R-QUPID，通过计算分区提升可扩展性。

Result: 在多种场景下，QUPID和R-QUPID在检测精度与对抗鲁棒性上均超越传统ML模型，且在引入差分隐私后仍保持高性能。

Conclusion: 量子机器学习为智能电网异常检测提供了高效、鲁棒且可扩展的新范式，具有实际部署潜力。

Abstract: Smart grid infrastructures have revolutionized energy distribution, but their day-to-day operations require robust anomaly detection methods to counter risks associated with cyber-physical threats and system faults potentially caused by natural disasters, equipment malfunctions, and cyber attacks. Conventional machine learning (ML) models are effective in several domains, yet they struggle to represent the complexities observed in smart grid systems. Furthermore, traditional ML models are highly susceptible to adversarial manipulations, making them increasingly unreliable for real-world deployment. Quantum ML (QML) provides a unique advantage, utilizing quantum-enhanced feature representations to model the intricacies of the high-dimensional nature of smart grid systems while demonstrating greater resilience to adversarial manipulation. In this work, we propose QUPID, a partitioned quantum neural network (PQNN) that outperforms traditional state-of-the-art ML models in anomaly detection. We extend our model to R-QUPID that even maintains its performance when including differential privacy (DP) for enhanced robustness. Moreover, our partitioning framework addresses a significant scalability problem in QML by efficiently distributing computational workloads, making quantum-enhanced anomaly detection practical in large-scale smart grid environments. Our experimental results across various scenarios exemplifies the efficacy of QUPID and R-QUPID to significantly improve anomaly detection capabilities and robustness compared to traditional ML approaches.

</details>


### [133] [MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management](https://arxiv.org/abs/2601.11505)
*Miriam K. Wolff,Peter Calhoun,Eleonora Maria Aiello,Yao Qin,Sam F. Royston*

Main category: cs.LG

TL;DR: 构建了一个名为MetaboNet的统一T1D数据集，整合多个公开数据源，支持更广泛可推广的算法开发。


<details>
  <summary>Details</summary>
Motivation: 现有T1D管理数据集结构碎片化、访问困难，阻碍了算法的整合与比较。

Method: 整合多个公开T1D数据集，筛选包含CGM和胰岛素泵记录的数据，并保留碳水摄入和运动等辅助信息，形成标准化的MetaboNet数据集。

Result: MetaboNet包含3135名受试者、1228患者年数据，提供公开可下载子集和需DUA申请的受限子集，并提供自动转换工具。

Conclusion: MetaboNet为T1D算法研究提供了更大规模、更全面、标准化的数据资源，提升了算法的泛化能力。

Abstract: Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.

</details>


### [134] [Building Production-Ready Probes For Gemini](https://arxiv.org/abs/2601.11516)
*János Kramár,Joshua Engels,Zheng Wang,Bilal Chughtai,Rohin Shah,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 提出新的激活探针架构以应对长上下文分布偏移，提升对恶意使用的检测能力，并成功部署于Gemini模型。


<details>
  <summary>Details</summary>
Motivation: 现有探针在生产环境中的分布偏移（如短上下文到长上下文）下泛化能力差，亟需更强的滥用缓解方法。

Method: 设计新型探针架构，结合多样分布训练与提示分类器，评估其在多轮对话、静态越狱和自适应红队攻击下的鲁棒性，并探索AlphaEvolve自动化优化。

Result: 新架构显著提升长上下文泛化能力，结合提示分类器实现高精度低成本检测，已成功部署于Gemini，AlphaEvolve初步验证自动化安全研究可行性。

Conclusion: 滥用缓解需架构创新与多样化训练协同，探针+提示分类器是高效方案，自动化工具正推动AI安全研究进步。

Abstract: Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.
  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.
  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.

</details>
