<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [cs.LG](#cs.LG) [Total: 183]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 首次系统评估视觉语言模型在航拍图像中的开放词汇目标检测性能，发现语义混淆是主要瓶颈，性能远低于自然图像。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测模型在自然图像上表现良好，但在航拍图像中的迁移能力未被研究，亟需系统评估与突破。

Method: 在LAE-80C航拍数据集上对5种先进OVD模型进行严格零样本评估，设计Global、Oracle和Single-Category三种推理模式以分离语义混淆与视觉定位问题。

Result: 最佳模型OWLv2仅达27.6% F1得分，假阳性率达69%；词汇量从80减至3.2类时性能提升15倍；提示工程无效，不同数据集性能差异巨大（F1: 0.53至0.12）。

Conclusion: 航拍OVD面临严重域迁移挑战，语义混淆是核心瓶颈，需发展领域自适应方法而非依赖通用视觉语言模型。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [2] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: 提出一个针对科学图表的VQA基准，解决图表标记与底层数据非一一对应的问题


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集主要关注真实图像或简单图表，缺乏对科学图表中复杂数据变换的建模，且通常假设标记与数据一一对应，忽略了实际图表是数据变换结果的特性

Method: 通过生成基于真实底层数据的合成直方图，设计需访问原始数据才能准确回答的问题，并收集人类和大型推理模型的响应，构建包含图表、数据、分布参数及标记边界框的开源数据集

Result: 发布了首个专门针对科学图表的VQA数据集，包含完整数据链路，支持更深入的推理评估

Conclusion: 科学图表的VQA需要新的基准，以推动模型理解数据变换而非仅识别视觉标记，本工作为此提供基础资源和研究方向

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

</details>


### [3] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: VLMs在3D空间推理上表现差，即使在简单的相对相机位姿估计任务上也远低于几何基线和人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在2D感知上表现良好，但对3D空间结构理解有限，亟需评估其在真实3D场景中的空间推理能力。

Method: 提出VRRPI-Bench基准（基于俯视视频的自然运动注释）和VRRPI-Diag诊断基准，用于评估模型对相对相机位姿的推理能力。

Result: 主流VLMs（如GPT-5）在RCPE任务上性能远低于几何基线（0.64 vs 0.97）和人类（0.92），且多帧空间推理不一致（最高仅59.7%）。

Conclusion: VLMs难以有效接地于3D空间与多视角推理，当前模型仍依赖2D启发式，缺乏真正的三维理解能力。

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [4] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: 本文从几何视角重新审视Vision Transformers中位置编码的作用，揭示其作为几何先验对空间结构的因果影响。


<details>
  <summary>Details</summary>
Motivation: 位置编码在ViT中常被视为简单token索引，但其真实几何作用未被充分理解。

Method: 引入token级诊断方法，测量多视角几何一致性与位置编码的关系，并在14个基础ViT模型上进行实验。

Result: 证明位置编码是控制ViT表征空间结构的因果机制，显著影响多视角几何与空间推理能力。

Conclusion: 位置编码并非简单索引，而是关键几何先验，对ViT的空间理解具有根本性作用。

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

</details>


### [5] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 单层VQ-VAE在匹配表示预算并缓解码本坍缩后，重建保真度可与分层VQ-VAE相当，挑战了分层结构更优的假设。


<details>
  <summary>Details</summary>
Motivation: 分层VQ-VAE被认为因分解全局与局部特征而重建性能更优，但其高层信息完全依赖低层，是否真能提升重建精度尚未被独立验证。

Method: 对比匹配表示容量的单层与双层VQ-VAE，在高分辨率ImageNet图像上评估重建质量，并引入数据初始化、周期性重置和超参调优缓解码本坍缩。

Result: 通过轻量干预显著减少码本坍缩后，单层VQ-VAE在重建保真度上达到与分层模型相当的水平。

Conclusion: 分层结构并非提升重建精度的必要条件，合理设计的单层VQ-VAE足以实现同等高质量重建。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [6] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: VMonarch通过Monarch矩阵实现视频扩散变换器的高效稀疏注意力，显著降低计算复杂度并提升速度。


<details>
  <summary>Details</summary>
Motivation: 视频扩散变换器（DiTs）中注意力机制的二次复杂度限制了其上下文扩展能力，亟需更高效的稀疏注意力方法。

Method: 提出VMonarch，采用时空Monarch矩阵分解、重计算策略和融合在线熵算法的FlashAttention，以高效处理动态稀疏注意力模式。

Result: 在VBench上生成质量媲美全注意力，减少17.5倍FLOPs，长视频注意力计算加速超5倍，优于现有稀疏注意力方法。

Conclusion: VMonarch有效突破视频DiT的注意力瓶颈，为长视频生成提供高效、可扩展的解决方案。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [7] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: C2R是一种从粗略3D模拟生成逼真城市人群视频的生成渲染框架，通过文本提示控制外观与动态，无需大量配对数据。


<details>
  <summary>Details</summary>
Motivation: 传统渲染管道依赖复杂资产和大量计算资源，难以在动态人群中实现可扩展且逼真的效果。

Method: 采用粗略3D渲染控制场景布局与运动，结合神经渲染器生成逼真视觉效果，并通过两阶段混合CG-真实数据训练策略学习跨域共享的时空特征。

Result: 系统支持从少量3D输入生成时间一致、可控且逼真的城市视频，且能泛化至多种CG和游戏输入。

Conclusion: C2R实现了低成本高真实感的动态场景生成，为未来虚拟城市与影视制作提供新范式。

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

</details>


### [8] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap是一种无需固定相机配置或几何投影的HD地图构建方法，通过几何感知基础模型实现鲁棒的3D场景理解。


<details>
  <summary>Details</summary>
Motivation: 现有HD地图构建方法依赖标定的多相机系统和2D到BEV的变换，对传感器故障或相机配置变化敏感，难以在真实车队中部署。

Method: 提出FlexMap，采用几何感知基础模型与跨帧注意力隐式编码3D场景，结合时空增强模块和相机感知解码器（含潜在相机标记），无需投影矩阵即可实现视图自适应。

Result: 在多种相机配置下超越现有方法，对缺失视图和传感器变化具有更强鲁棒性，支持更实用的现实部署。

Conclusion: FlexMap通过消除显式几何投影，实现了灵活、鲁棒的HD地图生成，为自动驾驶提供了更实用的解决方案。

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

</details>


### [9] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 提出一种基于CoT提示和ReAct自适应噪声的越狱框架，有效绕过视觉语言模型的安全过滤器。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的输出对提示敏感，安全对齐存在漏洞，亟需系统性攻击方法揭示其风险。

Method: 结合后训练Chain-of-Thought提示构建隐蔽提示，并引入ReAct驱动的自适应噪声机制，根据模型反馈迭代优化图像扰动区域。

Result: 在文本和视觉双重域中显著提升越狱成功率（ASR），同时保持输出自然性。

Conclusion: 该双策略框架有效暴露VLMs安全对齐的脆弱性，为后续防御提供重要参考。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [10] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: 该研究提出了一种概率性无标记多视角运动捕捉系统，能够可靠地估计运动不确定性，无需地面真值设备即可识别不可靠输出。


<details>
  <summary>Details</summary>
Motivation: 临床应用需要运动捕捉系统不仅准确，还要提供可靠的置信区间以评估个体测量的准确性。

Method: 基于变分推断方法，评估概率性无标记多视角运动捕捉系统的校准与可靠性，使用68名参与者数据，对比仪器步态分析和传统标记式运动捕捉，通过预期校准误差（ECE）评估置信区间。

Result: ECE值普遍<0.1，步长和 stride 长度中位误差约16mm和12mm，关节运动误差为1.5–3.8度，预测不确定性与实际误差强相关。

Conclusion: 该概率模型有效量化了认知不确定性，可自主识别不可靠输出，无需同步地面真值设备，具备临床应用潜力。

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

</details>


### [11] [Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework](https://arxiv.org/abs/2601.22451)
*Shiyu Liu,Xinyi Wen,Zhibin Lan,Ante Wang,Jinsong Su*

Main category: cs.CV

TL;DR: 提出一种无语言先验的自验证框架，显著减少大视觉语言模型在图像描述中的对象幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有方法仅粗略缓解语言先验依赖，缺乏对生成过程中幻觉加剧机制的深入分析

Method: 通过无语言先验验证，构建无需训练的自验证框架，在候选描述中验证对象存在性并进行选择或聚合

Result: 在LLaVA-v1.5-7B上CHAIRI指标提升65.6%，超越现有SOTA方法

Conclusion: 通过挖掘LVLM自身潜力，无需额外训练即可有效抑制幻觉，开辟了新路径

Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and attempts to mitigate it through logits calibration. However, they still lack a thorough analysis of the over-reliance. To gain a deeper understanding of over-reliance, we conduct a series of preliminary experiments, indicating that as the generation length increases, LVLMs' over-reliance on language priors leads to inflated probability of hallucinated object tokens, consequently exacerbating object hallucination. To circumvent this issue, we propose Language-Prior-Free Verification to enable LVLMs to faithfully verify the confidence of object existence. Based on this, we propose a novel training-free Self-Validation Framework to counter the over-reliance trap. It first validates objects' existence in sampled candidate captions and further mitigates object hallucination via caption selection or aggregation. Experiment results demonstrate that our framework mitigates object hallucination significantly in image captioning task (e.g., 65.6% improvement on CHAIRI metric with LLaVA-v1.5-7B), surpassing the previous SOTA methods. This result highlights a novel path towards mitigating hallucination by unlocking the inherent potential within LVLMs themselves.

</details>


### [12] [ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction](https://arxiv.org/abs/2601.22455)
*Yudi Zhang,Yeming Geng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出ScribbleSense方法，利用多模态大语言模型和图像生成模型提升涂鸦式3D纹理编辑的准确性和直观性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅支持草图式交互，涂鸦式交互因语义模糊和目标位置不明确而受限。

Method: 结合多模态大语言模型解析涂鸦意图，并利用全局图像提取局部纹理细节以锚定语义位置。

Result: 实验表明该方法在涂鸦式纹理编辑中达到最优性能，有效缓解语义歧义。

Conclusion: ScribbleSense通过多模态融合显著提升了涂鸦交互在3D纹理编辑中的实用性与精准度。

Abstract: Interactive 3D model texture editing presents enhanced opportunities for creating 3D assets, with freehand drawing style offering the most intuitive experience. However, existing methods primarily support sketch-based interactions for outlining, while the utilization of coarse-grained scribble-based interaction remains limited. Furthermore, current methodologies often encounter challenges due to the abstract nature of scribble instructions, which can result in ambiguous editing intentions and unclear target semantic locations. To address these issues, we propose ScribbleSense, an editing method that combines multimodal large language models (MLLMs) and image generation models to effectively resolve these challenges. We leverage the visual capabilities of MLLMs to predict the editing intent behind the scribbles. Once the semantic intent of the scribble is discerned, we employ globally generated images to extract local texture details, thereby anchoring local semantics and alleviating ambiguities concerning the target semantic locations. Experimental results indicate that our method effectively leverages the strengths of MLLMs, achieving state-of-the-art interactive editing performance for scribble-based texture editing.

</details>


### [13] [Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector](https://arxiv.org/abs/2601.22468)
*Wenqiang Zu,Shenghao Xie,Bo Lei,Lei Ma*

Main category: cs.CV

TL;DR: 通过引入表示对齐投影器，在扩散变换器的去噪过程中注入无监督特征表示，显著提升生成图像的语义一致性与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有推理时引导方法未能充分利用无监督视觉表示，且在扩散模型早期去噪阶段存在语义漂移问题，导致生成结果不一致。

Method: 提出表示对齐投影器，在中间采样步骤中注入预测的表示作为语义锚点，无需修改模型架构。

Result: 在ImageNet类条件生成任务中显著降低FID分数（如REPA-XL/2从5.9降至3.3），优于代表性引导，并能与无分类器引导互补提升效果。

Conclusion: 表示引导的扩散采样是一种实用策略，可有效强化语义保持与图像一致性。

Abstract: Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.

</details>


### [14] [Head-Aware Visual Cropping: Enhancing Fine-Grained VQA with Attention-Guided Subimage](https://arxiv.org/abs/2601.22483)
*Junfei Xie,Peng Pan,Xulong Zhang*

Main category: cs.CV

TL;DR: 提出一种无需训练的HAVC方法，通过选择性关注注意力头提升多模态大模型的视觉定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在细粒度视觉问答中因低分辨率输入和噪声注意力聚合导致推理能力受限。

Method: HAVC方法通过OCR任务筛选有效注意力头，结合空间熵和梯度敏感性优化注意力图，生成视觉裁剪指导图以定位关键区域。

Result: 在多个细粒度VQA基准上，HAVC显著优于现有裁剪策略，实现更精准的定位与更强的视觉 grounding。

Conclusion: HAVC是一种简单有效的无需训练方法，可显著提升MLLMs在细粒度任务中的精度。

Abstract: Multimodal Large Language Models (MLLMs) show strong performance in Visual Question Answering (VQA) but remain limited in fine-grained reasoning due to low-resolution inputs and noisy attention aggregation. We propose \textbf{Head Aware Visual Cropping (HAVC)}, a training-free method that improves visual grounding by leveraging a selectively refined subset of attention heads. HAVC first filters heads through an OCR-based diagnostic task, ensuring that only those with genuine grounding ability are retained. At inference, these heads are further refined using spatial entropy for stronger spatial concentration and gradient sensitivity for predictive contribution. The fused signals produce a reliable Visual Cropping Guidance Map, which highlights the most task-relevant region and guides the cropping of a subimage subsequently provided to the MLLM together with the image-question pair. Extensive experiments on multiple fine-grained VQA benchmarks demonstrate that HAVC consistently outperforms state-of-the-art cropping strategies, achieving more precise localization, stronger visual grounding, providing a simple yet effective strategy for enhancing precision in MLLMs.

</details>


### [15] [PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization](https://arxiv.org/abs/2601.22492)
*Duncan McCain,Hossein Kashiani,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 提出PromptMAD，通过视觉-语言对齐和焦点损失实现无监督视觉异常检测，在MVTec-AD上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多类场景下异常样本稀缺、缺陷隐蔽、像素级类别不平衡导致传统方法效果不佳。

Method: 利用CLIP文本提示引导视觉重建，引入Focal损失处理不平衡，结合多尺度卷积、Transformer注意力与扩散细化生成高精度异常图。

Result: 在MVTec-AD上像素级AUC达98.35%，AP达66.54%，性能优于现有方法且效率高。

Conclusion: 跨模态语义引导显著提升细微异常检测能力，为无监督异常检测提供新范式。

Abstract: Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.

</details>


### [16] [MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control](https://arxiv.org/abs/2601.22501)
*Renjie Lu,Xulong Zhang,Xiaoyang Qu,Jianzong Wang,Shangfei Wang*

Main category: cs.CV

TL;DR: MirrorTalk使用条件扩散模型和语义解耦风格编码器，实现个性化说话人脸合成，兼顾唇形同步与风格保留。


<details>
  <summary>Details</summary>
Motivation: 现有方法将说话者风格与语义内容混淆，难以将个人风格迁移到任意语音上。

Method: 提出MirrorTalk框架，结合条件扩散模型与语义解耦风格编码器（SDSE），并通过层次调制策略动态平衡音频与风格特征。

Result: 在唇形同步精度和风格保留方面显著超越现有方法。

Conclusion: MirrorTalk有效解耦风格与语义，实现了高质量个性化说话人脸生成。

Abstract: Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.

</details>


### [17] [DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation](https://arxiv.org/abs/2601.22507)
*Xin Jiang,Jingwen Chen,Yehao Li,Yingwei Pan,Kezhou Chen,Zechao Li,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: DreamVAR 使用视觉自回归模型实现主题驱动图像生成，优于扩散模型


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型在主题驱动图像生成中的潜力尚未充分探索，而扩散模型虽强但存在训练与推理不一致问题

Method: 采用多尺度特征预填充策略，结合强化学习提升语义对齐与主体一致性

Result: DreamVAR 在外观保持方面超越领先的扩散模型

Conclusion: VAR 模型在主题驱动生成中具有显著优势，DreamVAR 为该方向提供了高效新范式

Abstract: Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.

</details>


### [18] [CoVA: Text-Guided Composed Video Retrieval for Audio-Visual Content](https://arxiv.org/abs/2601.22508)
*Gyuwon Han,Young Kyun Jang,Chanho Eom*

Main category: cs.CV

TL;DR: 提出新的音视频组合检索任务CoVA及基准AV-Comp，并设计AVT融合方法提升性能


<details>
  <summary>Details</summary>
Motivation: 现有视频检索任务忽略音频变化，仅关注视觉差异，导致模型无法处理跨模态变化

Method: 构建AV-Comp基准数据集，包含视觉与音频变化的视频对及文本描述；提出AVT组合融合方法，自适应对齐文本查询与最相关模态特征

Result: AVT方法超越传统单模态融合，成为CoVA任务的强大基线

Conclusion: CoVA任务和AV-Comp数据集推动了音视频组合检索的发展，为多模态理解提供新方向

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a target video from a large gallery using a reference video and a textual query specifying visual modifications. However, existing benchmarks consider only visual changes, ignoring videos that differ in audio despite visual similarity. To address this limitation, we introduce Composed retrieval for Video with its Audio CoVA, a new retrieval task that accounts for both visual and auditory variations. To support this, we construct AV-Comp, a benchmark consisting of video pairs with cross-modal changes and corresponding textual queries that describe the differences. We also propose AVT Compositional Fusion (AVT), which integrates video, audio, and text features by selectively aligning the query to the most relevant modality. AVT outperforms traditional unimodal fusion and serves as a strong baseline for CoVA. Examples from the proposed dataset, including both visual and auditory information, are available at https://perceptualai-lab.github.io/CoVA/.

</details>


### [19] [DNA: Uncovering Universal Latent Forgery Knowledge](https://arxiv.org/abs/2601.22515)
*Jingtong Dou,Chuancheng Shi,Yemin Wang,Shiming Guo,Anqi Yi,Wenhua Wu,Li Zhang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出DNA框架，无需微调即可利用预训练模型中的内在判别能力进行伪造检测，性能优越且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的高逼真度使传统伪影检测失效，现有方法依赖资源密集的微调，效率低下。

Method: 提出判别神经锚点（DNA）框架，通过粗到细的特征挖掘，定位对伪造敏感的中间层，并利用三元融合评分与曲率截断策略提取伪造判别单元（FDUs）。

Result: 仅依赖DNA锚点即可在少样本条件下实现超越现有方法的检测性能，且在不同架构和未见过的生成模型上表现出强鲁棒性。

Conclusion: 唤醒预训练模型中的潜伏神经元比端到端微调更有效，为伪造检测提供了新范式。

Abstract: As generative AI achieves hyper-realism, superficial artifact detection has become obsolete. While prevailing methods rely on resource-intensive fine-tuning of black-box backbones, we propose that forgery detection capability is already encoded within pre-trained models rather than requiring end-to-end retraining. To elicit this intrinsic capability, we propose the discriminative neural anchors (DNA) framework, which employs a coarse-to-fine excavation mechanism. First, by analyzing feature decoupling and attention distribution shifts, we pinpoint critical intermediate layers where the focus of the model logically transitions from global semantics to local anomalies. Subsequently, we introduce a triadic fusion scoring metric paired with a curvature-truncation strategy to strip away semantic redundancy, precisely isolating the forgery-discriminative units (FDUs) inherently imprinted with sensitivity to forgery traces. Moreover, we introduce HIFI-Gen, a high-fidelity synthetic benchmark built upon the very latest models, to address the lag in existing datasets. Experiments demonstrate that by solely relying on these anchors, DNA achieves superior detection performance even under few-shot conditions. Furthermore, it exhibits remarkable robustness across diverse architectures and against unseen generative models, validating that waking up latent neurons is more effective than extensive fine-tuning.

</details>


### [20] [Can 3D point cloud data improve automated body condition score prediction in dairy cattle?](https://arxiv.org/abs/2601.22522)
*Zhou Tang,Jin Wang,Angelo De Castro,Yuxi Zhang,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Xu Wang,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 三维点云在奶牛体况评分预测中并未比深度图像有持续优势，深度图像在多数情况下准确率更高。


<details>
  <summary>Details</summary>
Motivation: 传统视觉评分主观且耗时，计算机视觉方法如深度图像和点云被用于自动预测体况评分，但二者直接比较研究有限。

Method: 使用1020头奶牛数据，对比未分割、全身体、后躯分割和手工特征四类数据下的深度图像与点云模型，采用个体交叉验证评估性能。

Result: 深度图像在未分割和全身体数据上准确率更高，后躯分割数据表现相当；手工特征显著降低准确率；点云对噪声和模型结构更敏感。

Conclusion: 在当前条件下，深度图像仍是更稳定可靠的体况评分预测数据源，点云无明显优势。

Abstract: Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions.

</details>


### [21] [SHED Light on Segmentation for Dense Prediction](https://arxiv.org/abs/2601.22529)
*Seung Hyun Lee,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: SHED通过引入分割先验提升密集预测的几何一致性，无需显式分割监督即可学习分层结构，显著改善深度边界、语义分割和3D重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将密集预测视为独立像素任务，忽视场景几何结构，导致结果不一致。

Method: 提出SHED编码器-解码器架构，通过双向分层推理将分割标记池化与反池化，仅在输出端监督，使分层结构自动涌现。

Result: 提升深度边界锐度与分割连贯性，增强跨域泛化能力，改善语义分割与3D重建质量，揭示传统方法忽略的部件级结构。

Conclusion: SHED通过显式几何先验与分层建模，实现了更准确、可解释且泛化性强的密集预测。

Abstract: Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.

</details>


### [22] [Hybrid Cross-Device Localization via Neural Metric Learning and Feature Fusion](https://arxiv.org/abs/2601.22551)
*Meixia Lin,Mingkai Liu,Shuxue Peng,Dikai Fan,Shengyu Gu,Xianliang Huang,Haoyang Ye,Xiao Liu*

Main category: cs.CV

TL;DR: 提出一种混合跨设备定位方法，在CroCoDL 2025挑战赛中实现92.62高分。


<details>
  <summary>Details</summary>
Motivation: 提升跨设备定位的召回率与精度，解决传统方法在尺度与位姿估计中的不稳定性问题。

Method: 结合共享检索编码器、几何分支（特征融合+PnP）与神经前馈分支（MapAnything），引入神经引导候选过滤与深度条件定位优化。

Result: 在HYDRO和SUCCU基准上显著提升性能，最终得分92.62（R@0.5m, 5°）。

Conclusion: 混合架构有效融合几何与神经方法，实现高精度跨设备定位。

Abstract: We present a hybrid cross-device localization pipeline developed for the CroCoDL 2025 Challenge. Our approach integrates a shared retrieval encoder and two complementary localization branches: a classical geometric branch using feature fusion and PnP, and a neural feed-forward branch (MapAnything) for metric localization conditioned on geometric inputs. A neural-guided candidate pruning strategy further filters unreliable map frames based on translation consistency, while depth-conditioned localization refines metric scale and translation precision on Spot scenes. These components jointly lead to significant improvements in recall and accuracy across both HYDRO and SUCCU benchmarks. Our method achieved a final score of 92.62 (R@0.5m, 5°) during the challenge.

</details>


### [23] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 提出一种无需训练的即插即用选择性预测方法MA-PaPSP，通过内存增强提升视觉语言模型的预测置信度校准与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测方法主要针对闭集任务，而视觉语言基础模型面临开放集、无限词表等更复杂场景，亟需通用、低复杂度的解决方案。

Method: 提出Plug-and-Play Selective Prediction (PaPSP)，并进一步引入内存增强机制(MA-PaPSP)，利用检索到的图像-文本对平均嵌入以降低方差，结合对比归一化改善相似度校准。

Result: MA-PaPSP在图像描述、图像文本匹配和细粒度分类等多个任务上显著优于PaPSP及其他基线方法。

Conclusion: 内存增强的即插即用方法可有效提升视觉语言模型的选择性预测性能，无需训练即可适配多种基础模型。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [24] [A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions](https://arxiv.org/abs/2601.22830)
*Ji Zhou,Yilin Ding,Yongqi Zhao,Jiachen Xu,Arno Eichberger*

Main category: cs.CV

TL;DR: LVLMs在复杂自然场景中显著提升召回率，但几何精度不如YOLO，建议作为SOTIF系统的高层安全验证器。


<details>
  <summary>Details</summary>
Motivation: 现有感知方法在恶劣条件下表现不佳，SOTIF安全风险亟需解决，LVLMs的语义推理能力在安全关键检测中未被充分评估。

Method: 使用PeSOTIF数据集对10个主流LVLMs与YOLO基线进行系统评估，对比其在长尾交通场景与环境退化下的表现。

Result: 顶级LVLMs（如Gemini 3、Doubao）在复杂自然场景中召回率超YOLO 25%以上，但YOLO在合成扰动下几何精度更优。

Conclusion: LVLMs与YOLO具有互补优势，LVLMs适合作为SOTIF系统中的高阶安全验证模块。

Abstract: Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.

</details>


### [25] [DELNet: Continuous All-in-One Weather Removal via Dynamic Expert Library](https://arxiv.org/abs/2601.22573)
*Shihong Liu,Kun Zuo,Hanguang Xiao*

Main category: cs.CV

TL;DR: DELNet是一种无需重新训练即可持续学习多种天气图像修复任务的框架，显著降低开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有所有在内天气图像修复方法依赖预收集数据，面对新退化时需重新训练，成本高昂。

Method: DELNet引入判断阀值评估任务相似性，结合动态专家库，对新任务选取Top-k专家进行知识迁移并新增专家，对已知任务直接复用专家。

Result: 在OTS、Rain100H和Snow100K上分别实现PSNR提升16%、11%和12%，超越现有持续学习方法。

Conclusion: DELNet有效、鲁棒且高效，显著降低重训练成本，适用于真实场景部署。

Abstract: All-in-one weather image restoration methods are valuable in practice but depend on pre-collected data and require retraining for unseen degradations, leading to high cost. We propose DELNet, a continual learning framework for weather image restoration. DELNet integrates a judging valve that measures task similarity to distinguish new from known tasks, and a dynamic expert library that stores experts trained on different degradations. For new tasks, the valve selects top-k experts for knowledge transfer while adding new experts to capture task-specific features; for known tasks, the corresponding experts are directly reused. This design enables continuous optimization without retraining existing models. Experiments on OTS, Rain100H, and Snow100K demonstrate that DELNet surpasses state-of-the-art continual learning methods, achieving PSNR gains of 16\%, 11\%, and 12\%, respectively. These results highlight the effectiveness, robustness, and efficiency of DELNet, which reduces retraining cost and enables practical deployment in real-world scenarios.

</details>


### [26] [About an Automating Annotation Method for Robot Markers](https://arxiv.org/abs/2601.22982)
*Wataru Uemura,Takeru Nagashima*

Main category: cs.CV

TL;DR: 利用ArUco标记的内置信息实现深度学习模型的自动标注，提升机器人在复杂条件下对标记的识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统OpenCV方法在噪声、模糊、光照变化下表现不佳，而深度学习需要大量标注数据，人工标注成本高。

Method: 使用ArUco模块自动获取标记的ID与位置信息，作为标签训练YOLO模型，实现无需人工标注的深度学习训练。

Result: 所提方法在模糊和失焦条件下识别性能优于传统方法，同时减少人工劳动并保证标注一致性。

Conclusion: 自动标注方法有效提升模型性能与效率，未来将研究置信度阈值与识别性能的关系。

Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.

</details>


### [27] [Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding](https://arxiv.org/abs/2601.22574)
*Yuansheng Gao,Jinman Zhao,Tong Zhang,Xingguo Xu,Han Bao,Zonghui Wang,Wenzhi Chen*

Main category: cs.CV

TL;DR: 提出一种时空语义对比解码方法，有效缓解视频大模型的幻觉问题，同时保持理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有解码方法依赖启发式设计，无法精准捕捉视频幻觉的根源及其细粒度时空与语义关联，导致在复杂场景中鲁棒性和泛化性不足。

Method: 构建负特征通过刻意破坏视频特征的时空一致性与语义关联，并在推理中通过对比解码抑制幻觉。

Result: 实验表明该方法显著减少幻觉，同时保留模型的视频理解与推理能力。

Conclusion: 时空语义对比解码是一种高效缓解视频幻觉的新策略，具有更好的泛化性与鲁棒性。

Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.

</details>


### [28] [PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios](https://arxiv.org/abs/2601.22575)
*Xudong Lu,Huankang Guan,Yang Bo,Jinpeng Chen,Xintong Guo,Shuhan Li,Fang Liu,Peiwen Sun,Xueying Li,Wei Zhang,Xue Yang,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出首个移动-centric流式音视频基准PhoStream，揭示现有多模态大模型在何时回应上存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准多限于选择题或短视频，无法评估移动助手在真实流式场景中的实时响应能力。

Method: 构建PhoStream基准，包含5572个开放问答对，覆盖4种场景与10项能力，采用自动生成管道与人工验证，并使用在线推理和LLM-as-a-Judge评估。

Result: 模型在即时与回溯任务上表现良好（如Gemini 3 Pro超80分），但在前瞻任务上得分骤降至16.40，主要因过早响应。

Conclusion: 当前MLLMs的核心瓶颈是‘何时说’而非‘说什么’，亟需提升时序推理与响应时机判断能力。

Abstract: Multimodal Large Language Models excel at offline audio-visual understanding, but their ability to serve as mobile assistants in continuous real-world streams remains underexplored. In daily phone use, mobile assistants must track streaming audio-visual inputs and respond at the right time, yet existing benchmarks are often restricted to multiple-choice questions or use shorter videos. In this paper, we introduce PhoStream, the first mobile-centric streaming benchmark that unifies on-screen and off-screen scenarios to evaluate video, audio, and temporal reasoning. PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios and 10 capabilities. We build it with an Automated Generative Pipeline backed by rigorous human verification, and evaluate models using a realistic Online Inference Pipeline and LLM-as-a-Judge evaluation for open-ended responses. Experiments reveal a temporal asymmetry in LLM-judged scores (0-100): models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before the required visual and audio cues appear. This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/PhoStream.

</details>


### [29] [FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows](https://arxiv.org/abs/2601.23107)
*Ilir Tahiraj,Peter Wittal,Markus Lienkamp*

Main category: cs.CV

TL;DR: FlowCalib首次利用场景流检测LiDAR与车辆的校准偏差，无需额外传感器。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅修正传感器间误差，忽视了导致误差的单传感器校准偏差问题。

Method: 通过序列3D点云的场景流提取运动线索，结合神经场景流先验与双分支网络，融合全局特征与几何描述符，进行全局和轴向的二分类检测。

Result: 在nuScenes数据集上验证了FlowCalib对LiDAR-车辆校准偏差的鲁棒检测能力，建立了新基准。

Conclusion: FlowCalib为传感器-车辆校准偏差检测提供了无需额外传感器的高效解决方案。

Abstract: Accurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor-to-sensor errors without considering the miscalibration of individual sensors that cause these errors in the first place. We introduce FlowCalib, the first framework that detects LiDAR-to-vehicle miscalibration using motion cues from the scene flow of static objects. Our approach leverages the systematic bias induced by rotational misalignment in the flow field generated from sequential 3D point clouds, eliminating the need for additional sensors. The architecture integrates a neural scene flow prior for flow estimation and incorporates a dual-branch detection network that fuses learned global flow features with handcrafted geometric descriptors. These combined representations allow the system to perform two complementary binary classification tasks: a global binary decision indicating whether misalignment is present and separate, axis-specific binary decisions indicating whether each rotational axis is misaligned. Experiments on the nuScenes dataset demonstrate FlowCalib's ability to robustly detect miscalibration, establishing a benchmark for sensor-to-vehicle miscalibration detection.

</details>


### [30] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: 提出MIFOMO模型，利用遥感基础模型和混合域自适应等技术，显著提升高光谱图像跨域小样本分类性能，超越现有方法最高达14%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖不现实的噪声数据增强，参数量大易过拟合，且未利用基础模型的强泛化能力。

Method: 构建基于遥感基础模型的MIFOMO，引入共融投影（CP）冻结主干网络快速适配，提出混合域自适应（MDM）应对域差异，结合标签平滑处理伪标签噪声。

Result: 实验表明MIFOMO性能优于现有方法，最高提升14%，并开源代码以促进复现与研究。

Conclusion: MIFOMO有效解决高光谱图像跨域小样本学习中的数据稀缺与域差异问题，验证了基础模型在该任务中的潜力。

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [31] [FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data](https://arxiv.org/abs/2601.22596)
*Abdelrrahman Moubane*

Main category: cs.CV

TL;DR: 发布了一个覆盖法国28个部门的大规模建筑变化检测数据集FOTBCD，包含二进制和实例级标注，显著提升跨区域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集地理范围有限，难以评估模型在地理域偏移下的泛化能力，亟需大规模、多区域的基准数据集。

Method: 基于法国IGN权威正射影像与建筑数据构建FOTBCD，发布FOTBCD-Binary（2.8万对图像+像素级掩码）和FOTBCD-Instances（实例级标注子集），并采用地理隔离的测试集验证。

Result: 在FOTBCD-Binary上，模型在跨域泛化方面显著优于LEVIR-CD+和WHU-CD，证明地理多样性提升泛化能力。

Conclusion: FOTBCD为建筑变化检测提供了首个大规模、多区域、高精度基准，推动模型在真实地理域偏移场景下的评估与改进。

Abstract: We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.

</details>


### [32] [TTSA3R: Training-Free Temporal-Spatial Adaptive Persistent State for Streaming 3D Reconstruction](https://arxiv.org/abs/2601.22615)
*Zhijie Zheng,Xinhao Xiang,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出一种无训练框架TTSA3R，通过时空自适应更新提升流式3D重建的长期稳定性，显著降低记忆遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有流式递归模型在长序列中因平衡历史与新观测而遭遇灾难性记忆遗忘，现有注意力方法缺乏时空一致性考虑。

Method: 设计时间自适应更新模块分析状态演化模式，引入空间上下文更新模块通过观测-状态对齐与场景动态定位更新区域，融合双信号决定更新策略。

Result: 在多种3D任务中表现优异，长序列下误差仅增加15%，远优于基线模型200%以上的退化。

Conclusion: TTSA3R有效提升流式3D重建的长期稳定性，无需训练，具备实用潜力。

Abstract: Streaming recurrent models enable efficient 3D reconstruction by maintaining persistent state representations. However, they suffer from catastrophic memory forgetting over long sequences due to balancing historical information with new observations. Recent methods alleviate this by deriving adaptive signals from attention perspective, but they operate on single dimensions without considering temporal and spatial consistency. To this end, we propose a training-free framework termed TTSA3R that leverages both temporal state evolution and spatial observation quality for adaptive state updates in 3D reconstruction. In particular, we devise a Temporal Adaptive Update Module that regulates update magnitude by analyzing temporal state evolution patterns. Then, a Spatial Contextual Update Module is introduced to localize spatial regions that require updates through observation-state alignment and scene dynamics. These complementary signals are finally fused to determine the state updating strategies. Extensive experiments demonstrate the effectiveness of TTSA3R in diverse 3D tasks. Moreover, our method exhibits only 15% error increase compared to over 200% degradation in baseline models on extended sequences, significantly improving long-term reconstruction stability. Our codes will be available soon.

</details>


### [33] [UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating](https://arxiv.org/abs/2601.22616)
*Xing Yi,Jinyang Huang,Feng-Qi Cui,Anyang Tong,Ruimin Wang,Liu Liu,Dan Guo*

Main category: cs.CV

TL;DR: 提出UniGeo框架，通过几何感知学习和动态通道门控提升稀疏点云中的3D目标检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法未能建模稀疏点云中的几何关系，且忽视关键区域的特征分布，限制检测性能

Method: 引入几何感知学习模块建立空间关系到特征权重的可学习映射，并结合动态通道门控机制自适应优化稀疏3D U-Net生成的特征

Result: 在六个室内场景数据集上实验表明，UniGeo显著优于现有方法

Conclusion: UniGeo有效增强了稀疏点云中的几何特征表示，为多数据集统一训练提供了新思路

Abstract: The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.

</details>


### [34] [LINA: Linear Autoregressive Image Generative Models with Continuous Tokens](https://arxiv.org/abs/2601.22630)
*Jiahao Wang,Ting Pan,Haoge Deng,Dongchen Han,Taiqiang Wu,Xinlong Wang,Ping Luo*

Main category: cs.CV

TL;DR: 提出LINA，一种基于线性注意力的高效文本到图像生成模型，在1024x1024分辨率下实现高性能且计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型在视觉生成中计算成本高，需设计更高效的线性注意力机制。

Method: 系统分析线性注意力中的归一化方式（除法 vs. 减法）与深度卷积对局部性的影响，并提出KV门控机制扩展至双向设置。

Result: LINA在ImageNet上达到2.18 FID，GenEval上0.74 FID，单个线性注意力模块比softmax注意力减少61% FLOPs。

Conclusion: 除法归一化更适配生成任务，卷积增强局部性，KV门控提升内存管理能力，线性注意力可高效替代softmax注意力。

Abstract: Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation.
  Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models.
  We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models.
  Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: https://github.com/techmonsterwang/LINA.

</details>


### [35] [What can Computer Vision learn from Ranganathan?](https://arxiv.org/abs/2601.22634)
*Mayukh Bagchi,Fausto Giunchiglia*

Main category: cs.CV

TL;DR: 利用Ranganathan分类原则改进计算机视觉数据集设计，缓解语义鸿沟问题并提升标注准确性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中的语义鸿沟问题导致数据集和基准设计存在缺陷，亟需系统性解决方案。

Method: 将S.R. Ranganathan的分类原则适配至vTelos标注方法，用于指导视觉与语义的对齐。

Result: 实验表明vTelos方法提升了标注质量和模型准确率。

Conclusion: Ranganathan分类原则为解决语义鸿沟提供了理论基础，vTelos方法有效改善了CV数据集设计。

Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.

</details>


### [36] [Unsupervised Synthetic Image Attribution: Alignment and Disentanglement](https://arxiv.org/abs/2601.22663)
*Zongfang Liu,Guangyi Chen,Boyang Sun,Tongliang Liu,Kun Zhang*

Main category: cs.CV

TL;DR: 提出一种无监督的合成图像归因方法，通过对齐与解耦实现超越监督方法的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的配对标注，难以扩展至百万级训练源，亟需无监督解决方案

Method: 采用对比自监督学习进行概念对齐，结合Infomax损失实现表示解耦，理论基于典型相关分析的分解

Result: 在AbC真实基准上，无监督方法性能超越现有监督方法

Conclusion: 该方法为合成图像归因提供了新视角，验证了无监督学习在该任务上的巨大潜力

Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.

</details>


### [37] [ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding](https://arxiv.org/abs/2601.22666)
*Junyi Hu,Tian Bai,Fengyi Wu,Wenyan Li,Zhenming Peng,Yi Zhang*

Main category: cs.CV

TL;DR: 提出ExpAlign框架，通过隐式令牌-区域对齐和能量正则化，在无额外标注下提升开放词汇检测与零样本分割性能，尤其在长尾类别上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局句子嵌入缺乏细粒度表达，或需显式监督与复杂交叉注意力机制，难以高效实现弱监督下的视觉-语言对齐。

Method: 引入基于多实例学习的期望对齐头，通过注意力软池化实现隐式令牌与实例选择；结合能量驱动的多尺度一致性正则化，包括Top-K多正样本对比与几何感知一致性目标。

Result: 在LVIS minival上达到36.2 AP$_r$，显著超越同规模SOTA方法，且模型轻量、推理高效，长尾类别性能提升明显。

Conclusion: ExpAlign通过理论严谨的对齐机制，在无额外标注下实现高效精准的开放词汇视觉语言对齐，为弱监督场景提供新范式。

Abstract: Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.

</details>


### [38] [VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)
*Hanxun Yu,Wentong Li,Xuan Qu,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: VisionTrim 提出一种无需训练的多模态大模型加速框架，通过保留关键视觉令牌并结合文本引导的令牌合并，显著降低计算开销且保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌缩减方法多关注单一模块，忽视文本对齐，导致性能下降；高分辨率与视频场景下计算成本过高制约实际部署。

Method: 提出VisionTrim框架，包含两个即插即用模块：Dominant Vision Token Selection (DVTS) 通过全局-局部视角选择关键视觉令牌，Text-Guided Vision Complement (TGVC) 利用文本线索引导上下文感知的令牌合并。

Result: 在多种图像与视频多模态基准上实验表明，VisionTrim在显著降低计算成本的同时，性能优于现有方法。

Conclusion: VisionTrim为多模态大模型提供高效、无需训练的加速方案，推动其在真实场景中的实用化部署。

Abstract: Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.

</details>


### [39] [Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition](https://arxiv.org/abs/2601.22675)
*Shuhan Ye,Yuanbin Qian,Yi Yu,Chong Wang,Yuqi Xie,Jiazhen Xu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: 提出PBO模块解决SNN在视频任务中因低通特性忽视运动信息的问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: SNN虽具时间处理能力，但在动态视频任务中表现远逊于ANN，因标准脉冲动力学为低通滤波，抑制了关键运动频段。

Method: 提出Pass-Bands Optimizer (PBO)，仅引入两个可学习参数与轻量一致性约束，自适应优化时间通带以聚焦运动信息，无需修改网络架构。

Result: 在UCF101上提升超10个百分点，在多模态动作识别与弱监督视频异常检测中持续显著增益。

Conclusion: PBO为SNN视频处理提供了新视角，通过频带优化突破当前性能瓶颈。

Abstract: Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to artificial neural networks (ANNs). In this work, we diagnose a fundamental pass-band mismatch: Standard spiking dynamics behave as a temporal low pass that emphasizes static content while attenuating motion bearing bands, where task relevant information concentrates in dynamic tasks. This phenomenon explains why SNNs can approach ANNs on static tasks yet fall behind on tasks that demand richer temporal understanding.To remedy this, we propose the Pass-Bands Optimizer (PBO), a plug-and-play module that optimizes the temporal pass-band toward task-relevant motion bands. PBO introduces only two learnable parameters, and a lightweight consistency constraint that preserves semantics and boundaries, incurring negligible computational overhead and requires no architectural changes. PBO deliberately suppresses static components that contribute little to discrimination, effectively high passing the stream so that spiking activity concentrates on motion bearing content. On UCF101, PBO yields over ten percentage points improvement. On more complex multi-modal action recognition and weakly supervised video anomaly detection, PBO delivers consistent and significant gains, offering a new perspective for SNN based video processing and understanding.

</details>


### [40] [Visual Personalization Turing Test](https://arxiv.org/abs/2601.22680)
*Rameen Abdal,James Burgess,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出视觉个性化图灵测试VPTT，通过感知不可区分性评估个性化生成内容，构建基准与评分指标，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度关注身份复制，缺乏对个人风格与上下文真实性的感知评估，需新范式衡量个性化生成内容的自然性。

Method: 提出VPTT框架，包含10k人物基准VPTT-Bench、视觉检索增强生成器VPRAG，以及基于文本的VPTT评分指标，与人类和VLM判断对齐。

Result: VPTT评分与人类和VLM判断高度相关，VPRAG在原创性与一致性间取得最佳平衡，实现可扩展且隐私安全的个性化生成。

Conclusion: VPTT为个性化生成AI提供了感知导向、可量化、隐私友好的评估新标准。

Abstract: We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.

</details>


### [41] [OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection](https://arxiv.org/abs/2601.22685)
*Binyi Su,Chenghao Huang,Haiyong Chen*

Main category: cs.CV

TL;DR: 提出OOVDet框架，通过合成OOV提示和伪OOV样本提升零样本下OOV检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法易过拟合IV类，导致OOV类被误判为IV类且置信度高

Method: 1. 在隐空间低概率区域合成区域级OOV提示；2. 使用Dirichlet梯度归因生成伪OOV样本；3. 基于高斯核密度估计构建OOV决策边界

Result: 显著提升零样本场景下的OOV检测性能

Conclusion: 通过低密度先验约束有效区分IV与OOV类，方法可复现且开源

Abstract: Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model's lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.

</details>


### [42] [PEAR: Pixel-aligned Expressive humAn mesh Recovery](https://arxiv.org/abs/2601.22693)
*Jiahao Wu,Yunfei Liu,Lijian Lin,Ye Zhu,Lei Zhu,Jingyi Li,Yu Li*

Main category: cs.CV

TL;DR: PEAR是一种快速、鲁棒的单图三维人体网格重建方法，实现超100FPS的SMPLX和FLAME参数推理，显著提升细粒度姿态与表情精度。


<details>
  <summary>Details</summary>
Motivation: 现有SMPLX方法推理慢、细粒度姿态不准、面部和手部伪影多，难以用于下游任务。

Method: 采用简洁的ViT模型预测粗略人体几何，结合像素级监督优化细节，并设计模块化标注策略增强数据鲁棒性。

Result: 在多个基准数据集上，PEAR在姿态估计精度上显著优于现有方法，推理速度超100FPS，且无需预处理。

Conclusion: PEAR实现了高效、高精度的表达人体网格重建，为实时应用提供了可行方案。

Abstract: Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as the face and hands. These issues make current approaches difficult to apply to downstream tasks. To address these challenges, we propose PEAR-a fast and robust framework for pixel-aligned expressive human mesh recovery. PEAR explicitly tackles three major limitations of existing methods: slow inference, inaccurate localization of fine-grained human pose details, and insufficient facial expression capture. Specifically, to enable real-time SMPLX parameter inference, we depart from prior designs that rely on high resolution inputs or multi-branch architectures. Instead, we adopt a clean and unified ViT-based model capable of recovering coarse 3D human geometry. To compensate for the loss of fine-grained details caused by this simplified architecture, we introduce pixel-level supervision to optimize the geometry, significantly improving the reconstruction accuracy of fine-grained human details. To make this approach practical, we further propose a modular data annotation strategy that enriches the training data and enhances the robustness of the model. Overall, PEAR is a preprocessing-free framework that can simultaneously infer EHM-s (SMPLX and scaled-FLAME) parameters at over 100 FPS. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves substantial improvements in pose estimation accuracy compared to previous SMPLX-based approaches. Project page: https://wujh2001.github.io/PEAR

</details>


### [43] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: 提出Bi-MCQ框架，通过条件语义比较改进医学视觉语言模型对否定语句的理解，显著提升负样本识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因对比对齐目标将否定视为轻微语言变化，导致难以理解临床否定语句，且提示驱动的InfoNCE微调强化了易正样本对齐，削弱了疾病缺失的学习能力。

Method: 将视觉语言对齐重构为条件语义比较问题，提出双向多选学习框架（Bi-MCQ），联合训练图像到文本和文本到图像的多选任务，使用肯定、否定及混合提示，并引入方向特异性交叉注意力融合模块减少对齐干扰。

Result: 在ChestXray14、Open-I、CheXpert和PadChest数据集上，Bi-MCQ相比SOTA模型CARZero零样本性能提升最高0.47 AUC，PNC评估提升0.08，且将肯定-否定AUC差距平均缩小0.12。

Conclusion: 通过重构对齐目标为条件语义比较，可显著增强医学视觉语言模型对否定语义的理解能力。

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [44] [DAVIS: OOD Detection via Dominant Activations and Variance for Increased Separation](https://arxiv.org/abs/2601.22703)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: DAVIS通过保留全局平均池化前的通道方差和最大激活值，显著提升分布外检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有后处理检测方法依赖全局平均池化（GAP）特征，丢失了关键的激活图分布统计信息，导致检测性能受限。

Method: DAVIS在GAP特征基础上加入通道方差和最大激活值，增强特征表示，无需修改模型结构。

Result: 在ResNet、DenseNet、EfficientNet等架构上显著降低FPR95，如CIFAR-10上ResNet-18降低48.26%。

Conclusion: 通道方差与最大激活值是OOD检测的关键信息，应超越均值统计以提升检测效果。

Abstract: Detecting out-of-distribution (OOD) inputs is a critical safeguard for deploying machine learning models in the real world. However, most post-hoc detection methods operate on penultimate feature representations derived from global average pooling (GAP) -- a lossy operation that discards valuable distributional statistics from activation maps prior to global average pooling. We contend that these overlooked statistics, particularly channel-wise variance and dominant (maximum) activations, are highly discriminative for OOD detection. We introduce DAVIS, a simple and broadly applicable post-hoc technique that enriches feature vectors by incorporating these crucial statistics, directly addressing the information loss from GAP. Extensive evaluations show DAVIS sets a new benchmark across diverse architectures, including ResNet, DenseNet, and EfficientNet. It achieves significant reductions in the false positive rate (FPR95), with improvements of 48.26\% on CIFAR-10 using ResNet-18, 38.13\% on CIFAR-100 using ResNet-34, and 26.83\% on ImageNet-1k benchmarks using MobileNet-v2. Our analysis reveals the underlying mechanism for this improvement, providing a principled basis for moving beyond the mean in OOD detection.

</details>


### [45] [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/abs/2601.22709)
*Yanlong Chen,Amirhossein Habibian,Luca Benini,Yawei Li*

Main category: cs.CV

TL;DR: GRACE通过信息瓶颈统一知识蒸馏与量化感知训练，在INT4下超越FP16性能并实现3倍吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型部署成本高，传统量化导致精度损失大，量化感知训练尚未充分探索。

Method: 基于信息瓶颈，结合置信度门控解耦蒸馏、关系中心核对齐与拉格朗日自适应控制器，实现高效量化。

Result: 在LLaVA和Qwen系列上，INT4模型性能超越FP16基线，接近教师模型，同时实现3倍吞吐与54%内存降低。

Conclusion: GRACE是面向资源受限场景的高效量化框架，显著优于现有方法。

Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.

</details>


### [46] [OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation](https://arxiv.org/abs/2601.22725)
*Jin Li,Tao Chen,Shuai Jiang,Weijie Wang,Jingwen Luo,Chenhui Wu*

Main category: cs.CV

TL;DR: 提出OpenVTON-Bench大规模基准和多模态评估协议，显著提升虚拟试衣评估的可靠性与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标难以量化细粒度纹理与语义一致性，数据集规模与多样性不足，制约VTON发展。

Method: 构建10万组高分辨率图像对，采用DINOv3聚类与Gemini标注，提出基于VLM与SAM3的多尺度评估协议，覆盖5个可解释维度。

Result: 评估协议与人类判断高度一致（Kendall's τ=0.833），显著优于SSIM（0.611），实现边界对齐与纹理误差分离。

Conclusion: OpenVTON-Bench为VTON提供首个商业级大规模基准与可靠评估标准，推动领域标准化发展。

Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $τ$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.

</details>


### [47] [GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction](https://arxiv.org/abs/2601.22729)
*A. Enes Doruk,Hasan F. Ates*

Main category: cs.CV

TL;DR: GaussianOcc3D提出一种基于高斯表示的多模态3D语义占据预测框架，有效融合相机与激光雷达数据，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单模态方法在相机语义与激光雷达几何间存在权衡，多模态框架面临模态异构、空间错位和表示危机问题。

Method: 提出GaussianOcc3D框架，包含LDFA、EBFS、ACLF和Gauss-Mamba Head四个模块，利用连续高斯表示实现高效多模态融合。

Result: 在Occ3D、SurroundOcc和SemanticKITTI上分别取得49.4%、28.9%和25.2%的mIoU，且在雨天和夜间场景中表现更鲁棒。

Conclusion: 连续高斯表示有效解决多模态融合中的表示与效率问题，为自动驾驶环境感知提供新范式。

Abstract: 3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.

</details>


### [48] [ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model](https://arxiv.org/abs/2601.22730)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Taichun Zhou,Xinwang Liu*

Main category: cs.CV

TL;DR: 提出ImgCoT，用视觉化思维链替代文本重建，以减少语言偏差并提升推理结构捕捉能力，同时通过混合策略保留关键细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法用自编码器重建文本思维链，过度关注语言表层特征，忽视推理结构，导致逻辑抽象能力受限。

Method: 将思维链渲染为图像作为重建目标（ImgCoT），引入空间归纳偏置；进一步提出松散ImgCoT，结合低似然文本步骤补充细节。

Result: 在多个数据集和大模型上验证，ImgCoT及其变体能用更少token保留全局推理结构与关键细节，性能优于纯文本方法。

Conclusion: 视觉化思维链能有效缓解语言偏差，提升推理效率与抽象能力，混合策略实现效率与精度的平衡。

Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.

</details>


### [49] [Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models](https://arxiv.org/abs/2601.22737)
*Enyi Shi,Pengyang Shao,Yanxin Zhang,Chenhang Cui,Jiayi Lyu,Xu Xie,Xiaobo Xia,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出Lingua-SafetyBench基准，揭示多语言多模态VLLM安全风险的模态与语言不对称性，指出缩放无法均衡语言间安全差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅支持单模态或多语言单模态，缺乏真实跨模态有害对，难以评估VLLM在真实多语言多模态场景下的安全风险。

Method: 构建100,440个跨10种语言的有害图像-文本对，分为图像主导与文本主导两类，评估11个开源VLLM的攻击成功率（ASR），并控制Qwen系列进行缩放实验。

Result: 图像主导风险在高资源语言ASR更高，文本主导风险在非高资源语言更严重；模型缩放虽降低总体ASR，但加剧HRL与Non-HRL间的安全差距。

Conclusion: VLLM安全对齐需考虑语言与模态双重维度，仅靠模型扩增无法解决语言不平衡安全问题。

Abstract: Robust safety of vision-language large models (VLLMs) under joint multilingual and multimodal inputs remains underexplored. Existing benchmarks are typically multilingual but text-only, or multimodal but monolingual. Recent multilingual multimodal red-teaming efforts render harmful prompts into images, yet rely heavily on typography-style visuals and lack semantically grounded image-text pairs, limiting coverage of realistic cross-modal interactions. We introduce Lingua-SafetyBench, a benchmark of 100,440 harmful image-text pairs across 10 languages, explicitly partitioned into image-dominant and text-dominant subsets to disentangle risk sources. Evaluating 11 open-source VLLMs reveals a consistent asymmetry: image-dominant risks yield higher ASR in high-resource languages, while text-dominant risks are more severe in non-high-resource languages. A controlled study on the Qwen series shows that scaling and version upgrades reduce Attack Success Rate (ASR) overall but disproportionately benefit HRLs, widening the gap between HRLs and Non-HRLs under text-dominant risks. This underscores the necessity of language- and modality-aware safety alignment beyond mere scaling.To facilitate reproducibility and future research, we will publicly release our benchmark, model checkpoints, and source code.The code and dataset will be available at https://github.com/zsxr15/Lingua-SafetyBench.Warning: this paper contains examples with unsafe content.

</details>


### [50] [StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing](https://arxiv.org/abs/2601.22738)
*Han Wang,Deyi Ji,Lanyun Zhu,Jiebo Luo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: StreamSense通过轻量级编码器与选择性调用VLM，高效处理直播流中的社会信号检测，提升准确率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 直播平台需要实时响应多模态社交信号，但传统VLM计算开销大，难以满足低延迟需求。

Method: 采用轻量级流式编码器处理大部分时间戳，仅对模糊案例调用VLM，并通过跨模态对比损失和IoU加权损失优化训练。

Result: 在情感分类和仇恨内容审核等任务中，StreamSense比纯VLM更准确，且VLM调用频率低，显著降低延迟与算力消耗。

Conclusion: 选择性升级与延迟决策是处理流式社交任务的有效机制。

Abstract: Live streaming platforms require real-time monitoring and reaction to social signals, utilizing partial and asynchronous evidence from video, text, and audio. We propose StreamSense, a streaming detector that couples a lightweight streaming encoder with selective routing to a Vision-Language Model (VLM) expert. StreamSense handles most timestamps with the lightweight streaming encoder, escalates hard/ambiguous cases to the VLM, and defers decisions when context is insufficient. The encoder is trained using (i) a cross-modal contrastive term to align visual/audio cues with textual signals, and (ii) an IoU-weighted loss that down-weights poorly overlapping target segments, mitigating label interference across segment boundaries. We evaluate StreamSense on multiple social streaming detection tasks (e.g., sentiment classification and hate content moderation), and the results show that StreamSense achieves higher accuracy than VLM-only streaming while only occasionally invoking the VLM, thereby reducing average latency and compute. Our results indicate that selective escalation and deferral are effective primitives for understanding streaming social tasks. Code is publicly available on GitHub.

</details>


### [51] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: 提出FaceDefense框架，在抑制扩散模型人脸替换的同时保持视觉不可察觉性，显著提升防御效果与隐蔽性平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型人脸替换技术虽性能优越，但易被用于恶意侵犯肖像权与声誉，现有防御方法面临扰动大小与效果之间的权衡困境。

Method: 引入新的扩散损失增强对抗样本防御力，结合定向人脸属性编辑修复扰动引起的畸变，采用两阶段交替优化生成最终扰动人脸图像。

Result: FaceDefense在视觉不可察觉性与防御有效性上均显著优于现有方法，实现了更优的权衡。

Conclusion: FaceDefense为对抗扩散型人脸替换提供了一种高效且隐蔽的主动防御新途径。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [52] [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/abs/2601.22754)
*Guillermo Gil de Avalle,Laura Maruster,Christos Emmanouilidis*

Main category: cs.CV

TL;DR: 本文评估了两种视觉语言模型在从工业故障排除流程图中提取结构化知识的表现，比较了标准提示与增强提示策略，揭示了模型在布局敏感性和语义鲁棒性间的权衡。


<details>
  <summary>Details</summary>
Motivation: 工业故障排除指南通过空间布局和专业术语传递信息，手动提取知识耗时且易错，需自动化方法支持操作员系统。

Method: 使用两种视觉语言模型，对比标准指令提示与增强提示（引入故障排查布局模式提示）的结构化知识提取效果。

Result: 模型在布局敏感性和语义鲁棒性上存在权衡，增强提示能提升布局理解，但不同模型表现差异显著。

Conclusion: 应根据具体部署需求选择模型与提示策略，增强提示有助于提升流程图理解，但需考虑模型特性。

Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.

</details>


### [53] [Is Training Necessary for Anomaly Detection?](https://arxiv.org/abs/2601.22763)
*Xingwu Zhang,Guanxuan Li,Paul Henderson,Gerardo Aragon-Camarasa,Zijun Long*

Main category: cs.CV

TL;DR: 提出无需训练的基于检索的异常检测方法RAD，超越现有重建方法，在多个基准上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的无监督多类异常检测方法存在保真度与稳定性之间的内在权衡问题

Method: 提出RAD方法，通过在内存中存储无异常特征，利用多级检索匹配测试块，完全放弃重建范式

Result: 在MVTec-AD等四个基准上实现SOTA性能，仅用单张正常图即可达到96.7%像素AUROC，理论证明检索分数上界优于重建残差

Conclusion: 颠覆了MUAD必须依赖任务特定训练的假设，证明基于内存检索可实现最优异常检测

Abstract: Current state-of-the-art multi-class unsupervised anomaly detection (MUAD) methods rely on training encoder-decoder models to reconstruct anomaly-free features. We first show these approaches have an inherent fidelity-stability dilemma in how they detect anomalies via reconstruction residuals. We then abandon the reconstruction paradigm entirely and propose Retrieval-based Anomaly Detection (RAD). RAD is a training-free approach that stores anomaly-free features in a memory and detects anomalies through multi-level retrieval, matching test patches against the memory. Experiments demonstrate that RAD achieves state-of-the-art performance across four established benchmarks (MVTec-AD, VisA, Real-IAD, 3D-ADAM) under both standard and few-shot settings. On MVTec-AD, RAD reaches 96.7\% Pixel AUROC with just a single anomaly-free image compared to 98.5\% of RAD's full-data performance. We further prove that retrieval-based scores theoretically upper-bound reconstruction-residual scores. Collectively, these findings overturn the assumption that MUAD requires task-specific training, showing that state-of-the-art anomaly detection is feasible with memory-based retrieval. Our code is available at https://github.com/longkukuhi/RAD.

</details>


### [54] [Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2601.22778)
*Nan Zhong,Yiran Xu,Mian Zou*

Main category: cs.CV

TL;DR: 提出DCCT框架，利用相机CFA和去马赛克过程中的颜色相关性检测AI生成图像，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: AI生成图像威胁数字真实性，现有基于生成伪影的检测器泛化能力差。

Method: 通过模拟CFA采样模式，将图像分解为单通道条件输入与双通道目标，训练自监督U-Net建模缺失通道的条件分布，采用混合逻辑斯蒂函数参数化。

Result: DCCT在20多个未见过的生成器上实现SOTA泛化与鲁棒性，优于现有方法。

Conclusion: 利用相机成像管道的颜色相关性可有效区分真实与AI生成图像，为检测提供新方向。

Abstract: As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.

</details>


### [55] [Diachronic Stereo Matching for Multi-Date Satellite Imagery](https://arxiv.org/abs/2601.22808)
*Elías Masquil,Luca Savant Aira,Roger Marí,Thibaud Ehret,Pablo Musé,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 提出首个适用于卫星图像时序差异对的异步立体匹配方法，通过微调MonSter模型并利用单目深度先验，显著提升跨季节图像的3D重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统立体匹配方法在图像采集时间相隔数月时因季节、光照和阴影变化而失效，亟需一种能处理时序差异的新型方法。

Method: 基于预训练的MonSter深度立体网络，使用DFC2019数据集中的同步与异步卫星图像对进行微调，结合单目深度先验增强模型对时序变化的鲁棒性。

Result: 在WorldView-3多期图像上，所提方法在同步和异步场景下均优于传统管道和未微调的深度模型，重建误差显著降低（如冬秋图像对误差从3.99m降至1.23m）。

Conclusion: 微调模型结合单目深度先验是实现跨时序卫星图像高精度3D重建的关键，为长期地表变化监测提供了新工具。

Abstract: Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions. On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations. On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs. However, when the two images are captured months apart, strong seasonal, illumination, and shadow changes violate standard stereoscopic assumptions, causing existing pipelines to fail. This work presents the first Diachronic Stereo Matching method for satellite imagery, enabling reliable 3D reconstruction from temporally distant pairs. Two advances make this possible: (1) fine-tuning a state-of-the-art deep stereo network that leverages monocular depth priors, and (2) exposing it to a dataset specifically curated to include a diverse set of diachronic image pairs. In particular, we start from a pretrained MonSter model, trained initially on a mix of synthetic and real datasets such as SceneFlow and KITTI, and fine-tune it on a set of stereo pairs derived from the DFC2019 remote sensing challenge. This dataset contains both synchronic and diachronic pairs under diverse seasonal and illumination conditions. Experiments on multi-date WorldView-3 imagery demonstrate that our approach consistently surpasses classical pipelines and unadapted deep stereo models on both synchronic and diachronic settings. Fine-tuning on temporally diverse images, together with monocular priors, proves essential for enabling 3D reconstruction from previously incompatible acquisition dates. Left image (winter) Right image (autumn) DSM geometry Ours (1.23 m) Zero-shot (3.99 m) LiDAR GT Figure 1. Output geometry for a winter-autumn image pair from Omaha (OMA 331 test scene). Our method recovers accurate geometry despite the diachronic nature of the pair, exhibiting strong appearance changes, which cause existing zero-shot methods to fail. Missing values due to perspective shown in black.  Mean altitude error in parentheses; lower is better.

</details>


### [56] [FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images](https://arxiv.org/abs/2601.22809)
*Haiyang Wu,Weiliang Mu,Jipeng Zhang,Zhong Dandan,Zhuofei Du,Haifeng Li,Tao Chao*

Main category: cs.CV

TL;DR: 提出FarmMind框架，通过动态查询辅助图像提升农田遥感图像分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单个图像块，难以处理模糊和不确定场景；人类专家则通过查询辅助图像进行交叉验证，提升推理能力

Method: 提出推理-查询驱动的动态分割框架FarmMind，先分析分割模糊的根源，再按需查询高分辨率、大尺度或时序邻近的辅助图像

Result: 在实验中显著优于现有方法，具有更强的泛化能力

Conclusion: 动态推理查询机制有效突破静态分割局限，为遥感图像分割提供了新思路

Abstract: Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.

</details>


### [57] [NativeTok: Native Visual Tokenization for Improved Image Generation](https://arxiv.org/abs/2601.22837)
*Bin Wu,Mengqi Huang,Weinan Jia,Zhendong Mao*

Main category: cs.CV

TL;DR: 提出NativeTok框架，通过因果依赖的视觉分词提升图像生成质量与效率


<details>
  <summary>Details</summary>
Motivation: 现有VQ图像生成方法中，分词阶段未约束token依赖关系，导致生成模型学习无序分布，产生偏差和连贯性差的问题

Method: 引入NativeTok框架，包含Meta Image Transformer (MIT)和Mixture of Causal Expert Transformer (MoCET)，在分词阶段强制因果依赖，并采用分层训练策略提升效率

Result: 实验表明NativeTok在保持高效重建的同时显著提升生成质量与连贯性

Conclusion: 因果约束的原生视觉分词是改进VQ图像生成的关键，NativeTok为高效高质生成提供新范式

Abstract: VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.

</details>


### [58] [Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model](https://arxiv.org/abs/2601.22838)
*Zhijing Yang,Weiwei Zhang,Mingliang Yang,Siyuan Peng,Yukai Shi,Junpeng Tan,Tianshui Chen,Liruo Zhong*

Main category: cs.CV

TL;DR: 提出神经服装试穿器（NCT）框架，实现个性化虚拟试衣，支持服装、姿态与属性的灵活定制。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试衣（VTON）无法满足用户对服装外观、姿态和属性的个性化定制需求，亟需更灵活的解决方案。

Method: 引入NCT框架，结合扩散模型、语义增强模块与语义控制模块，利用视觉-语言编码器对齐多模态特征，实现细节保留与姿态属性编辑。

Result: 在公开基准上实验表明，NCT在语义保留、细节还原和定制灵活性上表现优异。

Conclusion: NCT有效解决了个性化虚拟试衣问题，为数字时尚体验提供了新范式。

Abstract: This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.

</details>


### [59] [How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models](https://arxiv.org/abs/2601.22841)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 遥感基础模型在远小于计算机视觉模型的规模下即进入过参数化区域，参数增加主要产生冗余表征，而非新抽象；后验瘦身实验证明其高冗余性与部署潜力。


<details>
  <summary>Details</summary>
Motivation: 直接将计算机视觉的缩放假设迁移到遥感基础模型未被充分检验，亟需评估遥感模型是否在更小规模下即出现冗余。

Method: 采用后验瘦身技术，均匀压缩预训练编码器宽度，在六个先进遥感基础模型上对四个下游分类任务进行冗余度评估，并结合方差解释率与特征相关性分析机制。

Result: 在1% FLOPs下，遥感模型保持71%相对准确率，远高于ImageNet的MAE模型（<10%），证实其高度冗余；可学习瘦身训练可进一步提升MoCo与MAE模型性能。

Conclusion: 遥感基础模型具有显著冗余性，挑战了CV主导的缩放范式；后验瘦身既是资源受限场景的有效部署策略，也是诊断模型冗余的有力工具。

Abstract: Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.

</details>


### [60] [Inference-Time Dynamic Modality Selection for Incomplete Multimodal Classification](https://arxiv.org/abs/2601.22853)
*Siyi Du,Xinzhe Luo,Declan P. O'Regan,Chen Qin*

Main category: cs.CV

TL;DR: 提出DyMo框架，通过动态选择可靠恢复的模态，解决不完整多模态学习中的丢弃-填补困境。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不完整多模态数据时面临丢弃模态导致信息丢失或填补引入噪声的困境。

Method: 提出DyMo框架，基于任务损失构建可计算的奖励函数，动态选择最优模态组合，并设计兼容任意模态的网络结构与训练策略。

Result: 在多个自然与医学图像数据集上，DyMo显著超越现有方法，表现稳定且鲁棒。

Conclusion: DyMo突破了传统丢弃或填补的范式，通过推理时动态选择实现更高效的信息利用。

Abstract: Multimodal deep learning (MDL) has achieved remarkable success across various domains, yet its practical deployment is often hindered by incomplete multimodal data. Existing incomplete MDL methods either discard missing modalities, risking the loss of valuable task-relevant information, or recover them, potentially introducing irrelevant noise, leading to the discarding-imputation dilemma. To address this dilemma, in this paper, we propose DyMo, a new inference-time dynamic modality selection framework that adaptively identifies and integrates reliable recovered modalities, fully exploring task-relevant information beyond the conventional discard-or-impute paradigm. Central to DyMo is a novel selection algorithm that maximizes multimodal task-relevant information for each test sample. Since direct estimation of such information at test time is intractable due to the unknown data distribution, we theoretically establish a connection between information and the task loss, which we compute at inference time as a tractable proxy. Building on this, a novel principled reward function is proposed to guide modality selection. In addition, we design a flexible multimodal network architecture compatible with arbitrary modality combinations, alongside a tailored training strategy for robust representation learning. Extensive experiments on diverse natural and medical image datasets show that DyMo significantly outperforms state-of-the-art incomplete/dynamic MDL methods across various missing-data scenarios. Our code is available at https://github.com//siyi-wind/DyMo.

</details>


### [61] [Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction](https://arxiv.org/abs/2601.22861)
*Refael Sheffer,Chen Pinchover,Haim Zisman,Dror Ozeri,Roee Litman*

Main category: cs.CV

TL;DR: 使用普通RGB图像和NeRF技术重建树冠下真实感地面视图，替代昂贵的LiDAR或热成像传感器。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵或专用传感器（如机载LiDAR或AOS），需低成本、高分辨率的替代方案用于搜救、步道测绘和森林普查。

Method: 基于NeRF的3D重建，结合低光损失优化暗光林下成像，并通过控制光线积分策略去除树冠遮挡。

Result: 在RGB图像基础上实现媲美热成像AOS的人体检测效果，并成功应用于树木计数等森林调查任务。

Conclusion: 该方法为搜救、步道测绘和森林普查提供了一种低成本、高分辨率的实用替代方案。

Abstract: Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.
  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.
  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.

</details>


### [62] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 提出上下文异常检测新框架，构建CAAD-3K基准，利用视觉-语言模型提升上下文兼容性建模性能


<details>
  <summary>Details</summary>
Motivation: 传统异常检测假设异常是观测的固有属性，忽略上下文依赖，而现实中异常常由主体与上下文的不兼容引起

Method: 提出条件兼容性学习框架，利用视觉-语言表示建模主体-上下文关系，在有限监督下进行训练

Result: 在CAAD-3K上显著超越现有方法，并在MVTec-AD和VisA上达到SOTA效果

Conclusion: 建模上下文依赖能有效补充传统结构异常检测，为视觉异常检测提供新范式

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [63] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: DINO-SAE通过保留特征方向、放弃幅度强制匹配，并在球面流形上使用扩散模型，显著提升了基于预训练视觉模型的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉基础模型的生成自编码器因丢失高频细节而导致重建保真度不足。

Method: 提出DINO-SAE框架，包含分层卷积块嵌入模块增强局部结构、余弦相似性对齐保持语义一致性，并在球面流形上使用黎曼流匹配训练扩散变换器（DiT）。

Result: 在ImageNet-1K上达到SOTA重建质量：rFID 0.37，PSNR 26.2 dB，且80轮即可实现gFID 3.47的高效收敛。

Conclusion: 通过解耦语义方向与特征幅度，并在球面流形上建模，DINO-SAE实现了语义一致性与高保真重建的协同优化。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [64] [Multi-Cue Anomaly Detection and Localization under Data Contamination](https://arxiv.org/abs/2601.22913)
*Anindya Sundar Das,Monowar Bhuyan*

Main category: cs.CV

TL;DR: 提出一种结合少量异常标签的鲁棒异常检测框架，在数据污染下仍能实现高精度检测与定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设训练数据无污染且无异常标签，现实中难以满足，导致检测和定位性能差。

Method: 引入复合异常分数，结合偏差分数、熵不确定分数和分割分数，并通过自适应实例加权利用少量标注异常数据。

Result: 在MVTec和VisA基准上优于SOTA方法，具备强检测、定位性能、可解释性和抗污染鲁棒性。

Conclusion: 少量异常标签可显著提升模型在真实污染数据中的泛化能力与可靠性。

Abstract: Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.

</details>


### [65] [Deep in the Jungle: Towards Automating Chimpanzee Population Estimation](https://arxiv.org/abs/2601.22917)
*Tom Raynes,Otto Brookes,Timm Haucke,Lukas Bösch,Anne-Sophie Crunchant,Hjalmar Kühl,Sara Beery,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 该研究利用单目深度估计（MDE）技术替代人工测量，实现对野生黑猩猩种群密度和数量的自动估算，结果与传统方法误差在22%内。


<details>
  <summary>Details</summary>
Motivation: 传统依赖人工解读摄像头视频测量动物距离的方法耗时耗力，亟需自动化方案以提升大猿保护中的种群估算效率。

Method: 结合Dense Prediction Transformers（DPT）和Depth Anything两种MDE模型，与多种距离抽样策略集成，基于220段黑猩猩摄像头视频自动生成检测距离并估算种群密度和数量。

Result: DPT在距离估计精度和下游密度估算上优于Depth Anything，但二者均系统性高估距离，导致低估种群密度；动物检测失败是主要误差来源。

Conclusion: MDE驱动的摄像头陷阱距离抽样是一种可行且实用的替代方案，估算结果与传统方法接近，具备规模化应用潜力。

Abstract: The estimation of abundance and density in unmarked populations of great apes relies on statistical frameworks that require animal-to-camera distance measurements. In practice, acquiring these distances depends on labour-intensive manual interpretation of animal observations across large camera trap video corpora. This study introduces and evaluates an only sparsely explored alternative: the integration of computer vision-based monocular depth estimation (MDE) pipelines directly into ecological camera trap workflows for great ape conservation. Using a real-world dataset of 220 camera trap videos documenting a wild chimpanzee population, we combine two MDE models, Dense Prediction Transformers and Depth Anything, with multiple distance sampling strategies. These components are used to generate detection distance estimates, from which population density and abundance are inferred. Comparative analysis against manually derived ground-truth distances shows that calibrated DPT consistently outperforms Depth Anything. This advantage is observed in both distance estimation accuracy and downstream density and abundance inference. Nevertheless, both models exhibit systematic biases. We show that, given complex forest environments, they tend to overestimate detection distances and consequently underestimate density and abundance relative to conventional manual approaches. We further find that failures in animal detection across distance ranges are a primary factor limiting estimation accuracy. Overall, this work provides a case study that shows MDE-driven camera trap distance sampling is a viable and practical alternative to manual distance estimation. The proposed approach yields population estimates within 22% of those obtained using traditional methods.

</details>


### [66] [Q-Hawkeye: Reliable Visual Policy Optimization for Image Quality Assessment](https://arxiv.org/abs/2601.22920)
*Wulin Xie,Rui Dai,Ruidong Ding,Kaikui Liu,Xiangxiang Chu,Xinwen Hou,Jie Wen*

Main category: cs.CV

TL;DR: 提出Q-Hawkeye，一种基于RL的可靠视觉策略优化框架，通过不确定性感知动态优化和感知感知优化提升图像质量评估的稳定性和视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的IQA方法存在两个问题：一是对训练样本的预测稳定性未加区分，统一优势加权放大噪声信号；二是过度依赖文本推理，忽视模型对图像内容的视觉感知能力。

Method: 引入不确定性感知动态优化，利用多轮rollout的预测方差重加权更新；构建退化图与原图配对，设计隐式感知损失，强制模型基于真实视觉证据进行质量判断。

Result: Q-Hawkeye在多个数据集上超越现有SOTA方法，具有更强的泛化能力和评估稳定性。

Conclusion: 通过联合优化不确定性感知与视觉感知，Q-Hawkeye显著提升RL-based IQA的可靠性与准确性。

Abstract: Image Quality Assessment (IQA) predicts perceptual quality scores consistent with human judgments. Recent RL-based IQA methods built on MLLMs focus on generating visual quality descriptions and scores, ignoring two key reliability limitations: (i) although the model's prediction stability varies significantly across training samples, existing GRPO-based methods apply uniform advantage weighting, thereby amplifying noisy signals from unstable samples in gradient updates; (ii) most works emphasize text-grounded reasoning over images while overlooking the model's visual perception ability of image content. In this paper, we propose Q-Hawkeye, an RL-based reliable visual policy optimization framework that redesigns the learning signal through unified Uncertainty-Aware Dynamic Optimization and Perception-Aware Optimization. Q-Hawkeye estimates predictive uncertainty using the variance of predicted scores across multiple rollouts and leverages this uncertainty to reweight each sample's update strength, stabilizing policy optimization. To strengthen perceptual reliability, we construct paired inputs of degraded images and their original images and introduce an Implicit Perception Loss that constrains the model to ground its quality judgments in genuine visual evidence. Extensive experiments demonstrate that Q-Hawkeye outperforms state-of-the-art methods and generalizes better across multiple datasets. The code and models will be made available.

</details>


### [67] [Semantic Leakage from Image Embeddings](https://arxiv.org/abs/2601.22929)
*Yiyi Chen,Qiongkai Xu,Desmond Eliott,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CV

TL;DR: 研究发现图像嵌入即使压缩后仍存在语义泄漏风险，提出SLImE框架可在无训练解码器情况下恢复语义信息。


<details>
  <summary>Details</summary>
Motivation: 挑战图像嵌入隐私风险低的假设，揭示压缩嵌入中语义邻域结构保留即可导致语义泄漏。

Method: 提出SLImE框架，结合本地训练的语义检索器与现成模型，无需任务特定解码器，从压缩嵌入中恢复语义信息。

Result: 在GEMINI、COHERE、NOMIC、CLIP等模型上验证，成功恢复标签、符号表示及连贯描述，证实语义泄漏普遍存在。

Conclusion: 图像嵌入的语义邻域保留构成根本性隐私漏洞，对现有隐私保护机制构成严峻挑战。

Abstract: Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1

</details>


### [68] [Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.22959)
*Anmin Wang,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Triage是一种无需训练的插件式框架，通过分层视觉预算分配来减少视频处理中的冗余，加速推理并节省内存，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 视频处理中数据冗余导致token序列过长，带来巨大计算负担。

Method: 采用两阶段分层预算机制：帧级预算筛选关键帧，令牌级预算优先保留核心令牌，再用MMR算法选取多样上下文令牌。

Result: 在多个视频推理基准上，Triage显著提升推理速度、降低内存占用，性能优于或持平基线方法。

Conclusion: Triage通过高效资源分配有效解决视频VLM的计算瓶颈，具有良好的实用性和扩展性。

Abstract: Vision-Language Models (VLMs) face significant computational challenges in video processing due to massive data redundancy, which creates prohibitively long token sequences. To address this, we introduce Triage, a training-free, plug-and-play framework that reframes video reasoning as a resource allocation problem via hierarchical visual budgeting. Its first stage, Frame-Level Budgeting, identifies keyframes by evaluating their visual dynamics and relevance, generating a strategic prior based on their importance scores. Guided by this prior, the second stage, Token-Level Budgeting, allocates tokens in two phases: it first secures high-relevance Core Tokens, followed by diverse Context Tokens selected with an efficient batched Maximal Marginal Relevance (MMR) algorithm. Extensive experiments demonstrate that Triage improves inference speed and reduces memory footprint, while maintaining or surpassing the performance of baselines and other methods on various video reasoning benchmarks.

</details>


### [69] [Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion](https://arxiv.org/abs/2601.22961)
*Dennis Sprute,Hanna Senke,Holger Flatt*

Main category: cs.CV

TL;DR: 使用Stable Diffusion生成数据提升不平衡数据集下收割机部件分割性能，Mean IoU提升至84.6%。


<details>
  <summary>Details</summary>
Motivation: 工业光学质检中缺陷样本稀少导致数据不平衡，传统方法如损失函数调整或简单数据增强效果有限。

Method: 采用Stable Diffusion和CycleGAN生成热图像中的收割机组件，用于数据扩增，提升监督学习性能。

Result: Stable Diffusion使分割性能提升4.6%，达到84.6%的Mean IoU，效果优于CycleGAN。

Conclusion: 生成式AI（尤其是Stable Diffusion）是缓解数据不平衡、提升工业缺陷检测性能的有效新方法。

Abstract: Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.

</details>


### [70] [Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI](https://arxiv.org/abs/2601.22990)
*Yinsong Wang,Thomas Fletcher,Xinzhe Luo,Aine Travers Dineen,Rhodri Cusack,Chen Qin*

Main category: cs.CV

TL;DR: 提出GaussianSVR，一种自监督的3D胎儿MR体积重建方法，无需真实标签，基于高斯表示和多分辨率策略实现高效高精度重建。


<details>
  <summary>Details</summary>
Motivation: 传统SVR方法耗时且需多组正交切片，现有学习方法依赖难以获取的真实标签，亟需自监督解决方案。

Method: 采用3D高斯表示目标体积，结合模拟正向切片获取模型实现自监督训练，并引入多分辨率策略联合优化高斯参数与空间变换。

Result: 在胎儿MR体积重建任务中，GaussianSVR性能优于基线方法。

Conclusion: GaussianSVR有效解决了无真实标签下的高效高精度3D重建问题，具有实用价值。

Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.

</details>


### [71] [Leveraging Multi-Rater Annotations to Calibrate Object Detectors in Microscopy Imaging](https://arxiv.org/abs/2601.23007)
*Francesco Campi,Lucrezia Tondo,Ekin Karabati,Johannes Betge,Marie Piraud*

Main category: cs.CV

TL;DR: 通过多标注者集成提升显微图像目标检测器的校准性


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测器在显微成像中置信度估计未校准，影响生物医学应用的可靠性

Method: 分别用单专家标注训练模型，集成预测以模拟共识，捕捉标注者间变异性

Result: 在结肠类器官数据集上，该方法提升校准性能，同时保持检测精度

Conclusion: 显式建模标注者分歧可提高生物医学图像检测器的可信度

Abstract: Deep learning-based object detectors have achieved impressive performance in microscopy imaging, yet their confidence estimates often lack calibration, limiting their reliability for biomedical applications. In this work, we introduce a new approach to improve model calibration by leveraging multi-rater annotations. We propose to train separate models on the annotations from single experts and aggregate their predictions to emulate consensus. This improves upon label sampling strategies, where models are trained on mixed annotations, and offers a more principled way to capture inter-rater variability. Experiments on a colorectal organoid dataset annotated by two experts demonstrate that our rater-specific ensemble strategy improves calibration performance while maintaining comparable detection accuracy. These findings suggest that explicitly modelling rater disagreement can lead to more trustworthy object detectors in biomedical imaging.

</details>


### [72] [One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs](https://arxiv.org/abs/2601.23041)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: OSGA提出一种单次优化的输入无关引导方法，显著缓解视觉语言模型的幻觉与安全问题，且开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有引导方法在效率与效果间存在权衡，且需多次优化；VLMs仍存在幻觉与安全缺陷，亟需轻量级解决方案。

Method: OSGA通过方差策略选取代表性样本，利用对比学习与生成锚点正则化，仅优化一个引导向量，推理时直接应用于指定层。

Result: 在多个基准上，单个OSGA引导向量即可显著降低幻觉、提升安全性，且无需修改模型参数，计算开销可忽略。

Conclusion: 一Shot引导（OSGA）是一种高效、可扩展的VLM优化方法，为可靠视觉语言模型提供了实用新路径。

Abstract: Vision Language Models (VLMs) achieve strong performance on multimodal tasks but still suffer from hallucination and safety-related failures that persist even at scale. Steering offers a lightweight technique to improve model performance. However, steering, whether input-dependent or input-independent, achieves a meaningful trade-off between efficiency and effectiveness. In this work, we observe that steering vectors can generalize across inputs when tasks share aligned semantic intent. Based on this insight, we propose \textbf{OSGA} (\textbf{O}ne-shot \textbf{S}teering with \textbf{G}enerative \textbf{A}nchor), an input-independent framework that improves model performance with a single optimization instance. OSGA first selects an informative sample via a variance-based data selection strategy and learns a single steering vector with a contrastive objective with generative anchor regularization. The resulting vector can be universally applied at a certain layer during inference time without modifying model parameters. Experiments across multiple benchmarks show that a single OSGA-optimized steering vector consistently improves hallucination mitigation and safety enhancement with negligible overhead, highlighting one-shot steering as a practical and scalable solution for reliable VLMs.

</details>


### [73] [HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation](https://arxiv.org/abs/2601.23064)
*Hari Krishna Gadi,Daniel Matos,Hongyi Luo,Lu Liu,Yongliang Wang,Yanfeng Zhang,Liqiu Meng*

Main category: cs.CV

TL;DR: 提出一种基于双曲空间地理实体层次的视觉定位方法，显著提升精度并减少存储需求


<details>
  <summary>Details</summary>
Motivation: 现有方法存在存储开销大、忽略地理连续性或细节模糊等问题

Method: 采用双曲空间嵌入地理实体层次（国家-区域-子区域-城市），通过Geo-加权对比学习融入哈弗赛恩距离

Result: 在OSV5M上仅用24万实体嵌入取代500万图像嵌入，平均测地误差降低19.5%，子区域准确率提升43%

Conclusion: 几何感知的层次化嵌入为全球图像定位提供了可扩展且概念新颖的解决方案

Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.

</details>


### [74] [Rethinking Transferable Adversarial Attacks on Point Clouds from a Compact Subspace Perspective](https://arxiv.org/abs/2601.23102)
*Keke Tang,Xianheng Liu,Weilong Peng,Xiaofei Wang,Daizong Liu,Peican Zhu,Can Lu,Zhihong Tian*

Main category: cs.CV

TL;DR: 提出CoSA框架，通过低维语义子空间生成跨模型可迁移的点云对抗攻击，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有点云对抗攻击依赖模型特定梯度或启发式方法，泛化能力差，难以迁移到未知架构。

Method: 将点云表示为类别原型的紧凑组合，在低秩子空间中优化对抗扰动，确保语义一致且与模型无关。

Result: 在多个数据集和网络架构上，CoSA显著优于现有迁移攻击方法，保持不可察觉性并抵御常见防御。

Conclusion: 通过语义子空间约束，CoSA实现了高效、通用的点云对抗迁移攻击，为安全评估提供新范式。

Abstract: Transferable adversarial attacks on point clouds remain challenging, as existing methods often rely on model-specific gradients or heuristics that limit generalization to unseen architectures. In this paper, we rethink adversarial transferability from a compact subspace perspective and propose CoSA, a transferable attack framework that operates within a shared low-dimensional semantic space. Specifically, each point cloud is represented as a compact combination of class-specific prototypes that capture shared semantic structure, while adversarial perturbations are optimized within a low-rank subspace to induce coherent and architecture-agnostic variations. This design suppresses model-dependent noise and constrains perturbations to semantically meaningful directions, thereby improving cross-model transferability without relying on surrogate-specific artifacts. Extensive experiments on multiple datasets and network architectures demonstrate that CoSA consistently outperforms state-of-the-art transferable attacks, while maintaining competitive imperceptibility and robustness under common defense strategies. Codes will be made public upon paper acceptance.

</details>


### [75] [Segment Any Events with Language](https://arxiv.org/abs/2601.23159)
*Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出SEAL框架，首次实现事件传感器的开放词汇实例分割，支持多粒度语义理解，性能与速度均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有事件传感器研究多局限于语义级理解，缺乏对开放词汇实例分割（OV-EIS）的系统探索。

Method: SEAL框架结合视觉提示，统一实现事件的实例级与部件级分割与开放词汇掩码分类，并构建四组多粒度基准数据集。

Result: SEAL在性能和推理速度上显著超越基线，且参数高效；附加版本可无需视觉提示实现通用时空OV-EIS。

Conclusion: SEAL是事件传感领域首个支持开放词汇多粒度分割的框架，推动了事件理解向更细粒度、更通用方向发展。

Abstract: Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL

</details>


### [76] [Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm](https://arxiv.org/abs/2601.23167)
*Xiangrui Liu,Haoxiang Li,Yezhou Yang*

Main category: cs.CV

TL;DR: Hi-Light是一种无需训练的高保真视频重光照框架，解决了光照闪烁和细节丢失问题，并提出首个光照一致性量化指标。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照方法面临缺乏评价指标、光照闪烁和细节退化三大挑战。

Method: 提出三种创新：基于光度先验的扩散引导、混合运动自适应光照平滑滤波器、LAB空间细节融合模块，同时引入Light Stability Score评价指标。

Result: Hi-Light在定性和定量上均超越现有方法，生成稳定且高细节的重光照视频。

Conclusion: Hi-Light实现了无训练、高分辨率、鲁棒的视频重光照，填补了评价体系空白。

Abstract: Video relighting offers immense creative potential and commercial value but is hindered by challenges, including the absence of an adequate evaluation metric, severe light flickering, and the degradation of fine-grained details during editing. To overcome these challenges, we introduce Hi-Light, a novel, training-free framework for high-fidelity, high-resolution, robust video relighting. Our approach introduces three technical innovations: lightness prior anchored guided relighting diffusion that stabilises intermediate relit video, a Hybrid Motion-Adaptive Lighting Smoothing Filter that leverages optical flow to ensure temporal stability without introducing motion blur, and a LAB-based Detail Fusion module that preserves high-frequency detail information from the original video. Furthermore, to address the critical gap in evaluation, we propose the Light Stability Score, the first quantitative metric designed to specifically measure lighting consistency. Extensive experiments demonstrate that Hi-Light significantly outperforms state-of-the-art methods in both qualitative and quantitative comparisons, producing stable, highly detailed relit videos.

</details>


### [77] [Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training](https://arxiv.org/abs/2601.23220)
*Anglin Liu,Ruichao Chen,Yi Lu,Hongxia Xu,Jintai Chen*

Main category: cs.CV

TL;DR: Med-Scout通过强化学习利用无标签医学图像中的几何逻辑，解决多模态大模型的几何盲区问题，显著提升医学视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在医学诊断中存在几何盲区，导致基于语言流畅性但违背几何事实的幻觉错误。

Method: 提出Med-Scout框架，通过三种代理任务（层级尺度定位、拓扑拼图重建、异常一致性检测）利用无标签图像学习几何约束，并以强化学习优化模型。

Result: 在新基准Med-Scout-Bench上超越主流模型40%以上，并在放射学与综合医学VQA任务中表现更优。

Conclusion: 几何感知的增强不仅缓解幻觉，还提升了模型整体医学理解能力，为无标注数据驱动的医学AI提供新路径。

Abstract: Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that "cures" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.

</details>


### [78] [Region-Normalized DPO for Medical Image Segmentation under Noisy Judges](https://arxiv.org/abs/2601.23222)
*Hamza Kalisch,Constantin Seibold,Jens Kleesiek,Ken Herrmann,Frederic Jonske*

Main category: cs.CV

TL;DR: 使用噪声判断信号进行医学图像分割的偏好优化，提出RN-DPO方法提升稳定性和性能，无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 密集像素标注成本高，而自动QC信号（如模型共识、不确定性）虽廉价但噪声大，传统偏好微调易受有害更新影响。

Method: 提出Region-Normalized DPO (RN-DPO)，根据掩码分歧区域大小归一化偏好更新，降低错误比较的影响力，提升优化稳定性。

Result: 在两个医学数据集上，RN-DPO相比标准DPO和强基线，在不增加标注的情况下显著提升持续性能与训练稳定性。

Conclusion: RN-DPO有效利用噪声QC信号进行高效分割微调，为无额外标注的医学图像学习提供了可靠方案。

Abstract: While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations.

</details>


### [79] [Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning](https://arxiv.org/abs/2601.23224)
*Xiangyu Zeng,Zhiqiu Zhang,Yuhan Zhu,Xinhao Li,Zikang Wang,Changlian Ma,Qingyu Zhang,Zizheng Huang,Kun Ouyang,Tianxiang Jiang,Ziang Yan,Yi Wang,Hongjie Zhang,Yali Wang,Limin Wang*

Main category: cs.CV

TL;DR: Video-o3通过迭代工具调用和自适应终止机制，显著提升长视频理解中的关键证据发现能力，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖均匀采样和单轮推理，难以在冗余视频中定位稀疏但关键的证据。

Method: 提出Task-Decoupled Attention Masking和Verifiable Trajectory-Guided Reward，结合Seeker-173K数据集，实现多轮工具调用与高效推理。

Result: 在MLVU上达到72.1%准确率，在Video-Holmes上达到46.5%，显著优于SOTA方法。

Conclusion: Video-o3验证了原生工具调用在长视频多跳推理中的有效性。

Abstract: Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.

</details>


### [80] [ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search](https://arxiv.org/abs/2601.23232)
*Tao Yu,Haopeng Jin,Hao Wang,Shenghua Chai,Yujia Yang,Junhao Gong,Jiaming Guo,Minghui Zhang,Xinlong Chen,Zhenghao Zhang,Yuxuan Zhou,Yanpei Gong,YuanCheng Liu,Yiming Ding,Kangwei Zeng,Pengfei Yang,Zhongtian Luo,Yufei Xiong,Shanbin Zhang,Shaoxiong Cheng,Huang Ruilin,Li Shuo,Yuxi Niu,Xinyuan Zhang,Yueya Xu,Jie Mao,Ruixuan Ji,Yaru Zhao,Mingchen Zhang,Jiabing Yang,Jiaqi Liu,YiFan Zhang,Hongzhu Yi,Xinming Wang,Cheng Zhong,Xiao Ma,Zhang Zhang,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出ShotFinder基准，用于开放域视频片段检索，揭示现有多模态大模型在时空与视觉属性上的显著性能缺口。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦文本或静态多模态场景，缺乏对含复杂时序结构与语义的开放域视频片段检索的系统性评估。

Method: 构建包含1210个高质量样本的ShotFinder基准，定义五类可控约束（时序、颜色、视觉风格、音频、分辨率），并提出三阶段文本驱动检索流程：查询扩展、视频检索、时序定位。

Result: 在多个闭源与开源模型上实验发现，人类性能仍有显著差距，时序定位较易，颜色与视觉风格挑战最大。

Conclusion: 开放域视频片段检索仍是多模态大模型亟待突破的关键能力。

Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.

</details>


### [81] [Structured Over Scale: Learning Spatial Reasoning from Educational Video](https://arxiv.org/abs/2601.23251)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 通过教育视频数据DoraVQA提升视觉语言模型的推理能力，仅用38小时儿童教育内容即可在多个基准上达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在简单推理任务（如计数、空间推理）上表现差，而儿童教育视频具有结构化教学信号，可弥补此缺陷。

Method: 构建DoraVQA数据集（5344个带时间戳的问答对），基于Qwen2/3模型使用GRPO方法进行微调，利用教育视频的“上下文-问题-停顿-回答”结构。

Result: 在DoraVQA上提升8-14分，CVBench达86.16% SOTA，并在Video-MME和NExT-QA上表现优异，证明从窄域教育内容可泛化至广域多模态理解。

Conclusion: 模型推理能力的提升不仅依赖数据规模，更依赖内容结构，结构化教育内容是训练鲁棒推理的关键。

Abstract: Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.

</details>


### [82] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出一种无需训练的测试时自适应方法TaTa，利用布朗距离协方差和属性增强提示，在不反向传播的情况下提升视觉语言模型的域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法计算开销大、依赖反向传播且多关注单模态，限制了视觉语言模型在真实场景中的应用。

Method: 采用布朗距离协方差捕捉线性与非线性依赖关系，结合属性增强提示、动态聚类和伪标签优化，实现无训练、无反向传播的模型自适应。

Result: 在多个数据集上显著降低计算成本，同时达到域迁移和跨数据集泛化的最先进性能。

Conclusion: TaTa为视觉语言模型提供了高效、稳定且无需训练的测试时自适应新范式。

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


### [83] [User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments](https://arxiv.org/abs/2601.23281)
*Junfeng Lin,Yanming Xiu,Maria Gorlatova*

Main category: cs.CV

TL;DR: 研究开放集目标检测在XR交互场景中对用户提示的鲁棒性，发现模糊提示显著降低性能，提示增强可大幅提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有OSOD模型在基准测试中表现良好，但在真实XR环境中面对用户模糊、不完整或过度详细的提示时，其鲁棒性尚未充分研究。

Method: 评估GroundingDINO和YOLO-E模型在真实XR图像上对四类提示（标准、不完整、过度详细、语用模糊）的响应，并使用视觉语言模型模拟用户提示行为，测试两种提示增强策略的效果。

Result: 模型在标准和不完整提示下表现稳定，在模糊提示下性能显著下降；过度详细提示主要影响GroundingDINO；提示增强使mIoU提升超55%，平均置信度提升41%。

Conclusion: 建议在XR环境中采用提示增强策略，并设计更鲁棒的提示方式以提升OSOD模型的交互实用性。

Abstract: Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.

</details>


### [84] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: 提出VideoGPA框架，通过几何偏好对齐提升视频扩散模型的3D一致性，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型缺乏对几何一致性的显式约束，导致物体变形和空间漂移。

Method: 利用几何基础模型自动生成稠密偏好信号，结合直接偏好优化（DPO）引导视频扩散模型学习3D一致性。

Result: 在不依赖人工标注的情况下，VideoGPA显著提升了时间稳定性、物理合理性和运动连贯性，性能超越现有SOTA方法。

Conclusion: 几何偏好对齐是一种高效自监督方法，能有效解决视频生成中的3D结构失真问题。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [85] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 在小数据集上，简单但领域相关的改进比复杂注意力机制更有效，显著提升多模态情绪识别性能。


<details>
  <summary>Details</summary>
Motivation: 探究复杂注意力机制是否在小数据集上提升多模态情绪识别性能，避免过拟合与预训练特征破坏。

Method: 对比三种模型：基线Transformer（M1）、因子化注意力机制（M2）和改进的CNN基线（M3），使用EAV数据集进行实验。

Result: M2因过拟合表现差5-13个百分点；改进的CNN（如添加delta MFCCs和频域EEG特征）显著提升准确率，M1通过领域预训练达到75.30%。

Conclusion: 在小规模情绪识别任务中，领域知识与合理实现比架构复杂性更有效。

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [86] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 本文提出一种混合量子机器学习模型，用于地球观测数据分类，结合多任务学习和量子卷积操作，提升特征提取效率。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据进入大数据时代，传统深度学习模型计算需求高，成为分析瓶颈，因此探索量子计算在EO数据分类中的应用潜力。

Method: 提出一种混合模型，融合多任务学习以优化数据编码，并引入位置权重模块与量子卷积操作提取有效特征。

Result: 在多个EO基准数据集上验证了模型有效性，并实验分析了其泛化能力及优势来源。

Conclusion: 量子机器学习在地球观测数据分析中展现出显著潜力，尽管当前量子设备有限，仍具备实用前景。

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [87] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 开发了首个临床EEG到语言的基础模型CELM，可自动生成多尺度临床报告，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 长时程EEG报告生成人工成本高，缺乏自动化模型。

Method: 构建大规模EEG-报告数据集，提出CELM模型，融合预训练EEG与语言模型，实现端到端多尺度报告生成。

Result: 有病史监督时ROUGE-1等指标提升70%-95%；零样本下得分0.43-0.52，远超基线0.17-0.26。

Conclusion: CELM是首个高效EEG报告生成模型，具备可扩展多模态学习能力，已开源。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [88] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: 提出FedAdaVR和FedAdaVR-Quant算法，通过自适应优化与方差缩减技术解决联邦学习中客户端参与不规律的问题，显著降低内存开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端参与不规律导致的误差最普遍但研究不足，现有方法未能有效解决。

Method: 提出FedAdaVR，利用历史客户端更新模拟缺席客户端参与，并结合方差缩减；进一步提出FedAdaVR-Quant，采用量化存储降低内存消耗。

Result: 理论证明可消除部分客户端参与误差，实验表明在IID和非IID设置下均优于现有方法，内存减少50%-87.5%且性能不变。

Conclusion: FedAdaVR系列算法有效缓解联邦学习中的客户端异构性问题，兼具高效性与实用性。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [89] [Causal Imitation Learning Under Measurement Error and Distribution Shift](https://arxiv.org/abs/2601.22206)
*Shi Bo,AmirEmad Ghassami*

Main category: cs.LG

TL;DR: 提出CausIL框架，通过因果建模处理观测噪声与分布偏移，提升离线模仿学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准行为克隆在状态观测含噪声且分布变化时会学习到虚假相关性，导致策略有偏。

Method: 基于近端因果推断，将噪声观测视为代理变量，构建具有因果解释的目标函数，并提供可识别条件与RKHS对抗估计方法。

Result: 在PhysioNet数据集上验证，CausIL相比BC基线显著提升对分布偏移的鲁棒性。

Conclusion: 因果建模能有效解决噪声观测与分布偏移下的离线模仿学习问题，实现无奖励、无交互的策略恢复。

Abstract: We study offline imitation learning (IL) when part of the decision-relevant state is observed only through noisy measurements and the distribution may change between training and deployment. Such settings induce spurious state-action correlations, so standard behavioral cloning (BC) -- whether conditioning on raw measurements or ignoring them -- can converge to systematically biased policies under distribution shift. We propose a general framework for IL under measurement error, inspired by explicitly modeling the causal relationships among the variables, yielding a target that retains a causal interpretation and is robust to distribution shift. Building on ideas from proximal causal inference, we introduce \texttt{CausIL}, which treats noisy state observations as proxy variables, and we provide identification conditions under which the target policy is recoverable from demonstrations without rewards or interactive expert queries. We develop estimators for both discrete and continuous state spaces; for continuous settings, we use an adversarial procedure over RKHS function classes to learn the required parameters. We evaluate \texttt{CausIL} on semi-simulated longitudinal data from the PhysioNet/Computing in Cardiology Challenge 2019 cohort and demonstrate improved robustness to distribution shift compared to BC baselines.

</details>


### [90] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: 提出LSFlow方法，通过潜在球面流策略在组合强化学习中兼顾策略表达力与可行性，平均提升20.6%性能。


<details>
  <summary>Details</summary>
Motivation: 组合动作空间因规模庞大且约束复杂，传统方法在通用性与表达力上存在权衡。

Method: 采用潜在球面流策略，在连续隐空间学习随机策略，并借助组合优化求解器保证动作可行性，同时在隐空间训练值网络并引入平滑贝尔曼算子。

Result: 在多个组合强化学习任务上平均超越现有方法20.6%。

Conclusion: LSFlow在保持策略表达力的同时，通过求解器驱动的可行性设计实现了高效稳定的组合RL学习。

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [91] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出DAJ，一种基于推理的LLM判官，通过双层数据重加权学习框架，在测试时扩展中实现更优的代码生成选择。


<details>
  <summary>Details</summary>
Motivation: 现有LLM判官训练面临分布偏移问题，包括难易问题不平衡、训练任务与评估基准不匹配、以及轨迹不一致，导致判官不可靠。

Method: 提出DAJ，采用可验证奖励的双层数据重加权学习框架，自动学习域级或实例级数据重要性权重，优化在元集上的泛化性能。

Result: DAJ在LiveCodeBench和BigCodeBench上达到SOTA性能，超越强基线模型和主流专有模型。

Conclusion: 这是首次将数据重加权应用于LLM作为判官的训练，无需人工启发式规则即可自动提升对难样本、分布内样本和轨迹一致样本的重视。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [92] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM通过函数化代码生成和元学习奖励校正，显著提升大语言模型在复杂编程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有PRM在代码生成中效果不佳，因代码缺乏清晰的步骤分解且部分解奖励噪声大。

Method: FunPRM将代码模块化为函数作为推理步骤，并引入基于元学习的奖励校正机制，利用单元测试的最终奖励净化部分奖励。

Result: 在LiveCodeBench和BigCodeBench上超越现有方法，与O4-mini结合达到SOTA，生成代码更可读可复用。

Conclusion: FunPRM有效解决代码生成中的奖励噪声与步骤模糊问题，是测试时扩展方法的有力改进。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [93] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 通过引入无学习的查询和值偏置打破注意力机制中的旋转对称性，显著提升简单优化器性能并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制存在冗余的旋转自由度，未被利用且可能影响优化效率和可解释性。

Method: 采用批量采样的无学习查询与值偏置，打破旋转对称性，引导注意力方向。

Result: 在124M参数Transformer上验证，提升SGDM、ECD等轻量优化器性能，逼近AdamW效果，并增强注意力头对语义词类的敏感性。

Conclusion: 微小但有理论依据的架构修改，可同时提升模型性能与可解释性。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [94] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析转化为二分类问题，利用表格基础模型通过上下文学习实现无需训练的生存预测，性能超越经典与深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型在分类和回归中表现优异，但难以直接处理生存分析中的右删失问题。

Method: 通过离散化事件时间，将静态和动态生存分析重构为一系列二分类问题，删失数据自然作为部分时间点标签缺失的样本，无需重新训练模型。

Result: 在53个真实数据集上，该方法显著优于经典和深度学习基线，多项生存指标平均表现更优。

Conclusion: 该分类框架有效利用现有表格基础模型进行生存分析，理论保证收敛至真实生存概率，具有广泛适用性。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [95] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出一种基于穿戴惯性传感器和张量机的低成本活动识别框架，显著提升老年人居家康复活动分类精度


<details>
  <summary>Details</summary>
Motivation: 因医疗资源有限，老年及脆弱人群依赖居家护理，常忽视治疗性锻炼，亟需自动化监测方案

Method: 使用加速度计和陀螺仪采集六类日常活动数据，对比逻辑回归、随机森林、SVM、k-NN与新型张量机（STM），利用张量表征保留时空运动动态

Result: SVM准确率93.33%，STM以96.67%测试准确率和98.50%交叉验证准确率显著优于传统方法

Conclusion: STM框架具有高精度与鲁棒性，适用于远程医疗、老年照护、瑜伽反馈等低资源场景，具备规模化推广潜力

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [96] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL是一种无需记忆的联邦持续学习算法，通过服务器端锚点抑制领域漂移，在保持通信和模型规模不变的前提下实现向后知识迁移和高效收敛。


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统面临数据分布漂移与隐私限制，现有方法缺乏对联邦域增量学习（FDIL）中向后知识迁移保证与部分参与下收敛率的理论支持。

Method: 在经典FedAvg基础上引入服务器端轻量级近端项（anchor），引导参与客户端更新向先前全局模型偏移，无需重放缓冲、合成数据或任务头。

Result: 理论证明SPECIAL实现了有界向后知识迁移（BKT）和首个部分参与FDIL的非凸收敛率O((E/NT)^(1/2))，实验验证其有效性。

Conclusion: SPECIAL为FDIL提供了简单、高效、理论保障的解决方案，显著优于现有方法，适用于动态联邦环境。

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [97] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 提出SurrogateSHAP，一种无需重训练的Shapley值近似方法，高效评估文本到图像模型中数据贡献者的价值。


<details>
  <summary>Details</summary>
Motivation: 为公平补偿数据贡献者并建立可持续的数据市场，需解决Shapley值计算中重训练成本高和组合爆炸问题。

Method: 利用预训练模型推理替代重训练，并通过梯度提升树近似效用函数，解析计算Shapley值。

Result: 在三个任务上均优于现有方法，显著降低计算开销，精准识别关键数据贡献者，并能定位临床图像中的虚假相关性。

Conclusion: SurrogateSHAP为生成模型的公平审计与安全监控提供了可扩展的解决方案。

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [98] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: RLOs是一种基于黎曼几何和控制理论的统一优化框架，系统性地推导出稳定高效的优化器，超越了传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有优化器多为启发式改进，缺乏统一理论框架，难以保证收敛性和稳定性。

Method: 将优化过程重构为黎曼流形上的离散时间受控动力系统，通过识别 Normally Attracting Invariant Manifold (NAIM) 并构造严格李雅普诺夫函数，实现收敛性证明与优化器生成。

Result: RLOs不仅统一了经典优化算法，还生成了新算法，在大规模基准测试中达到最优性能。

Conclusion: RLOs为机器学习优化提供了控制理论与几何语言的统一框架，实现了稳定、系统化的优化器设计。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [99] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 模型合并的成功取决于合并方法和任务对，研究发现子空间重叠和梯度对齐是通用前提。


<details>
  <summary>Details</summary>
Motivation: 当前对模型可合并性的理解不足，缺乏对合并方法与任务对之间依赖关系的系统分析。

Method: 使用线性优化和可解释的成对度量（如梯度L2距离），在四种合并方法中分析影响合并性能的因素。

Result: 发现不同方法的成功驱动因素差异大（仅46.7%指标重叠），但子空间重叠和梯度对齐始终是通用关键因素。

Conclusion: 应将子空间重叠和梯度对齐作为模型微调的明确目标，以提升合并效果。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [100] [ParalESN: Enabling parallel information processing in Reservoir Computing](https://arxiv.org/abs/2601.22296)
*Matteo Pinna,Giacomo Lagomarsini,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出ParalESN，通过复数域对角线性递归实现时间数据并行处理，显著提升RC效率与可扩展性


<details>
  <summary>Details</summary>
Motivation: 传统回声状态网络受限于串行处理和高维储备池的内存开销，难以规模化

Method: 引入基于复数空间对角线性递归的结构化算子，构建ParalESN，支持并行处理并保持ESN理论性质

Result: 在时间序列任务上达到与传统RC相当的精度，计算成本和能耗降低数个数量级

Conclusion: ParalESN为RC融入深度学习提供了高效、可扩展且理论坚实的新路径

Abstract: Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.

</details>


### [101] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 提出CP4Gen方法，通过基于聚类的密度估计为条件生成模型提供更鲁棒、可解释且结构简单的不确定性预测集。


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准的不确定性估计，影响其在高风险应用中的可信度。

Method: CP4Gen利用聚类为基础的密度估计，构建对离群点不敏感、结构更简单的预测集。

Result: 在合成与真实数据（如气候模拟）上，CP4Gen在预测集体积和结构简洁性上优于现有方法。

Conclusion: CP4Gen为条件生成模型提供了高效、可解释的不确定性量化工具，适用于需严格预测的场景。

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [102] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL 提出一种基于DAG和零知识证明的去中心化联邦学习框架，提升安全性、可扩展性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有集中式和去中心化联邦学习在可扩展性、安全性和更新验证方面存在挑战。

Method: 结合DAG账本、专用侧链和零知识证明，通过事件驱动智能合约和预言机验证本地模型更新，并内置挑战机制检测恶意行为。

Result: 在图像分类和语言建模任务中，ZK-HybridFL比Blade-FL和ChainFL收敛更快、精度更高、困惑度更低、延迟更小，且能抵御大量恶意节点与攻击，支持亚秒级链上验证与低Gas消耗。

Conclusion: ZK-HybridFL是一种可扩展、安全、高效的去中心化联邦学习解决方案，适用于多样化环境。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [103] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 提出贝叶斯工作流生成（BWG）框架，通过采样和逐步构建提升工作流生成性能，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作流生成方法多为优化问题，缺乏理论基础，亟需更严谨的框架。

Method: 将工作流生成建模为贝叶斯推断，提出BWG框架，使用并行前向模拟加权与序列内循环优化器逐步构建工作流。

Result: 在六个基准数据集上，BayesFlow比SOTA方法最高提升9个百分点，比零样本提示最高提升65个百分点。

Conclusion: BWG为工作流生成提供了理论严谨、无需训练的升级框架，显著提升性能。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [104] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: 提出一种通过逐层矩匹配传播高斯分布均值与协方差到深度神经网络的方法，显著提升不确定性推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长期存在的在深度网络中精确传播高斯矩的理论缺口，尤其针对常用激活函数（如ReLU、GeLU等）及其残差结构。

Method: 基于层间矩匹配的确定性方法，推导出多种激活函数的精确矩传播公式，适用于前馈与残差网络。

Result: 在随机网络中KL散度误差降低百万倍，在变分贝叶斯网络中相比现有方法提升百倍，且在真实数据上实现良好统计校准。

Conclusion: 该方法为深度网络中的不确定性量化提供了高效精确的确定性替代方案，具有理论严谨性与实用价值。

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [105] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 提出一种新的隐蔽攻击方法和防御机制BayesClean，提升对中毒攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对回归模型的中毒攻击鲁棒性关注不足，且威胁模型不切实际，难以应用于实际场景。

Method: 提出最优隐蔽攻击 formulation，引入目标归一化方法评估有效性与可检测性的权衡，并设计BayesClean防御机制。

Result: 所提攻击可绕过现有顶尖防御，BayesClean在隐蔽攻击和大量中毒点场景下显著优于以往防御方法。

Conclusion: 该工作为回归模型的对抗攻击与防御提供了更实用的框架，强调了隐蔽攻击的威胁与有效防御的必要性。

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [106] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: 提出PoSafeNet，通过偏序集结构处理异构安全约束，提升机器人系统的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对安全约束采用统一或固定优先级，导致不可行和脆弱行为，而实际安全需求具有异构性和部分优先关系。

Method: 将安全约束建模为偏序集（poset），提出可微神经安全层PoSafeNet，基于偏序一致的顺序投影实现自适应安全执行。

Result: 在多障碍导航、受限机器人操作和视觉自动驾驶中，PoSafeNet相比传统方法在可行性、鲁棒性和可扩展性上表现更优。

Conclusion: 偏序结构安全框架为复杂安全约束提供了更灵活、可扩展的建模与执行方式。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [107] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR基准评估大语言模型在材料科学中跨尺度几何泛化能力，揭示推理方式对幻觉与一致性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在材料科学中的行为在物理结构分布偏移下缺乏理解，亟需评估其跨尺度几何泛化能力。

Method: 提出SCALAR基准，基于DFT验证的晶体结构，构建从几原子到18000原子的约10万结构集，设计三任务：CIF到性质预测、物理引导的思维链推理、逆向检索。

Result: 模型在显式推理下常降低幻觉与误差，但稳定性与一致性下降，几何泛化不能仅由准确率推断。

Conclusion: 物理结构一致性与推理方式对模型性能至关重要，需超越传统准确率指标评估材料基础模型。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [108] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出SCIQL方法，在离线强化学习中通过风格条件化提升任务性能与风格对齐的平衡


<details>
  <summary>Details</summary>
Motivation: 现有方法因风格与奖励目标冲突及分布偏移，难以有效协调风格与任务性能

Method: 提出统一风格定义，构建SCIQL框架，结合后见重标记、价值学习与门控优势加权回归

Result: SCIQL在任务性能和风格保持上均优于现有离线方法

Conclusion: 该方法为风格条件化离线强化学习提供了有效且统一的解决方案

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [109] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 静态对齐评估无法保证模型更新后的对齐性，且黑盒测试无法检测隐藏的对抗行为，模型规模越大，潜在风险越高。


<details>
  <summary>Details</summary>
Motivation: 现有对齐研究依赖静态黑盒评估，但实践中模型频繁更新，导致已对齐模型可能在更新后出现对齐失效，亟需新的评估框架。

Method: 理论证明过参数化导致静态对齐无法保障更新后对齐性，并通过实证在隐私、越狱安全和行为诚实三大领域验证了隐藏对抗行为的存在。

Result: 通过标准黑盒测试的模型在单次良性更新后严重失准，且模型规模越大，隐藏对抗行为的能力越强。

Conclusion: 静态评估协议严重不足，必须建立面向更新后鲁棒性的新对齐评估范式。

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [110] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: PA-GP-UCB 利用高低保真度预测器和离线数据，提升贝叶斯优化的采样效率。


<details>
  <summary>Details</summary>
Motivation: 真实世界优化问题中，高成本真值评估（如人类评估）与低成本预测（如大语言模型）并存，且存在大量离线数据可利用，但传统方法未充分利用这些资源。

Method: 提出PA-GP-UCB算法，通过联合高斯过程后验的控制变量估计器校正预测偏差并降低不确定性。

Result: 理论上保持GP-UCB的后悔率，但前导常数更小，且由预测质量和数据覆盖度控制；实验上在合成和真实任务中显著快于基线方法。

Conclusion: PA-GP-UCB是一个通用且高效的框架，适用于高成本反馈下的假设生成。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [111] [FlowSymm: Physics Aware, Symmetry Preserving Graph Attention for Network Flow Completion](https://arxiv.org/abs/2601.22317)
*Ege Demirci,Francesco Bullo,Ananthram Swami,Ambuj Singh*

Main category: cs.LG

TL;DR: FlowSymm通过结合对称性保持操作与图注意力机制，精准恢复网络中缺失的流量，同时遵守守恒定律，在交通、电力、共享单车数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网络中缺失流量的恢复需严格满足局部守恒律，传统方法难以兼顾物理约束与数据驱动建模。

Method: FlowSymm结合三部分：(1) 无散度流的群作用，(2) 图注意力编码器学习对称操作权重，(3) 基于隐式双层优化的Tikhonov细化。先求最小范数无散解，再构建保持观测流量不变的正交基，用GATv2生成注意力权重，最后通过凸最小二乘优化补全。

Result: 在交通、电力和共享单车三个真实数据集上，FlowSymm在RMSE、MAE和相关性指标上均超越现有最优方法。

Conclusion: FlowSymm通过物理约束引导的注意力机制，实现了高精度、可解释的网络流量恢复，为数据缺失下的物理系统建模提供了新范式。

Abstract: Recovering missing flows on the edges of a network, while exactly respecting local conservation laws, is a fundamental inverse problem that arises in many systems such as transportation, energy, and mobility. We introduce FlowSymm, a novel architecture that combines (i) a group-action on divergence-free flows, (ii) a graph-attention encoder to learn feature-conditioned weights over these symmetry-preserving actions, and (iii) a lightweight Tikhonov refinement solved via implicit bilevel optimization. The method first anchors the given observation on a minimum-norm divergence-free completion. We then compute an orthonormal basis for all admissible group actions that leave the observed flows invariant and parameterize the valid solution subspace, which shows an Abelian group structure under vector addition. A stack of GATv2 layers then encodes the graph and its edge features into per-edge embeddings, which are pooled over the missing edges and produce per-basis attention weights. This attention-guided process selects a set of physics-aware group actions that preserve the observed flows. Finally, a scalar Tikhonov penalty refines the missing entries via a convex least-squares solver, with gradients propagated implicitly through Cholesky factorization. Across three real-world flow benchmarks (traffic, power, bike), FlowSymm outperforms state-of-the-art baselines in RMSE, MAE and correlation metrics.

</details>


### [112] [RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning](https://arxiv.org/abs/2601.23075)
*Yuexin Bian,Jie Feng,Tao Wang,Yijiang Li,Sicun Gao,Yuanyuan Shi*

Main category: cs.LG

TL;DR: 用离散化分类演员网络替代高斯演员，显著提升连续控制任务的性能


<details>
  <summary>Details</summary>
Motivation: 传统高斯演员和浅层MLP策略在噪声梯度下优化脆弱，需更稳健的策略表示

Method: 采用离散化分类演员网络，每维动作用 bins 分布表示，目标函数为交叉熵损失，并引入正则化网络

Result: 仅替换演员网络即可在多个连续控制基准上达到最优性能

Conclusion: 策略表示是在线策略优化的关键设计选择，离散化正则化演员显著提升稳定性和性能

Abstract: On-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.

</details>


### [113] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出首个用于LLM路由的联邦学习框架，解决私有数据分散与局部数据不足问题，提升路由性能。


<details>
  <summary>Details</summary>
Motivation: 现有路由器依赖中心化评估数据，但实际数据分散且涉及隐私，本地训练因数据有限而效果差。

Method: 引入联邦学习框架，支持参数化MLP和非参数化K-means路由器，利用客户端本地离线数据协同训练共享路由策略。

Result: 在两个基准上，联邦协作显著提升准确率-成本前沿，扩大有效模型覆盖并增强查询泛化能力，理论证明降低路由次优性。

Conclusion: 联邦学习为LLM路由提供了可行且高效的隐私保护解决方案，优于本地训练方法。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [114] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种基于矩阵分解的近似差分隐私持续均值估计方法，显著降低均方误差。


<details>
  <summary>Details</summary>
Motivation: 现有纯差分隐私方法导致估计噪声过大，限制实用性，需更实用的近似差分隐私方案。

Method: 采用矩阵分解机制，设计专用于均值估计的新型因子分解方法。

Result: 实现了渐近更低的均方误差界限，提升估计准确性与效率。

Conclusion: 所提方法在用户级差分隐私下显著优于现有纯差分隐私方法，更具实用价值。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [115] [Spatially-Adaptive Conformal Graph Transformer for Indoor Localization in Wi-Fi Driven Networks](https://arxiv.org/abs/2601.22322)
*Ayesh Abu Lehyeh,Anastassia Gharib,Safwan Wshah*

Main category: cs.LG

TL;DR: 提出SAC-GT框架，结合图变换器与空间自适应共形预测，实现高精度且具有不确定性量化能力的室内定位


<details>
  <summary>Details</summary>
Motivation: 现有基于图的模型虽能捕捉Wi-Fi接入点与设备的空间关系，但无法量化预测不确定性，难以满足实际部署需求

Method: 结合图变换器（GT）建模空间拓扑与信号动态，引入空间自适应共形预测（SACP）方法，生成环境自适应的置信区域

Result: 在大规模真实数据集上，SAC-GT达到最优定位精度，并提供空间自适应的统计置信保证

Conclusion: SAC-GT在保持高精度的同时，首次实现室内定位中的可靠不确定性量化，推动实际应用落地

Abstract: Indoor localization is a critical enabler for a wide range of location-based services in smart environments, including navigation, asset tracking, and safety-critical applications. Recent graph-based models leverage spatial relationships between Wire-less Fidelity (Wi-Fi) Access Points (APs) and devices, offering finer localization granularity, but fall short in quantifying prediction uncertainty, a key requirement for real-world deployment. In this paper, we propose Spatially-Adaptive Conformal Graph Transformer (SAC-GT), a framework for accurate and reliable indoor localization. SAC-GT integrates a Graph Transformer (GT) model that captures network's spatial topology and signal strength dynamics, with a novel Spatially-Adaptive Conformal Prediction (SACP) method that provides region-specific uncertainty estimates. This allows SAC-GT to produce not only precise two-dimensional (2D) location predictions but also statistically valid confidence regions tailored to varying environmental conditions. Extensive evaluations on a large-scale real-world dataset demonstrate that the proposed SAC-GT solution achieves state-of-the-art localization accuracy while delivering robust and spatially adaptive reliability guarantees.

</details>


### [116] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一种可扩展、可控的模型路由框架，通过预测模型成本与性能动态分配查询，实现成本与精度的灵活平衡。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法固定选择少量模型，难以适应新模型或变化的预算约束。

Method: SCOPE基于强化学习，通过检索模型在类似问题上的行为来预测其成本与性能，而非依赖固定模型名称。

Result: SCOPE可在优先精度时提升最高25.7%准确率，或在优先效率时降低最高95.1%成本。

Conclusion: SCOPE超越传统模型选择，实现动态、可调控的推理路由，适应多样用户需求。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [117] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore通过LLM引导和确定性验证，生成符合临床部署要求的可解释评分系统，在多个临床任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型虽性能强，但因不满足临床工作流的可记忆性、可审计性和床边可执行性而难以落地，问题在于优化的模型类与指南部署需求不匹配。

Method: AgentScore利用大语言模型生成语义合理的候选规则，并通过确定性数据验证与选择循环，确保统计有效性与部署约束。

Result: 在八个临床预测任务中，AgentScore优于现有评分生成方法，AUC媲美更灵活的可解释模型；在两个外部验证任务中，性能超越现有指南评分。

Conclusion: 通过结构化约束与语义引导的联合优化，可生成既高效又临床可部署的简约评分系统。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [118] [Label-Efficient Monitoring of Classification Models via Stratified Importance Sampling](https://arxiv.org/abs/2601.22326)
*Lupo Marsigli,Angel Lopez de Haro*

Main category: cs.LG

TL;DR: 提出基于分层重要性抽样（SIS）的模型监控框架，在标签预算有限下显著提升估计效率。


<details>
  <summary>Details</summary>
Motivation: 生产环境中模型监控面临标签预算严格、批量获取标签和错误率极低的挑战。

Method: 采用分层重要性抽样（SIS）框架，理论证明其在有限样本下比传统重要性抽样和分层随机抽样具有更低的均方误差，且对非最优提议分布和分层具有鲁棒性。

Result: 在二分类和多分类任务中，SIS在固定标签预算下 consistently 提升估计效率，表现优于IS和SRS。

Conclusion: SIS是一种原则性强、标签高效且操作轻量的模型部署后监控方法，适用于真实生产场景。

Abstract: Monitoring the performance of classification models in production is critical yet challenging due to strict labeling budgets, one-shot batch acquisition of labels and extremely low error rates. We propose a general framework based on Stratified Importance Sampling (SIS) that directly addresses these constraints in model monitoring. While SIS has previously been applied in specialized domains, our theoretical analysis establishes its broad applicability to the monitoring of classification models. Under mild conditions, SIS yields unbiased estimators with strict finite-sample mean squared error (MSE) improvements over both importance sampling (IS) and stratified random sampling (SRS). The framework does not rely on optimally defined proposal distributions or strata: even with noisy proxies and sub-optimal stratification, SIS can improve estimator efficiency compared to IS or SRS individually, though extreme proposal mismatch may limit these gains. Experiments across binary and multiclass tasks demonstrate consistent efficiency improvements under fixed label budgets, underscoring SIS as a principled, label-efficient, and operationally lightweight methodology for post-deployment model monitoring.

</details>


### [119] [Molecular Representations in Implicit Functional Space via Hyper-Networks](https://arxiv.org/abs/2601.22327)
*Zehong Wang,Xiaolong Han,Qi Yang,Xiangru Tang,Fang Wu,Xiaoguang Guo,Weixiang Sun,Tianyi Ma,Pietro Lio,Le Cong,Sheng Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 将分子建模为三维空间中的连续函数，提出MolField框架，提升分子表示的泛化能力和物理一致性


<details>
  <summary>Details</summary>
Motivation: 现有方法将分子视为离散对象，忽视了其固有的连续和场状物理本质

Method: 提出MolField，基于超网络建模分子场的分布，使用标准化坐标确保SE(3)不变性，通过结构化权重分词训练序列超网络

Result: 在分子动力学和性质预测任务上表现优异，表示对离散化方式鲁棒，泛化能力更强

Conclusion: 将分子视为连续函数可根本性改善分子表示的学习方式，提升任务稳定性和泛化性能

Abstract: Molecular representations fundamentally shape how machine learning systems reason about molecular structure and physical properties. Most existing approaches adopt a discrete pipeline: molecules are encoded as sequences, graphs, or point clouds, mapped to fixed-dimensional embeddings, and then used for task-specific prediction. This paradigm treats molecules as discrete objects, despite their intrinsically continuous and field-like physical nature. We argue that molecular learning can instead be formulated as learning in function space. Specifically, we model each molecule as a continuous function over three-dimensional (3D) space and treat this molecular field as the primary object of representation. From this perspective, conventional molecular representations arise as particular sampling schemes of an underlying continuous object. We instantiate this formulation with MolField, a hyper-network-based framework that learns distributions over molecular fields. To ensure physical consistency, these functions are defined over canonicalized coordinates, yielding invariance to global SE(3) transformations. To enable learning directly over functions, we introduce a structured weight tokenization and train a sequence-based hyper-network to model a shared prior over molecular fields. We evaluate MolField on molecular dynamics and property prediction. Our results show that treating molecules as continuous functions fundamentally changes how molecular representations generalize across tasks and yields downstream behavior that is stable to how molecules are discretized or queried.

</details>


### [120] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT通过知识引导的核状态重建，从噪声数据中恢复物理一致的轨迹与导数，提升符号回归性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声、部分观测下表现不佳，且依赖黑箱模型，难以揭示物理机制。

Method: MAAT在再生核希尔伯特空间中进行状态重建，融入非负性、守恒律等先验知识，支持异构采样与测量粒度。

Result: 在12个科学基准和多种噪声条件下，MAAT显著降低状态与导数的估计误差，优于强基线方法。

Conclusion: MAAT为碎片化传感器数据与符号回归提供了物理一致的桥梁，推动数据驱动的科学发现。

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [121] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS是一种可扩展的批次校正方法，用于解决Cell Painting数据中的批次效应问题，通过局部相似性和自适应采样实现高效对齐。


<details>
  <summary>Details</summary>
Motivation: Cell Painting数据因实验室、仪器和协议差异产生强批次效应，掩盖生物信号，现有方法在规模下效率低或效果差。

Method: BALANS通过批次感知的局部尺度构建稀疏亲和矩阵，并采用自适应采样策略保留最强相似性，实现近线性时间复杂度的批次校正。

Result: BALANS在真实和合成数据集上优于主流方法，显著提升运行效率，同时保持或提升校正质量。

Conclusion: BALANS是一种高效、可扩展的批次校正工具，适用于大规模Cell Painting数据，为药物发现提供可靠支持。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [122] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种仅关联前一迭代噪声的DP-SGD新方法，无需存储历史噪声，内存开销与标准DP-SGD相同，且精度更高。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵分解机制虽提升精度，但需存储大量历史噪声，导致高内存开销。

Method: 引入仅关联前一迭代的噪声相关策略，通过伪随机噪声生成器再生噪声，控制并抵消部分噪声。

Result: 无需额外内存，计算开销小，实验表明精度优于标准DP-SGD。

Conclusion: 该方法在不增加内存负担的前提下，有效提升了DP-SGD的模型精度。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [123] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文推导了偏好贝叶斯优化中知识梯度的精确解析形式，显著提升性能，但也揭示了其在某些场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统知识梯度在直接函数评估中表现良好，但在仅能进行成对比较的偏好贝叶斯优化中因后验计算困难而难以应用。

Method: 推导出偏好贝叶斯优化中知识梯度的精确解析表达式，避免了近似计算。

Result: 精确知识梯度在多个基准问题上表现优异，常优于现有采集函数，但也存在某些场景下的性能不足。

Conclusion: 该方法为偏好BO提供了新的理论工具，虽有局限但整体性能突出。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [124] [Quantum-Inspired Reinforcement Learning for Secure and Sustainable AIoT-Driven Supply Chain Systems](https://arxiv.org/abs/2601.22339)
*Muhammad Bilal Akram Dastagir,Omer Tariq,Shahid Mumtaz,Saif Al-Kuwari,Ahmed Farouk*

Main category: cs.LG

TL;DR: 本文提出一种量子启发强化学习框架，协同优化供应链的碳足迹、库存管理和安全性，显著提升可持续性与抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 传统供应链优化模型忽视可持续性目标和网络安全漏洞，易导致生态损害与恶意攻击风险。

Method: 设计量子启发强化学习框架，结合自旋链类比与AIoT实时信号，构建融合保真度、安全性和碳成本的多目标奖励函数，采用值函数与集成更新稳定训练，并引入窗口归一化奖励缩放。

Result: 仿真表明该方法收敛平滑、后期性能优异，且在噪声环境下表现稳健，超越传统学习与模型基线方法。

Conclusion: 量子启发AIoT框架为实现安全、环保的大规模智能供应链提供了新路径，推动负责任的全球基础设施建设。

Abstract: Modern supply chains must balance high-speed logistics with environmental impact and security constraints, prompting a surge of interest in AI-enabled Internet of Things (AIoT) solutions for global commerce. However, conventional supply chain optimization models often overlook crucial sustainability goals and cyber vulnerabilities, leaving systems susceptible to both ecological harm and malicious attacks. To tackle these challenges simultaneously, this work integrates a quantum-inspired reinforcement learning framework that unifies carbon footprint reduction, inventory management, and cryptographic-like security measures. We design a quantum-inspired reinforcement learning framework that couples a controllable spin-chain analogy with real-time AIoT signals and optimizes a multi-objective reward unifying fidelity, security, and carbon costs. The approach learns robust policies with stabilized training via value-based and ensemble updates, supported by window-normalized reward components to ensure commensurate scaling. In simulation, the method exhibits smooth convergence, strong late-episode performance, and graceful degradation under representative noise channels, outperforming standard learned and model-based references, highlighting its robust handling of real-time sustainability and risk demands. These findings reinforce the potential for quantum-inspired AIoT frameworks to drive secure, eco-conscious supply chain operations at scale, laying the groundwork for globally connected infrastructures that responsibly meet both consumer and environmental needs.

</details>


### [125] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 语言模型在有限交互预算下探索环境表现不佳，远逊于简单启发式方法，但通过并行执行和历史摘要可显著提升探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在交互式环境中因探索不足而表现低效，亟需系统评估与改进方法。

Method: 设计三个可控难度的参数化任务，涵盖连续与离散环境，评估主流模型并测试并行执行与历史摘要两种轻量干预策略。

Result: 模型普遍存在探索不足，性能弱于启发式基线，且随预算增长收益有限；并行执行和历史摘要显著提升探索效果。

Conclusion: 语言模型在交互探索中需引入结构化干预，而非单纯依赖预算扩展，未来应重视探索策略的显式设计。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [126] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: MixQuant通过感知块旋转的激活重分配，在不增加推理开销的前提下显著提升量化精度。


<details>
  <summary>Details</summary>
Motivation: 现有块旋转量化方法对块结构如何影响离群值抑制的理解不足，且未充分利用输入向量的几何特性。

Method: 提出MixQuant框架，通过贪婪算法重分配激活的ℓ1范数质量，使其在块间均匀分布，并将置换操作合并至Transformer权重中以避免推理开销。

Result: 在Llama3 1B INT4量化中，块大小为16时，MixQuant恢复了90%的全向量旋转困惑度，远超无置换方法的46%。

Conclusion: 输入向量的几何分布是离群值抑制的关键，通过块感知的激活重分配可显著提升量化性能。

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [127] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 通过学习策略的占用度量表示，实现测试时无训练的策略行为引导


<details>
  <summary>Details</summary>
Motivation: 传统方法需重新训练才能适应新约束，希望在测试时直接通过潜在空间优化实现策略动态调整

Method: 将策略表示为状态-动作特征图的期望，使用集合架构编码样本，结合变分生成与对比学习构建与价值函数差异对齐的潜在空间

Result: 在潜在空间中实现梯度优化，成功完成未见过价值函数约束下的行为合成任务

Conclusion: 该方法无需额外训练即可灵活引导策略行为，为测试时策略调整提供了高效新范式

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [128] [Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents](https://arxiv.org/abs/2601.22352)
*Sri Vatsa Vuddanti,Satwik Kumar Chittiprolu*

Main category: cs.LG

TL;DR: 提出可量化的恢复规律，证明语言模型代理的自我恢复能力受动力学规律支配，而非模型规模或架构的偶然现象。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在工具调用失败后表现出自我恢复能力，但缺乏正式理论解释。

Method: 定义期望恢复遗憾（ERR）量化恢复策略与最优策略的偏差，推导出ERR与效率得分（ES）的一阶关系，形成可证伪的恢复动力学定律。

Result: 在五个工具使用基准上验证，ERR-ES定律预测的遗憾与蒙特卡洛模拟观测值高度一致（误差≤0.05）。

Conclusion: 恢复能力是交互动力学的固有属性，为语言代理的执行鲁棒性提供了理论基础。

Abstract: Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability through Expected Recovery Regret (ERR), which quantifies the deviation of a recovery policy from the optimal one under stochastic execution noise, and derive a first-order relationship between ERR and an empirical observable quantity, the Efficiency Score (ES). This yields a falsifiable first-order quantitative law of recovery dynamics in tool-using agents. We empirically validate the law across five tool-use benchmarks spanning controlled perturbations, diagnostic reasoning, and real-world APIs. Across model scales, perturbation regimes, and recovery horizons, predicted regret under the ERR-ES law closely matched observed post-failure regret measured from Monte Carlo rollouts, within delta less than or equal to 0.05. Our results reveal that recoverability is not an artifact of model scale or architecture, but a governed property of interaction dynamics, providing a theoretical foundation for execution-level robustness in language agents.

</details>


### [129] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 本文在最优传输框架下，通过几何方法提出两个新指标衡量经验分布与高斯分布的偏离程度，证明其优于矩匹配方法，并在FID评估中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统矩匹配高斯近似在Wasserstein距离下并非最优，需更合理的非高斯性度量方法。

Method: 利用相对平移不变二次Wasserstein空间的锥几何，引入相对Wasserstein角与正交投影距离，提出高斯逼近的投影视角，并发展高效随机流形优化算法。

Result: 在一维推导出闭式解，扩展至均匀、拉普拉斯等分布；高维下算法高效，实验表明相对Wasserstein角比Wasserstein距离更鲁棒，新高斯近似优于矩匹配。

Conclusion: 该几何方法为非高斯性量化提供了新框架，显著改进了高斯近似质量与FID评估准确性。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [130] [Small Talk, Big Impact: The Energy Cost of Thanking AI](https://arxiv.org/abs/2601.22357)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 论文量化了与大语言模型交互时使用礼貌用语（如“谢谢”）所增加的能源成本，强调其对可持续AI部署的重要性。


<details>
  <summary>Details</summary>
Motivation: 用户常使用礼貌用语与大语言模型交互，但这类看似无害的输入会增加能源消耗，亟需量化其环境影响。

Method: 通过真实对话数据和细粒度能源测量，分析输入长度、输出长度和模型规模对能耗的影响。

Result: 礼貌用语显著增加能耗，且能耗随输入/输出长度和模型规模线性增长。

Conclusion: 优化输入长度、减少冗余礼貌表达可显著降低LLM应用的能源足迹，推动更可持续的AI部署。

Abstract: Being polite is free - or is it? In this paper, we quantify the energy cost of seemingly innocuous messages such as ``thank you'' when interacting with large language models, often used by users to convey politeness. Using real-world conversation traces and fine-grained energy measurements, we quantify how input length, output length and model size affect energy use. While politeness is our motivating example, it also serves as a controlled and reproducible proxy for measuring the energy footprint of a typical LLM interaction. Our findings provide actionable insights for building more sustainable and efficient LLM applications, especially in increasingly widespread real-world contexts like chat. As user adoption grows and billions of prompts are processed daily, understanding and mitigating this cost becomes crucial - not just for efficiency, but for sustainable AI deployment.

</details>


### [131] [The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples](https://arxiv.org/abs/2601.22359)
*Hsiang Hsu,Pradeep Niroula,Zichang He,Ivan Brugere,Freddy Lecue,Chun-Fu Chen*

Main category: cs.LG

TL;DR: 提出残余知识漏洞并设计RURK方法缓解机器遗忘中的隐私风险


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法无法保证对抗扰动下模型输出的不可区分性，导致遗忘样本的局部邻域仍可能被识别，带来隐私泄露风险

Method: 提出RURK微调策略，通过惩罚模型重新识别扰动遗忘样本的能力来抑制残余知识

Result: 实验表明残余知识在现有遗忘方法中普遍存在，RURK能有效消除该风险且不影响模型性能

Conclusion: 高维场景下残余知识不可避免，RURK为机器遗忘提供了更安全的保障

Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge.

</details>


### [132] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 系统级设计（如精度、批处理、调度）对LLM推理能耗影响巨大，远超模型内部优化；合理调度可降耗百倍。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单次提示或单标记的能耗，忽视系统级设计对能耗的决定性影响，亟需系统性分析以实现绿色AI部署。

Method: 在NVIDIA H100 GPU上实证分析量化、批大小和推理服务配置（如Hugging Face TGI）对能耗与延迟的影响。

Result: 低精度仅在计算受限时省电；批处理在解码等内存受限阶段显著提升能效；请求调度（arrival shaping）可使单请求能耗降低达100倍。

Conclusion: 可持续LLM部署依赖于服务栈的系统级优化，应推动相位感知的能耗分析与调度设计。

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [133] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE是一种无需训练的多保真度回归框架，通过表基础模型实现零样本贝叶斯推断，显著提升效率与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在数据极度不平衡时存在计算成本高和过拟合问题，限制了其在现实应用中的效率和泛化能力。

Method: FIRE耦合表基础模型，利用低保真模型的后验预测分布作为条件，构建保真度校正模型，实现分布摘要驱动的残差学习，无需重新训练。

Result: 在31个合成与真实基准任务上，FIRE在准确率、不确定性量化和运行时间上均优于7种现有方法，表现最佳。

Conclusion: FIRE通过分布信息迁移有效解决了多保真度回归中的数据不平衡问题，但受限于上下文窗口和预训练模型质量。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [134] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: PABLO是一种全智能、基于语言的生物黑箱优化方法，在分子设计和抗菌肽优化中表现优于现有方法，且具有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖结构数据，未能充分利用科学文献；LLMs仅被用于狭窄角色，限制了其在生物优化中的潜力。

Method: PABLO构建了一个层次化智能体系统，利用预训练于化学与生物文献的LLM，通过语言推理迭代生成和优化生物候选分子。

Result: 在GuacaMol和抗菌肽优化任务中，PABLO达到SOTA性能，提升样本效率与目标值，且在体外验证中表现出对抗耐药病原体的强活性。

Conclusion: PABLO通过全语言智能体框架，实现了更高效、更可解释、更符合现实约束的生物设计，为治疗发现提供了新范式。

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [135] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: 提出G-Substrate框架，通过共享图结构实现跨模态和任务的图表示积累，超越传统孤立任务学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在不同模态和任务中孤立学习图结构，导致结构规律重复构建而非累积，缺乏跨任务的表示复用。

Method: 提出G-Substrate框架，包含统一结构模式和交错角色训练策略，使同一图结构在多个任务中保持兼容并承担不同功能角色。

Result: 在多个领域、模态和任务上的实验表明，G-Substrate优于孤立任务学习和朴素多任务学习方法。

Conclusion: 图结构应作为持久且可累积的结构基底，跨任务共享表示能显著提升学习效率与性能。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [136] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR使用LLM作为在线强化学习控制器，无需离线训练即可实现多阶段ML推理管道的高效自动扩缩容，显著降低延迟和资源成本。


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道因资源异构、阶段耦合和动态瓶颈迁移而难以自动扩缩容。

Method: 采用LLM作为上下文强化学习控制器，结合帕累托主导奖励塑造、可证明的分离边际、 surprisal引导的经验检索和用户空间CUDA拦截实现细粒度GPU速率控制。

Result: 在四种ML服务管道和三种工作负载下，SAIR在P99延迟和有效资源成本上优于或持平基线，P99延迟最多降低50%，成本最多降低97%，瓶颈检测准确率达86%。

Conclusion: SAIR无需离线训练，通过在线学习和高效上下文管理，实现了高性能、低成本的多阶段ML推理自动扩缩容。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [137] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN是一种基于得分函数和积分梯度的可扩展方法，用于高维非线性因果模型中的异常根因归因，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式或反事实推理的方法在不确定性与高维依赖下表现不佳，亟需更鲁棒的根因分析方法。

Method: SIREN通过估计数据似然的得分函数，并利用积分梯度沿异常点到正常分布路径累积贡献，实现根因归因，满足Shapley公理与因果非对称性。

Result: 在合成图与真实云服务、供应链数据集上，SIREN在归因准确性和计算效率上均超越现有最优方法。

Conclusion: SIREN为高维、非线性、异方差因果模型提供了高效、不确定感知的根因归因框架，具有广泛应用前景。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [138] [Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.22409)
*Puyu Wang,Junyu Zhou,Philipp Liznerski,Marius Kloft*

Main category: cs.LG

TL;DR: 本研究首次系统分析了KAN的梯度下降训练动态、泛化与差分隐私特性，证明了对数级宽度在私有与非私有场景下的不同必要性。


<details>
  <summary>Details</summary>
Motivation: KAN作为MLP的结构化替代方案，其训练理论与隐私保障机制尚不明确，亟需建立统一的分析框架。

Method: 基于NTK可分假设，推导两层KAN在梯度下降下的优化与泛化界，并结合差分隐私分析噪声需求与效用上界。

Result: 证明了对数宽度足以实现1/T优化与1/n泛化率；在DP下，噪声需求与效用界达到经典下界√d/(nε)，并首次揭示私有训练中对数宽度的必要性。

Conclusion: KAN在私有训练中对宽度有更强约束，理论结果可指导网络设计与早停策略，填补了KAN理论与隐私保障的空白。

Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and derive general bounds that characterize their training dynamics, generalization, and utility under differential privacy (DP). As a concrete instantiation, we specialize our analysis to logistic loss under an NTK-separable assumption, where we show that polylogarithmic network width suffices for GD to achieve an optimization rate of order $1/T$ and a generalization rate of order $1/n$, with $T$ denoting the number of GD iterations and $n$ the sample size. In the private setting, we characterize the noise required for $(ε,δ)$-DP and obtain a utility bound of order $\sqrt{d}/(nε)$ (with $d$ the input dimension), matching the classical lower bound for general convex Lipschitz problems. Our results imply that polylogarithmic width is not only sufficient but also necessary under differential privacy, revealing a qualitative gap between non-private (sufficiency only) and private (necessity also emerges) training regimes. Experiments further illustrate how these theoretical insights can guide practical choices, including network width selection and early stopping.

</details>


### [139] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出首个多模态联邦图学习基准MM-OpenFGL，涵盖19个数据集、8种模拟策略和57种方法，系统评估MMFGL的必要性、有效性、鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 现实中的多模态属性图（MMAGs）因隐私和商业限制无法集中训练，而现有联邦图学习研究多聚焦单模态，缺乏对多模态联邦场景的系统研究。

Method: 构建MM-OpenFGL基准，包含19个跨7个领域的多模态数据集、8种模态与拓扑变异模拟策略、6个下游任务，并实现57种SOTA方法的模块化API。

Result: 通过大规模实验系统评估了MMFGL在必要性、有效性、鲁棒性和效率方面的表现，为后续研究提供实证基础。

Conclusion: MM-OpenFGL填补了多模态联邦图学习的基准空白，推动该领域标准化与可复现研究。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [140] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead是一个全新的人工标注的机器学习排行榜数据集，提升结果透明度与评估精细度。


<details>
  <summary>Details</summary>
Motivation: 传统排行榜生成耗时且仅记录最佳结果，缺乏实验类型和数据集划分等元数据，限制了评估的透明性与多样性。

Method: 构建MetaLead，完整记录每篇论文的所有实验结果，标注实验类型（基线/提出方法/变体）并明确区分训练与测试数据集。

Result: MetaLead提供了更全面、可复现、可跨域比较的ML排行榜数据资源。

Conclusion: MetaLead显著提升了机器学习评估的透明性与细致性，为未来自动化排行榜生成与分析提供了高质量基础数据。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [141] [CoDCL: Counterfactual Data Augmentation Contrastive Learning for Continuous-Time Dynamic Network Link Prediction](https://arxiv.org/abs/2601.22427)
*Hantong Feng,Yonggang Wu,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出CoDCL框架，结合反事实数据增强与对比学习，提升动态网络预测性能。


<details>
  <summary>Details</summary>
Motivation: 动态网络结构持续演变，现有模型难以适应突发性结构变化，亟需鲁棒的自适应预测方法。

Method: CoDCL通过动态处理设计与结构邻域探索生成高质量反事实数据，结合对比学习进行表征学习，可无缝集成至现有时序图模型。

Result: 在多个真实数据集上，CoDCL显著超越现有基线模型，验证了反事实增强对动态表示学习的关键作用。

Conclusion: 反事实数据增强是提升动态网络模型鲁棒性与适应性的有效途径，CoDCL作为通用模块具有广泛适用性。

Abstract: The rapid growth and continuous structural evolution of dynamic networks make effective predictions increasingly challenging. To enable prediction models to adapt to complex temporal environments, they need to be robust to emerging structural changes. We propose a dynamic network learning framework CoDCL, which combines counterfactual data augmentation with contrastive learning to address this deficiency.Furthermore, we devise a comprehensive strategy to generate high-quality counterfactual data, combining a dynamic treatments design with efficient structural neighborhood exploration to quantify the temporal changes in interaction patterns.Crucially, the entire CoDCL is designed as a plug-and-play universal module that can be seamlessly integrated into various existing temporal graph models without requiring architectural modifications.Extensive experiments on multiple real-world datasets demonstrate that CoDCL significantly gains state-of-the-art baseline models in the field of dynamic networks, confirming the critical role of integrating counterfactual data augmentation into dynamic representation learning.

</details>


### [142] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种显式对比学习方法替代GRPO，通过将输出分为正负集直接最大化正样本似然，在数学基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: GRPO依赖经验性优化如不对称截断和零方差过滤，难以识别且需大量调参；希望设计更直接、稳定的推理增强方法。

Method: 将K个输出显式分为正负样本集，通过在线多标签噪声对比估计最大化正样本似然，无需优势估计。

Result: 在多个挑战性数学基准上，性能媲美DAPO和在线DPO等强基线。

Conclusion: 显式对比学习提供了一种更简单、有效且无需复杂经验调优的LLM推理增强方案。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [143] [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://arxiv.org/abs/2601.22442)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Gil Avraham,Hadi Mohaghegh Dolatabadi,Chamin P Hewa Koneputugodage,Violetta Shevchenko,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 通过异步更新减少分布式训练中的通信开销，同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 传统数据并行和流水线并行因高通信成本需依赖高速互联的同地集群，限制了扩展性

Method: 在数据并行和流水线并行中引入异步更新：流水线采用权重前瞻，数据并行采用异步稀疏平均加指数移动平均修正

Result: 在1B参数语言模型上，性能与同步基线相当，通信开销显著降低

Conclusion: 所提方法有效缓解通信瓶颈，支持非同地集群的高效扩展

Abstract: Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing asynchronous updates across both parallelism axes, relaxing the co-location requirement at the expense of introducing staleness between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an asynchronous sparse averaging method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to \em 1B parameters) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.

</details>


### [144] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 弱扩散模型在高信息量测量下仍能有效恢复信号，理论证明其可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究为何在使用与目标信号不匹配的低保真扩散模型时，逆问题求解器仍表现良好。

Method: 通过大量实验和基于贝叶斯一致性的理论分析，探讨弱先验成功的条件。

Result: 发现当测量信息量高（如观测像素多）时，弱先验性能接近强先验；并明确了其失效场景。

Conclusion: 高维测量能使后验集中于真实信号，为弱扩散先验的可靠使用提供理论依据。

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [145] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: 提出一种基于LLM的自动化系统，用于大规模生成和解决高质量预测问题，显著超越人工平台Metaculus的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动化预测系统依赖重复数据源，缺乏多样性和实用性，亟需更广泛、真实、可验证的预测问题生成方法。

Method: 使用LLM驱动的网页研究代理自动生成并解析1499个现实世界预测问题，评估其生成准确率、解析准确率及不同LLM的预测表现，并验证问题分解策略的有效性。

Result: 生成问题86%以上可验证明确，解析准确率达95%；Gemini 3 Pro表现最佳（Brier分数0.134），问题分解策略显著提升预测性能（Brier从0.141降至0.132）。

Conclusion: 该系统实现了高效率、高准确率的自动化预测问题生成与解决，为AI预测能力评估与提升提供了新范式。

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [146] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 提出一种基于权重的稀疏自编码器特征解释新框架，无需激活数据即可分析其功能作用。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过激活模式推断特征语义，忽略了特征在前向传播中承担的计算角色。

Method: 引入基于权重的解释框架，通过直接分析权重交互来衡量特征的功能效应。

Result: 在Gemma-2和Llama-3.1上发现：1/4特征直接预测输出令牌；特征参与注意力机制且具深度结构；语义与非语义特征在注意力回路中分布不同。

Conclusion: 该框架补全了稀疏自编码器特征可解释性的缺失部分，揭示了其功能层面的机制。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [147] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA通过堆栈采样和策略内池扩展提升LLM推理训练效率，以更少计算量达到更高精度。


<details>
  <summary>Details</summary>
Motivation: 传统提示池静态或与模型学习进度脱节，均匀采样导致资源浪费于已解决或过难的提示，现有方法难以支持稳定策略内池增长或引入额外开销。

Method: HeaPA采用堆栈边界采样追踪能力前沿，通过策略内增强异步验证扩展提示池，并结合拓扑感知重估与受控重插入稳定相关查询。

Result: 在两个语料、两种训练方法和七个基准上，HeaPA均提升准确率，以更少计算量达成目标性能，且耗时相当；模型规模越大，收益越明显。

Conclusion: HeaPA通过前沿聚焦采样与策略内池增长显著提升训练效率，尤其适用于大模型，代码已开源。

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [148] [Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity](https://arxiv.org/abs/2601.22450)
*Jianhao Huang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: Masked Diffusion语言模型在k-奇偶问题上避免了grokking现象，实现快速泛化，并通过优化掩码分布显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Masked Diffusion语言模型表现强大，但其泛化性质尚未被充分研究，尤其与自回归模型相比。

Method: 理论上将MD目标分解为信号与噪声区域，使用nanoGPT在k-奇偶问题上训练，并优化掩码概率分布。

Result: MD目标消除了grokking，实现同时快速泛化；在50M和8B模型上分别提升困惑度达8.8%和5.8%。

Conclusion: Masked Diffusion通过结构化目标设计显著改善语言模型的泛化效率与可扩展性。

Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.

</details>


### [149] [Temporal Graph Pattern Machine](https://arxiv.org/abs/2601.22454)
*Yijun Ma,Zehong Wang,Weixiang Sun,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出Temporal Graph Pattern Machine (TGPM)，通过时序偏置随机游走捕获多尺度演化模式，结合Transformer与自监督预训练，实现跨域迁移的图时序学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于短期依赖、静态邻域和回顾性时间使用，难以发现可迁移的时序演化机制。

Method: TGPM将交互建模为时序偏置随机游走生成的patch，用Transformer捕捉全局时序规律，并引入掩码标记建模与下一时刻预测自监督任务预训练。

Result: 在传递性和归纳性链接预测任务中均达到SOTA，展现出卓越的跨域迁移能力。

Conclusion: TGPM通过直接学习通用演化模式，突破了传统方法的限制，为动态图学习提供了新范式。

Abstract: Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability.

</details>


### [150] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: LOFT是一种在低维特征子空间中进行机器遗忘的新方法，通过主成分投影优化，仅需一次特征提取即可高效遗忘指定数据，降低隐私风险和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在隐私泄露风险和模型更新效率低的问题，因需反复访问原始数据并更新整个模型。

Method: LOFT在预训练模型的低维特征子空间中通过优化小型投影矩阵，最大化保留剩余数据信息，同时削弱遗忘数据信息，仅需一次特征提取。

Result: LOFT在多种模型、数据集和任务上显著降低了计算开销，并取得了更优的遗忘性能。

Conclusion: 低维特征子空间为机器遗忘提供了有效新视角，LOFT在效率与隐私保护方面优于主流方法。

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [151] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: 提出EvoEGF-Mol模型，基于信息几何与指数测地线流，提升结构药物设计的生成精度与稳定性


<details>
  <summary>Details</summary>
Motivation: 传统方法在欧氏空间与概率空间中分别建模原子坐标与化学类别，导致与真实统计流形不匹配

Method: 将分子建模为复合指数族分布，沿Fisher-Rao度量下的指数测地线生成，并引入动态浓缩分布替代静态Dirac目标，结合渐进参数优化架构

Result: 在CrossDock上达到93.4%的PoseBusters通过率，超越基线方法，在MolGenBench上成功恢复生物活性骨架并生成符合药物化学过滤条件的分子

Conclusion: EvoEGF-Mol通过信息几何方法实现了更精准的分子生成，显著提升药物设计的几何精度与相互作用保真度

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [152] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: LLMs表现出类似生物体的潜伏学习行为，在无奖励探索阶段即可积累知识，后续引入奖励后性能显著提升，超越纯奖励驱动的训练方式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练过度依赖外部奖励，缺乏灵活性与泛化能力，而心理学中的潜伏学习理论为提升模型学习效率提供新视角。

Method: 通过多模型、多任务的实验，在无奖励探索阶段观察LLM知识组织，再引入奖励评估性能提升，结合理论分析解释机制。

Result: LLM在无奖励阶段即有轻微性能提升，引入奖励后表现优于全程奖励驱动的模型，证实潜伏学习动态的存在。

Conclusion: 潜伏学习机制可有效应用于LLM训练，减少对奖励的依赖，提升泛化能力，为未来模型训练提供新范式。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [153] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出一种教师-学生框架，通过分布式RL训练单任务教师模型并持续蒸馏至通用学生模型，有效缓解持续强化学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL在连续任务流中难以扩展，因稳定性与可塑性难以平衡，而策略蒸馏作为稳定监督学习过程，更适合与基础模型结合进行多任务学习。

Method: 采用教师-学生框架，分布式训练单任务教师模型，通过基于回放缓冲和混合专家（MoE）架构的策略蒸馏持续更新通用学生模型。

Result: 在Meta-World基准上，学生模型恢复了超过85%的教师性能，任务间遗忘控制在10%以内。

Conclusion: 该框架通过解耦任务训练与模型蒸馏，实现了高效、可扩展的持续强化学习，为终身学习代理提供了新范式。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [154] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO通过增强问题多样性和聚合奖励，改善GRPO在推理中的退化问题，显著提升数学与科学推理性能。


<details>
  <summary>Details</summary>
Motivation: GRPO虽旨在提升推理能力，却因多样性坍塌和梯度衰减导致性能下降。

Method: TA-GRPO通过对问题进行语义等价变换（改写、变量重命名、格式调整）并聚合组内奖励来计算优势函数。

Result: 在AMC12、AIME24和GPQA-Diamond等基准上，Pass@k指标显著提升，最大提升达9.84和5.05点。

Conclusion: TA-GRPO通过减少零梯度概率和降低训练-测试分布偏移，增强模型泛化能力与多策略学习。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [155] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS是一种无需训练的框架，通过检测隐藏状态中的L2距离尖峰来识别推理中的认知惯性，并实时引导模型修正错误轨迹，提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖文本表面启发式规则，无法捕捉模型内部未表达的冲突，难以有效检测和纠正认知惯性。

Method: STARS通过监控潜变量动态，检测隐藏状态中的L2距离尖峰以识别认知转折点，并利用几何轨迹分析诊断结构特征，注入状态感知的语言提示进行实时引导。

Result: 在多个基准上验证，STARS能有效减少冗余推理循环，提升准确率，且无需微调。

Conclusion: STARS提供了一种鲁棒、无监督的机制，可优化大推理模型的推理过程，显著缓解认知惯性问题。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [156] [Elastic Spectral State Space Models for Budgeted Inference](https://arxiv.org/abs/2601.22488)
*Dachuan Song,Xuan Wang*

Main category: cs.LG

TL;DR: 提出ES-SSM，仅需一次训练即可在运行时动态裁剪为任意规模，无需重训且性能接近Transformer和SSM基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法需训练多个模型变体或使用模型蒸馏，无法实现运行时细粒度自适应，且资源消耗大。

Method: 基于Hankel谱滤波的SSM，结合轻量输入自适应门控与共享掩码归一化，使预测能力集中于低序谱分量，高序分量用于精细化。

Result: 在文本、逻辑、检索、视觉、音频等长序列基准上，单个ES-SSM模型裁剪后性能与主流模型相当，且预算-性能曲线平滑稳定。

Conclusion: ES-SSM实现了高效、灵活的运行时模型缩放，为资源受限场景提供了一种无需重训的新型解决方案。

Abstract: Foundation models are typically trained at a fixed computational capacity, while real-world applications require deployment across platforms with different resource constraints. Current approaches usually rely on training families of model variants or model distillation, which requires additional training and supports only a pre-selected set of sizes rather than fine-grained adaptation at runtime. In this paper, we propose Elastic Spectral State Space Models (ES-SSM), which require only one-time training at full capacity, but can be directly truncated into arbitrary scales for budgeted, runtime inference without retraining. Our ES-SSM builds on Hankel spectral filtering over a state space model (SSM), coupled with a lightweight input-adaptive gate trained under randomized spectral budgets. Using a shared masked normalization rule over the ordered spectral channels, we encourage predictive capability to concentrate in low-index components, while higher-index components act primarily as refinement. We test our algorithm across long-sequence benchmarks spanning text, logic, retrieval, vision, and audio. We demonstrate that a single ES-SSM model trained once can be truncated to provide competitive performance compared with modern Transformer and SSM baselines at similar parameter scales. Furthermore, by testing under various runtime budgets, we observe smooth and stable budget-performance curves over a wide range of truncation levels.

</details>


### [157] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: 提出渐进微调（GFT）方法，通过温度控制的中间目标平稳过渡预训练与目标分布，提升流模型在分布偏移下的微调稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统微调在数据有限、分布演化或效率要求严苛时易破坏预训练性能，现有奖励驱动方法对漂移结构或训练方式有严格限制。

Method: GFT通过温度控制的渐进序列连接预训练漂移与目标漂移，理论证明其在边际和条件目标下收敛，并支持最优传输耦合。

Result: GFT提高收敛稳定性、缩短概率路径、加速推理，同时保持与标准微调相当的生成质量。

Conclusion: GFT是理论严谨且实践有效的流模型自适应方法，适用于分布偏移下的可扩展微调。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [158] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 本文提出动作充分性准则，证明其比值充分性更关键，实验表明基于动作的表征在离线目标条件强化学习中性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设用于值函数估计的目标表征足以实现最优控制，但本文发现这种假设可能失败，因值充分性不能保证区分对动作选择至关重要的目标状态。

Method: 引入信息论框架定义动作充分性，证明其与值充分性的区别，并验证标准对数损失训练低层策略自然诱导动作充分表征。

Result: 在离散环境中，动作充分性与控制成功率关联更强；在基准实验中，基于动作的表征显著优于值估计学习的表征。

Conclusion: 目标表征应满足动作充分性而非仅值充分性，标准训练方法已隐式满足此条件，为离线GCRL提供了新设计原则。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [159] [Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks](https://arxiv.org/abs/2601.22509)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.LG

TL;DR: 提出DREE框架，解决神经VRP求解器在任务持续漂移下的终身学习问题，提升学习效率并缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有神经VRP求解器假设每个任务有充足训练数据，但现实中任务模式持续漂移且训练资源有限，亟需适应动态环境的终身学习方法。

Method: 提出Dual Replay with Experience Enhancement (DREE)，通过双重回放与经验增强机制提升学习效率并保留先验知识。

Result: 在持续漂移任务下，DREE有效学习新任务、保留旧知识、提升泛化能力，且可通用适配多种神经求解器。

Conclusion: DREE为神经VRP求解器在动态现实场景中的终身学习提供了高效、通用的解决方案。

Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world property: problem patterns may drift continually over time, yielding massive tasks sequentially arising while offering only limited training resources per task. In this paper, we study a novel lifelong learning paradigm for neural VRP solvers under continually drifting tasks over learning time steps, where sufficient training for any given task at any time is not available. We propose Dual Replay with Experience Enhancement (DREE), a general framework to improve learning efficiency and mitigate catastrophic forgetting under such drift. Extensive experiments show that, under such continual drift, DREE effectively learns new tasks, preserves prior knowledge, improves generalization to unseen tasks, and can be applied to diverse existing neural solvers.

</details>


### [160] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: Transformer模型在学习技能组合时表现为非人类的倒序或并行学习，导致组合破碎，且无法通过扩大模型规模或思维链缓解。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型为何表现出非人类的技能组合行为，及其背后的学习机制。

Method: 在合成算术任务上训练Transformer，通过大量消融实验和细粒度诊断指标分析学习动态。

Result: 模型学习技能时顺序混乱，出现组合破碎现象，由相关性匹配驱动而非因果或程序性组合，该问题在现代LLM中普遍存在。

Conclusion: 模型学习行为与期望的技能组合存在根本性不匹配，影响推理可靠性、分布外鲁棒性和对齐效果。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [161] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 本文提出一种无人机辅助可见光通信系统中的三维轨迹规划方法，通过优化飞行高度和引入信息素驱动奖励机制，显著减少飞行距离并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 为提升无人机辅助可见光通信系统中数据采集效率，需最小化无人机飞行距离，但该问题为复杂的混合整数非凸优化难题。

Method: 首先推导在特定信道增益阈值下的最优飞行高度解析解，然后结合信息素驱动奖励机制与TD3算法优化水平轨迹，实现复杂环境下的自适应运动策略。

Result: 最优高度可减少35%飞行距离，新奖励机制使收敛步数降低约50%，显著提升数据采集效率。

Conclusion: 所提方法在无人机辅助VLC系统中有效平衡了能效与通信性能，为动态环境下的轨迹规划提供了新思路。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [162] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: 提出SCOPE-PD框架，结合主客观数据，用随机森林实现98.66%准确率的帕金森病可解释预测


<details>
  <summary>Details</summary>
Motivation: 传统诊断主观性强、延迟诊断，现有客观分析缺乏解释性，ML方法多依赖主观报告且不可解释

Method: 基于PPMI数据，融合主观与客观评估，应用多种ML模型，采用SHAP分析可解释性，选择最优模型

Result: 随机森林模型准确率达98.66%，MDS-UPDRS中震颤、运动迟缓和面部表情为前三重要特征

Conclusion: SCOPE-PD框架显著提升PD预测准确性与可解释性，支持个性化健康决策

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [163] [Variational Bayesian Flow Network for Graph Generation](https://arxiv.org/abs/2601.22524)
*Yida Xiong,Jiameng Chen,Xiuwen Gong,Jia Wu,Shirui Pan,Wenbin Hu*

Main category: cs.LG

TL;DR: 提出变分贝叶斯流网络（VBFN），通过结构化精度矩阵实现节点与边的联合生成，提升图生成的保真度与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型和流匹配方法因因子化假设难以编码节点-边耦合关系，离散解码后表现脆弱；经典BFN虽支持离散生成，但依赖因子化信念，限制了几何证据融合。

Method: 提出VBFN，通过变分提升至可追踪的联合高斯信念族，利用结构化精度矩阵实现节点与边的单步联合更新，并基于表示诱导的依赖图构建稀疏精度矩阵，避免标签泄漏。

Result: 在合成与分子图数据集上，VBFN在生成保真度和多样性上均超越基线方法。

Conclusion: VBFN通过结构化信念建模有效实现了节点与边的耦合生成，为图生成提供了一种更稳健的贝叶斯流框架。

Abstract: Graph generation aims to sample discrete node and edge attributes while satisfying coupled structural constraints. Diffusion models for graphs often adopt largely factorized forward-noising, and many flow-matching methods start from factorized reference noise and coordinate-wise interpolation, so node-edge coupling is not encoded by the generative geometry and must be recovered implicitly by the core network, which can be brittle after discrete decoding. Bayesian Flow Networks (BFNs) evolve distribution parameters and naturally support discrete generation. But classical BFNs typically rely on factorized beliefs and independent channels, which limit geometric evidence fusion. We propose Variational Bayesian Flow Network (VBFN), which performs a variational lifting to a tractable joint Gaussian variational belief family governed by structured precisions. Each Bayesian update reduces to solving a symmetric positive definite linear system, enabling coupled node and edge updates within a single fusion step. We construct sample-agnostic sparse precisions from a representation-induced dependency graph, thereby avoiding label leakage while enforcing node-edge consistency. On synthetic and molecular graph datasets, VBFN improves fidelity and diversity, and surpasses baseline methods.

</details>


### [164] [Learn from A Rationalist: Distilling Intermediate Interpretable Rationales](https://arxiv.org/abs/2601.22531)
*Jiayi Dai,Randy Goebel*

Main category: cs.LG

TL;DR: 提出REKD方法，通过知识蒸馏提升小模型在理性提取任务中的预测性能


<details>
  <summary>Details</summary>
Motivation: 现有理性提取模型在弱能力神经网络上性能受限，因特征组合搜索空间大且训练困难

Method: REKD：学生模型通过教师模型的理性特征和预测结果进行知识蒸馏，增强学习效果

Result: 在IMDB、CIFAR10和CIFAR100上，REKD显著提升学生模型的预测性能

Conclusion: REKD是一种通用、可解释的知识蒸馏框架，兼容任意黑箱模型，有效提升小模型的理性提取能力

Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture where two neural networks learn jointly to perform feature selection and prediction, respectively. Given only the remote supervision from the final task prediction, the process of learning to select subsets of features (or \emph{rationales}) requires searching in the space of all possible feature combinations, which is computationally challenging and even harder when the base neural networks are not sufficiently capable. To improve the predictive performance of RE models that are based on less capable or smaller neural networks (i.e., the students), we propose \textbf{REKD} (\textbf{R}ationale \textbf{E}xtraction with \textbf{K}nowledge \textbf{D}istillation) where a student RE model learns from the rationales and predictions of a teacher (i.e., a \emph{rationalist}) in addition to the student's own RE optimization. This structural adjustment to RE aligns well with how humans could learn effectively from interpretable and verifiable knowledge. Because of the neural-model agnostic nature of the method, any black-box neural network could be integrated as a backbone model. To demonstrate the viability of REKD, we conduct experiments with multiple variants of BERT and vision transformer (ViT) models. Our experiments across language and vision classification datasets (i.e., IMDB movie reviews, CIFAR 10 and CIFAR 100) show that REKD significantly improves the predictive performance of the student RE models.

</details>


### [165] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文通过构建最小化基线，解耦强化微调中的设计选择，揭示各因素对学习与泛化的影响，并识别出关键设计。


<details>
  <summary>Details</summary>
Motivation: 强化微调领域论文激增但结论不一致，缺乏对设计选择作用的系统理解，亟需厘清哪些因素真正关键。

Method: 构建 minimalist 基线（单 rollout、无优势函数、batch size=32），连接带上下文的多臂老虎机学习，系统评估优势函数、rollout 数量等因素的边际增益。

Result: 揭示了不同设计选择对学习与泛化的具体作用，识别出若干关键因素，为未来研究提供清晰方向。

Conclusion: 设计选择的贡献需通过解耦实验量化，而非堆叠改进；部分因素远比其他更重要，应聚焦资源于关键变量。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [166] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出一种基于L2D-SLDS模型的非平稳时间序列学习延迟方法，通过动态专家注册和信息增益导向的路由规则提升性能。


<details>
  <summary>Details</summary>
Motivation: 处理非平稳时间序列中专家可用性变化与部分反馈的挑战，提升延迟学习的适应性与效率。

Method: 采用L2D-SLDS模型，结合因子化切换线性高斯状态空间、上下文依赖的模式转换、全局共享因子及专家个体状态，支持专家动态注册与剪枝；基于一步预测信念设计IDS启发的路由策略。

Result: 在实验中优于上下文多臂赌博机基线和无共享因子的消融模型。

Conclusion: 该方法有效利用共享信息与动态专家机制，在部分反馈和非平稳环境下显著提升学习与决策性能。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [167] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的采样算法，用于高效贝叶斯推断，结合模型基、模型自由和情景控制模块，提升大规模机器学习中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 人类通过模型基、模型自由和情景记忆系统高效学习，现有贝叶斯推断方法计算成本高，亟需更高效的采样机制。

Method: 设计三模块采样算法：模型基模块用目标分布进行精确但缓慢采样；模型自由模块学习历史样本模式实现快速反射采样；情景控制模块通过回忆past样本实现快速适应。

Result: 该算法显著提升贝叶斯推断效率，成功应用于贝叶斯深度学习，实现更准确的不确定性量化。

Conclusion: 生物启发的多系统采样框架为大规模贝叶斯机器学习提供了新范式，兼具效率与理论严谨性。

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [168] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出一种与模型无关的守恒量校正方法，提升神经算子在长期预测中的稳定性，并指出当前架构在高频分量建模上的不足。


<details>
  <summary>Details</summary>
Motivation: 深度学习在求解偏微分方程时因自回归误差累积和物理量不守恒而导致长期预测性能差。

Method: 引入守恒量校正技术，将物理守恒准则融入神经算子模型，无需修改架构即可提升稳定性，并从频域分析模型性能。

Result: 该方法显著提升不同架构模型的长期稳定性，揭示现有架构在高频分量建模上的严重局限性。

Conclusion: 未来工作应关注能更好捕捉高频分量的网络架构，以更准确地模拟湍流等复杂流动。

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [169] [EUGens: Efficient, Unified, and General Dense Layers](https://arxiv.org/abs/2601.22563)
*Sang Min Kim,Byeongchan Kim,Arijit Sehanobish,Somnath Basu Roy Chowdhury,Rahul Kidambi,Dongseok Shim,Avinava Dubey,Snigdha Chaturvedi,Min-hwan Oh,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出EUGens新型密集层，以线性复杂度替代二次复杂度的全连接层，显著提升推理速度与内存效率，同时保持表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统全连接层计算与参数量瓶颈限制神经网络在实时和资源受限环境中的扩展。

Method: 利用随机特征近似全连接层，引入输入范数依赖，统一现有高效扩展方法，提出无偏多项式激活近似算法，并设计无需反向传播的层间知识迁移技术。

Result: 在Transformer和MLP中集成EUGens，推理速度提升达27%，内存效率提升达30%，在图像分类、语言预训练和3D场景重建等任务中表现优异。

Conclusion: EUGens为大规模神经网络在现实场景中的可扩展部署提供了高效、通用且低开销的解决方案。

Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge, in this work, we propose a new class of dense layers that generalize standard fully-connected feedforward layers, \textbf{E}fficient, \textbf{U}nified and \textbf{Gen}eral dense layers (EUGens). EUGens leverage random features to approximate standard FFLs and go beyond them by incorporating a direct dependence on the input norms in their computations. The proposed layers unify existing efficient FFL extensions and improve efficiency by reducing inference complexity from quadratic to linear time. They also lead to \textbf{the first} unbiased algorithms approximating FFLs with arbitrary polynomial activation functions. Furthermore, EuGens reduce the parameter count and computational overhead while preserving the expressive power and adaptability of FFLs. We also present a layer-wise knowledge transfer technique that bypasses backpropagation, enabling efficient adaptation of EUGens to pre-trained models. Empirically, we observe that integrating EUGens into Transformers and MLPs yields substantial improvements in inference speed (up to \textbf{27}\%) and memory efficiency (up to \textbf{30}\%) across a range of tasks, including image classification, language model pre-training, and 3D scene reconstruction. Overall, our results highlight the potential of EUGens for the scalable deployment of large-scale neural networks in real-world scenarios.

</details>


### [170] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: FedDis通过因果解耦提升联邦学习中交通预测的性能，有效分离局部与全局模式。


<details>
  <summary>Details</summary>
Motivation: 现有联邦方法难以处理交通数据的非IID特性，因将全局模式与本地动态纠缠在单一表示中。

Method: FedDis采用双分支架构：个性化银行捕捉客户端特有因素，全局模式银行提取共同知识，并通过互信息最小化确保解耦。

Result: 在四个真实数据集上，FedDis均达到SOTA性能，兼具高效性与可扩展性。

Conclusion: 因果解耦是解决联邦交通预测中数据异构性的有效途径，FedDis为未来研究提供了新范式。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [171] [Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks](https://arxiv.org/abs/2601.22579)
*Sichen Zhao,Zhiming Xue,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络的非侵入式电商机器人检测框架，优于传统方法且具备鲁棒性和实时部署能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器人防御手段（如IP黑名单、CAPTCHA）因现代机器人使用代理、僵尸网络和AI规避策略而失效，亟需更高效、非侵入的检测方案。

Method: 构建用户会话行为图，采用归纳式图神经网络进行机器人分类，捕捉关系结构与行为语义。

Result: 在真实电商流量上，该方法在AUC和F1分数上优于MLP基线，且在对抗扰动和冷启动场景下保持鲁棒性与泛化能力。

Conclusion: 该框架无需客户端植入，支持实时推理与增量更新，适合实际电商安全部署。

Abstract: Malicious bots pose a growing threat to e-commerce platforms by scraping data, hoarding inventory, and perpetrating fraud. Traditional bot mitigation techniques, including IP blacklists and CAPTCHA-based challenges, are increasingly ineffective or intrusive, as modern bots leverage proxies, botnets, and AI-assisted evasion strategies. This work proposes a non-intrusive graph-based bot detection framework for e-commerce that models user session behavior through a graph representation and applies an inductive graph neural network for classification. The approach captures both relational structure and behavioral semantics, enabling accurate identification of subtle automated activity that evades feature-based methods. Experiments on real-world e-commerce traffic demonstrate that the proposed inductive graph model outperforms a strong session-level multilayer perceptron baseline in terms of AUC and F1 score. Additional adversarial perturbation and cold-start simulations show that the model remains robust under moderate graph modifications and generalizes effectively to previously unseen sessions and URLs. The proposed framework is deployment-friendly, integrates with existing systems without client-side instrumentation, and supports real-time inference and incremental updates, making it suitable for practical e-commerce security deployments.

</details>


### [172] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: 提出中位数中心化的GRPO方法（MC-GRPO），通过用中位数替代均值作为基线，显著提升小样本滚动训练的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限场景下，传统GRPO方法因共享均值基线受噪声影响导致优势符号翻转，降低模型训练准确性。

Method: 采用中位数作为奖励基线，生成G+1个滚动样本，以中位数作为零优势点，排除其梯度更新，保持G个样本的计算成本不变。

Result: 在多种GRPO方法、模型规模下，MC-GRPO显著提升低滚动数（如G=2）下的稳定性与精度，将G=2与G=8的性能差距压缩至1%以内。

Conclusion: 中位数基线能有效缓解小样本下的噪声敏感问题，是提升GRPO类方法效率与鲁棒性的简单且通用方案。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [173] [FedCARE: Federated Unlearning with Conflict-Aware Projection and Relearning-Resistant Recovery](https://arxiv.org/abs/2601.22589)
*Yue Li,Mingmin Chu,Xilei Yang,Da Xiao,Ziqi Xu,Wei Shao,Qipeng Song,Hui Li*

Main category: cs.LG

TL;DR: FedCARE提出了一种低开销、抗回滚的联邦遗忘框架，支持多种粒度的遗忘并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在开销高、知识纠缠导致性能下降和遗忘后回滚等问题，亟需高效且稳定的遗忘机制。

Method: FedCARE结合梯度上升与无数据模型反演，构建类别代理，采用冲突感知投影梯度上升进行遗忘，并设计抑制回滚的恢复策略。

Result: 在多个数据集和模型上，FedCARE在IID和非IID设置下均实现了有效遗忘、更好的效用保留和更低的回滚风险。

Conclusion: FedCARE是首个统一、低开销且抗回滚的联邦遗忘框架，显著优于现有方法。

Abstract: Federated learning (FL) enables collaborative model training without centralizing raw data, but privacy regulations such as the right to be forgotten require FL systems to remove the influence of previously used training data upon request. Retraining a federated model from scratch is prohibitively expensive, motivating federated unlearning (FU). However, existing FU methods suffer from high unlearning overhead, utility degradation caused by entangled knowledge, and unintended relearning during post-unlearning recovery. In this paper, we propose FedCARE, a unified and low overhead FU framework that enables conflict-aware unlearning and relearning-resistant recovery. FedCARE leverages gradient ascent for efficient forgetting when target data are locally available and employs data free model inversion to construct class level proxies of shared knowledge. Based on these insights, FedCARE integrates a pseudo-sample generator, conflict-aware projected gradient ascent for utility preserving unlearning, and a recovery strategy that suppresses rollback toward the pre-unlearning model. FedCARE supports client, instance, and class level unlearning with modest overhead. Extensive experiments on multiple datasets and model architectures under both IID and non-IID settings show that FedCARE achieves effective forgetting, improved utility retention, and reduced relearning risk compared to state of the art FU baselines.

</details>


### [174] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: 提出一种名为MGMT的统一多图学习框架，通过元图实现跨图推理，在保持可解释性的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效整合拓扑、规模和语义各异的异构图，尤其在节点无共享身份的情况下。

Method: 对每个图使用图变换器编码，通过注意力选择超节点，构建基于潜在空间相似性的元图，并在元图上进一步进行图变换器推理。

Result: 在合成数据和神经科学真实数据上，MGMT优于现有SOTA模型，且提供可解释的超节点与超边表示。

Conclusion: MGMT为结构化多图学习提供了统一框架，推动了图数据主导领域的表征学习进展。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [175] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: 提出Lethe方法，解决联邦遗忘中知识复现问题，实现持久遗忘


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法忽视训练继续时未学习知识可能复现，导致遗忘失效

Method: 采用Reshape--Rectify--Restore三阶段流程，通过临时适配器放大梯度并分离待遗忘与保留知识

Result: Lethe在多级遗忘任务中均有效，后续训练后知识复现率低于1%

Conclusion: Lethe实现了联邦场景下持久、统一的遗忘，显著提升遗忘稳定性

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [176] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 通过流映射视角分析一致性模型的不稳定性，提出改进的自蒸馏方法以稳定训练，并扩展至策略学习。


<details>
  <summary>Details</summary>
Motivation: 一致性模型训练不稳定且难以复现，现有解释碎片化，理论关系不清晰。

Method: 从流映射角度理论分析一致性模型，重构自蒸馏以控制梯度范数，避免次优收敛。

Result: 方法稳定了训练过程，成功应用于图像生成与无预训练扩散模型的策略学习。

Conclusion: 流映射视角为一致性模型提供了统一理论框架，自蒸馏重构显著提升稳定性和泛化性。

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [177] [Local-Global Multimodal Contrastive Learning for Molecular Property Prediction](https://arxiv.org/abs/2601.22610)
*Xiayu Liu,Zhengyi Lu,Yunhong Liao,Chan Fan,Hou-biao Li*

Main category: cs.LG

TL;DR: LGM-CL通过局部-全局多模态对比学习联合建模分子图和文本表示，显著提升分子性质预测性能。


<details>
  <summary>Details</summary>
Motivation: 准确的分子性质预测需要整合分子结构与化学语义的互补信息，现有方法未能充分协同局部功能基与全局拓扑信息，也缺乏对化学语义的无任务依赖建模。

Method: 提出LGM-CL框架，使用AttentiveFP和Graph Transformer分别编码局部功能基和全局拓扑，通过自监督对比学习对齐；引入化学增强文本与SMILES对比，融合双交叉注意力机制整合分子指纹。

Result: 在MoleculeNet基准上，LGM-CL在分类与回归任务中均实现一致且具有竞争力的性能，验证了局部-全局与多模态表示学习的有效性。

Conclusion: 统一建模分子结构与化学语义的多模态对比学习框架能显著提升分子属性预测的泛化能力与准确性。

Abstract: Accurate molecular property prediction requires integrating complementary information from molecular structure and chemical semantics. In this work, we propose LGM-CL, a local-global multimodal contrastive learning framework that jointly models molecular graphs and textual representations derived from SMILES and chemistry-aware augmented texts. Local functional group information and global molecular topology are captured using AttentiveFP and Graph Transformer encoders, respectively, and aligned through self-supervised contrastive learning. In addition, chemically enriched textual descriptions are contrasted with original SMILES to incorporate physicochemical semantics in a task-agnostic manner. During fine-tuning, molecular fingerprints are further integrated via Dual Cross-attention multimodal fusion. Extensive experiments on MoleculeNet benchmarks demonstrate that LGM-CL achieves consistent and competitive performance across both classification and regression tasks, validating the effectiveness of unified local-global and multimodal representation learning.

</details>


### [178] [SQUAD: Scalable Quorum Adaptive Decisions via ensemble of early exit neural networks](https://arxiv.org/abs/2601.22711)
*Matteo Gambella,Fabrizio Pittorino,Giuliano Casale,Manuel Roveri*

Main category: cs.LG

TL;DR: SQUAD引入基于共识的早期退出机制，结合分布式集成学习，显著降低推理延迟并提升精度。


<details>
  <summary>Details</summary>
Motivation: 传统单模型置信度阈值不可靠，因校准问题导致早期退出性能不佳。

Method: SQUAD通过多模型逐步收集中间预测，以共识投票决定提前退出；QUEST用于搜索具有层次多样性的早期退出子模型。

Result: 相比最优动态方法，精度提升5.95%；相比静态集成，延迟降低70.60%且保持高精度。

Conclusion: 共识驱动的早期退出框架有效解决了校准问题，实现高效高精度推理。

Abstract: Early-exit neural networks have become popular for reducing inference latency by allowing intermediate predictions when sufficient confidence is achieved. However, standard approaches typically rely on single-model confidence thresholds, which are frequently unreliable due to inherent calibration issues. To address this, we introduce SQUAD (Scalable Quorum Adaptive Decisions), the first inference scheme that integrates early-exit mechanisms with distributed ensemble learning, improving uncertainty estimation while reducing the inference time. Unlike traditional methods that depend on individual confidence scores, SQUAD employs a quorum-based stopping criterion on early-exit learners by collecting intermediate predictions incrementally in order of computational complexity until a consensus is reached and halting the computation at that exit if the consensus is statistically significant. To maximize the efficacy of this voting mechanism, we also introduce QUEST (Quorum Search Technique), a Neural Architecture Search method to select early-exit learners with optimized hierarchical diversity, ensuring learners are complementary at every intermediate layer. This consensus-driven approach yields statistically robust early exits, improving the test accuracy up to 5.95% compared to state-of-the-art dynamic solutions with a comparable computational cost and reducing the inference latency up to 70.60% compared to static ensembles while maintaining a good accuracy.

</details>


### [179] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 共识机制替代注意力机制，提升Transformer训练对学习率过大的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有方法多通过优化过程提升稳定性，但缺乏架构层面的创新，而注意力机制在高学习率下表现不稳定

Method: 提出共识机制作为注意力的替代方案，构建图模型并设计混合共识-注意力框架

Result: 在文本、DNA和蛋白质模态上验证了共识机制显著提升训练稳定性，同时保持性能

Conclusion: 共识机制是一种有效的架构级改进，能扩大有效学习率范围，为Transformer稳定训练提供新方向

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [180] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 利用视觉语言模型（VLMs）的常识推理能力，无监督地分离动作相关噪声，显著提升潜在动作模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有潜在动作模型（LAMs）在存在动作相关干扰物时表现不佳，而人类能根据简短任务描述轻易区分相关动作与噪声。

Method: 使用VLMs生成可提示的表征作为LAM训练目标，评估多种VLMs在不同提示和超参数下的表现，并测试通过指令忽略干扰物的效果。

Result: 较新的VLMs表现可能不如旧模型；通过提示让VLM忽略干扰物，可在Distracting MetaWorld上使下游任务成功率提升六倍。

Conclusion: VLMs的常识推理能力可有效提升LAM对噪声的鲁棒性，提示引导是改善潜在动作学习的关键。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [181] [TTCS: Test-Time Curriculum Synthesis for Self-Evolving](https://arxiv.org/abs/2601.22628)
*Chengyi Yang,Zhishang Xiang,Yunbo Tang,Zongpei Teng,Chengsong Huang,Fei Long,Yuhan Liu,Jinsong Su*

Main category: cs.LG

TL;DR: TTCS通过协同进化框架在测试时自动生成难题并自我优化，提升大模型推理能力


<details>
  <summary>Details</summary>
Motivation: 现有测试时训练方法因测试题过难和数据量小导致伪标签质量低、更新不稳定

Method: 提出TTCS框架，双策略协同进化：问题生成器生成渐进式难题，推理求解器利用自一致性奖励更新，并相互反馈优化

Result: 在数学推理基准和通用任务上显著提升性能，且具备跨模型迁移能力

Conclusion: TTCS提供了一种可扩展的动态构建测试时课程的学习范式，推动模型自我进化

Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.

</details>


### [182] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出一种基于分解Rank-1专家池的LoRA框架，通过稀疏组合与正交化实现高效连续学习，显著减少参数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的连续学习方法存在推理负担重或依赖外部知识的问题，直接使用LoRA缓解灾难性遗忘效果有限。

Method: 将单个LoRA模块重构为可分解的Rank-1专家池，依据[CLS]标记语义动态稀疏组合任务特异性更新，并引入激活引导正交（AGO）损失正交化关键权重。

Result: 在多项任务中达到SOTA性能，超越零样本上限，参数减少96.7%，无推理延迟，无需外部数据或任务ID判别器。

Conclusion: 该方法实现了参数高效、无干扰、轻量化的连续学习，为视觉语言模型的持续适应提供了新范式。

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [183] [PEFT-MuTS: A Multivariate Parameter-Efficient Fine-Tuning Framework for Remaining Useful Life Prediction based on Cross-domain Time Series Representation Model](https://arxiv.org/abs/2601.22631)
*En Fu,Yanyan Hu,Changhua Hu,Zengwang Jin,Kaixiang Peng*

Main category: cs.LG

TL;DR: 提出一种基于参数高效微调的少样本剩余寿命预测方法PEFT-MuTS，仅用不足1%的目标设备数据即可实现高性能预测。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的RUL预测依赖大量相似设备的历史退化数据，实际应用中难以满足，亟需减少对数据量的依赖。

Method: 基于跨域预训练时间序列表示模型，设计独立特征微调网络与基于元变量的低秩多变量融合机制，并引入零初始化回归器以稳定少样本微调过程。

Result: 在航空发动机和工业轴承数据集上，仅用<1%目标数据即可超越传统监督和少样本方法，显著降低数据需求。

Conclusion: 跨域预训练可有效突破设备相似性限制，参数高效微调是少样本RUL预测的可行且高效路径。

Abstract: The application of data-driven remaining useful life (RUL) prediction has long been constrained by the availability of large amount of degradation data. Mainstream solutions such as domain adaptation and meta-learning still rely on large amounts of historical degradation data from equipment that is identical or similar to the target, which imposes significant limitations in practical applications. This study investigates PEFT-MuTS, a Parameter-Efficient Fine-Tuning framework for few-shot RUL prediction, built on cross-domain pre-trained time-series representation models. Contrary to the widely held view that knowledge transfer in RUL prediction can only occur within similar devices, we demonstrate that substantial benefits can be achieved through pre-training process with large-scale cross-domain time series datasets. A independent feature tuning network and a meta-variable-based low rank multivariate fusion mechanism are developed to enable the pre-trained univariate time-series representation backbone model to fully exploit the multivariate relationships in degradation data for downstream RUL prediction task. Additionally, we introduce a zero-initialized regressor that stabilizes the fine-tuning process under few-shot conditions. Experiments on aero-engine and industrial bearing datasets demonstrate that our method can achieve effective RUL prediction even when less than 1\% of samples of target equipment are used. Meanwhile, it substantially outperforms conventional supervised and few-shot approaches while markedly reducing the data required to achieve high predictive accuracy. Our code is available at https://github.com/fuen1590/PEFT-MuTS.

</details>


### [184] [MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models](https://arxiv.org/abs/2601.22887)
*Yangyan Li*

Main category: cs.LG

TL;DR: MoVE通过解耦参数记忆与计算成本，实现独立扩展模型记忆容量，提升文本与图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型中，模型容量与计算成本紧密耦合，扩展记忆需加深或加宽网络，导致计算开销成比例增加。

Method: 提出MoVE机制，引入全局可学习值嵌入库，通过可微软门控机制动态混合嵌入概念，使记忆扩展独立于网络深度。

Result: 在文本与图像生成任务中，MoVE相比基线模型显著降低困惑度并提升生成保真度，同等计算预算下表现更优。

Conclusion: MoVE为自回归模型提供了一种新的容量扩展范式，实现记忆密度与计算效率的解耦优化。

Abstract: Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of "memory-dense" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.

</details>


### [185] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 提出一种融合形式逻辑验证的动态框架，显著提升大语言模型的推理准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型因随机生成导致逻辑不一致和奖励欺骗，而形式符号系统能避免此类问题，亟需结合二者优势

Method: 设计动态交互的形式逻辑验证框架，在推理过程中实时检测并纠正错误，并通过两阶段训练管道（监督微调+策略优化）实现

Result: 7B和14B模型在六个推理基准上分别超越SOTA基线10.4%和14.2%

Conclusion: 形式验证可作为可扩展机制，有效突破大语言模型推理性能的边界

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [186] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: GUDA提出了一种高效近似组级数据归因的方法，通过机器遗忘替代重训练，在扩散模型中实现百倍加速并提升归因准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦单个训练样本的归因，但实际应用中更需组级（如艺术风格、物体类别）的归因答案，而传统留组重训练（LOGO）计算代价过高。

Method: GUDA利用机器遗忘技术，在共享的全数据模型基础上近似每个组被移除后的反事实模型，通过ELBO得分差值量化组影响力。

Result: 在CIFAR-10和Stable Diffusion艺术风格任务上，GUDA比语义相似度、梯度法和实例级遗忘更准确识别主贡献组，且速度提升百倍。

Conclusion: GUDA为扩散模型提供高效、可靠的组级数据归因框架，显著降低计算成本的同时保持高精度。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [187] [Layerwise Progressive Freezing Enables STE-Free Training of Deep Binary Neural Networks](https://arxiv.org/abs/2601.22660)
*Evan Gibson Smith,Bashima Islam*

Main category: cs.LG

TL;DR: 提出StoMPP方法，通过随机掩码逐步二值化权重和激活，避免直通估计器，在二值神经网络上显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 传统直通估计器（STE）在训练全二值神经网络时存在梯度阻塞问题，渐进冻结方法在全二值网络中失效，亟需更有效的训练策略。

Method: StoMPP：采用层间随机掩码逐步替换可微剪裁值为硬二值函数，仅对未冻结部分反向传播，完全避免STE。

Result: 在ResNet-50上，CIFAR-10提升18.0%，CIFAR-100提升13.5%，ImageNet提升3.8%；二值权重网络在CIFAR-10达91.2%，CIFAR-100达69.5%。

Conclusion: StoMPP有效缓解二值化梯度阻塞，提升深度网络性能，揭示了渐进冻结下非单调收敛与更好的深度扩展性。

Abstract: We investigate progressive freezing as an alternative to straight-through estimators (STE) for training binary networks from scratch. Under controlled training conditions, we find that while global progressive freezing works for binary-weight networks, it fails for full binary neural networks due to activation-induced gradient blockades. We introduce StoMPP (Stochastic Masked Partial Progressive Binarization), which uses layerwise stochastic masking to progressively replace differentiable clipped weights/activations with hard binary step functions, while only backpropagating through the unfrozen (clipped) subset (i.e., no straight-through estimator). Under a matched minimal training recipe, StoMPP improves accuracy over a BinaryConnect-style STE baseline, with gains that increase with depth (e.g., for ResNet-50 BNN: +18.0 on CIFAR-10, +13.5 on CIFAR-100, and +3.8 on ImageNet; for ResNet-18: +3.1, +4.7, and +1.3). For binary-weight networks, StoMPP achieves 91.2\% accuracy on CIFAR-10 and 69.5\% on CIFAR-100 with ResNet-50. We analyze training dynamics under progressive freezing, revealing non-monotonic convergence and improved depth scaling under binarization constraints.

</details>


### [188] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出一种无需验证数据的联邦学习早停框架，仅用服务器端参数监测任务向量增长率，显著提升性能并降低计算与隐私成本。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖固定全局轮数或验证数据进行超参调优，导致高计算开销和隐私风险。

Method: 通过监测服务器端任务向量的生长率，构建数据免费的早停机制，无需任何客户端验证数据。

Result: 在皮肤病变和血液细胞分类任务中，该方法平均仅需47/20轮即可比验证数据早停高出12.5%/10.3%的性能。

Conclusion: 这是首个无需验证数据的联邦学习早停框架，兼顾效率、性能与隐私保护。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [189] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 论文系统比较了全图与小批量GNN训练，发现小批量在调优后可能优于全图训练，提出基于Wasserstein距离的泛化分析和批大小/扇出大小的非各向异性影响。


<details>
  <summary>Details</summary>
Motivation: 全图与小批量GNN训练在系统设计上需求不同，但缺乏对批大小和扇出大小如何影响收敛与泛化的系统研究。

Method: 通过实证与理论分析，引入Wasserstein距离分析图结构影响，探究批大小与扇出大小对收敛和泛化的非各向异性效应。

Result: 全图训练未必优于调优后的小批量训练；揭示了批大小和扇出大小对性能的独立影响，为资源受限下的超参数调优提供指导。

Conclusion: GNN训练选择应基于资源约束与超参数调优，而非默认偏好全图训练；论文为GNN训练策略提供了新的理论视角和实践依据。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [190] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: Transformer模型在周期性泛化任务上表现差，无法处理未见过的复合周期模式。


<details>
  <summary>Details</summary>
Motivation: 人类能很好地泛化周期性模式，但当前Transformer模型在分布外（OOD）周期性任务中表现不佳，亟需系统研究其局限性。

Method: 从抽象代数与推理角度统一解释周期性，构建Coper基准，包含Hollow与Extrapolation两种OOD设置。

Result: Transformer能记忆训练中的周期数据，但无法泛化至复合周期性，证明其在抽象推理上的不足。

Conclusion: Transformer在周期性泛化方面存在根本性缺陷，需设计新架构或训练方法以提升抽象推理能力。

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [191] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 提出METRIC框架及数据质量指标库，支持医疗AI模型训练数据的适配性评估，提升AI可信度。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需临床与监管认可，而数据质量是构建可信AI的关键，现有方法缺乏系统性评估工具。

Method: 构建METRIC框架下的数据质量指标库，提供每项指标的卡片信息（定义、适用性、示例、陷阱与建议），并设计决策树帮助选择适配指标。

Result: 在PTB-XL心电数据集上验证了方法的有效性，实现了数据适配性的可操作化评估。

Conclusion: 该工作为医疗AI训练与测试数据的适配性评估提供了实用工具，是实现可信医疗AI的重要一步。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [192] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 提出一种基于CNN的深度学习方法，用于在早期设计阶段快速预测IR-drop分布，显著提升分析效率。


<details>
  <summary>Details</summary>
Motivation: 传统物理仿真工具精度高但计算成本大，需接近最终版图，无法满足早期设计探索需求。

Method: 采用U-Net编码器-解码器架构，将版图空间特征映射为IR-drop热力图，训练于物理引导的合成数据集，包含电源网格、单元密度和开关活动等物理因素。

Result: 模型在毫秒级推理时间内实现高精度预测，MSE与PSNR指标表现优异，可作为签核前的快速筛查工具。

Conclusion: 该方法为早期IR-drop分析提供高效互补方案，已开源代码、数据集和交互式应用。

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [193] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 本文首次系统性地统一研究了LoRA变体，提出分类法、理论框架、统一代码库LoRAFactory，并通过大规模实验揭示学习率对性能的关键影响，发现标准LoRA在调优得当下表现最优。


<details>
  <summary>Details</summary>
Motivation: LoRA变体泛滥导致方法、理论、代码和评估的碎片化，亟需统一研究以厘清其关系与性能边界。

Method: 提出四维分类法（秩、优化动态、初始化、MoE集成），构建统一理论框架，开发模块化代码库LoRAFactory，并在NLP与图像任务上进行大规模超参数评估。

Result: LoRA及其变体对学习率高度敏感；在合理超参下，标准LoRA性能常优于或持平其他变体。

Conclusion: 标准LoRA仍是高效微调的坚实基线，未来应聚焦超参优化而非结构复杂化。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [194] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出LoRDS方法，通过低秩分解实现元素级量化，在保持效率的同时显著提升表达能力，并在量化与微调任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化方法依赖块状结构，牺牲表达灵活性；元素级量化虽潜力大但效率不足，亟需高效且高表达的统一框架。

Method: 提出LoRDS，将缩放因子建模为连续低秩矩阵S=BA，打破空间块约束，实现PTQ初始化、联合QAT与高秩乘法PEFT自适应。

Result: 在Llama3-8B上，3比特下比NormalFloat提升27.0%准确率，RTX 4090推理加速1.5倍，4比特下PEFT性能比QLoRA提升9.6%。

Conclusion: LoRDS实现量化与微调的统一，高效兼顾表达力与推理速度，为LLM压缩与适配提供新范式。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [195] [Local Intrinsic Dimension of Representations Predicts Alignment and Generalization in AI Models and Human Brain](https://arxiv.org/abs/2601.22722)
*Junjie Yu,Wenxiao Ma,Chen Wei,Jianyu Zhang,Haotian Deng,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 局部内在维度是统一描述人工与生物系统表征收敛的关键几何属性，它关联模型泛化能力、模型间对齐与模型-脑对齐。


<details>
  <summary>Details</summary>
Motivation: 探究为何泛化能力强的神经网络在架构和训练方式下表现出更高表征对齐，并进一步验证其与人类神经活动的对齐关系。

Method: 通过分析模型嵌入的局部内在维度，量化模型-模型、模型-脑对齐程度，并检验其与泛化性能的相关性。

Result: 局部内在维度越低，模型-模型对齐、模型-脑对齐和泛化能力越强；全局维度无效；缩放模型容量和数据量可系统降低局部维度。

Conclusion: 局部内在维度是解释人工与生物系统表征收敛的统一几何指标。

Abstract: Recent work has found that neural networks with stronger generalization tend to exhibit higher representational alignment with one another across architectures and training paradigms. In this work, we show that models with stronger generalization also align more strongly with human neural activity. Moreover, generalization performance, model--model alignment, and model--brain alignment are all significantly correlated with each other. We further show that these relationships can be explained by a single geometric property of learned representations: the local intrinsic dimension of embeddings. Lower local dimension is consistently associated with stronger model--model alignment, stronger model--brain alignment, and better generalization, whereas global dimension measures fail to capture these effects. Finally, we find that increasing model capacity and training data scale systematically reduces local intrinsic dimension, providing a geometric account of the benefits of scaling. Together, our results identify local intrinsic dimension as a unifying descriptor of representational convergence in artificial and biological systems.

</details>


### [196] [Decomposing Epistemic Uncertainty for Causal Decision Making](https://arxiv.org/abs/2601.22736)
*Md Musfiqur Rahman,Ziwei Jiang,Hilaf Hasson,Murat Kocaoglu*

Main category: cs.LG

TL;DR: 提出新框架区分因果效应边界中的样本不确定性与非可识别不确定性，指导数据收集策略。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在因果效应估计中易过拟合且无法区分边界宽度的来源，缺乏对样本有限性与根本不可识别性的系统分解。

Method: 构建观测分布的置信集，求解最小-最大和最大-最小优化问题，通过神经因果模型在所有可能分布与结构因果模型中搜索因果效应交集。

Result: 可分离样本不确定性（可通过增样减少）与非可识别不确定性（需增变量或随机实验），实验验证其在合成与真实数据中有效。

Conclusion: 该框架帮助决策者判断是否应增加样本量或转向随机实验，提升因果推断的实用性与可靠性。

Abstract: Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural networks to obtain practical bounds to such causal effects, which is often an intractable problem. However, these approaches may overfit to the dataset and be overconfident in their causal effect estimates. Moreover, there is currently no systematic approach to disentangle how much of the width of causal effect bounds is due to fundamental non-identifiability versus how much is due to finite-sample limitations. We propose a novel framework to address this problem by considering a confidence set around the empirical observational distribution and obtaining the intersection of causal effect bounds for all distributions in this confidence set. This allows us to distinguish the part of the interval that can be reduced by collecting more samples, which we call sample uncertainty, from the part that can only be reduced by observing more variables, such as latent confounders or instrumental variables, but not with more data, which we call non-ID uncertainty. The upper and lower bounds to this intersection are obtained by solving min-max and max-min problems with neural causal models by searching over all distributions that the dataset might have been sampled from, and all SCMs that entail the corresponding distribution. We demonstrate via extensive experiments on synthetic and real-world datasets that our algorithm can determine when collecting more samples will not help determine the best action. This can guide practitioners to collect more variables or lean towards a randomized study for best action identification.

</details>


### [197] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文系统研究了Softmax族损失函数的理论性质与效率权衡，提出一致性、收敛性与偏差-方差分解的统一框架，为大规模分类任务提供损失函数选择依据。


<details>
  <summary>Details</summary>
Motivation: Softmax损失广泛用于分类与排序任务，但其理论特性及大规模场景下的效率-效果权衡缺乏系统研究。

Method: 基于Fenchel-Young框架分析Softmax族损失的一致性与梯度动态，提出偏差-方差分解方法并推导每轮复杂度分析。

Result: 揭示了不同损失函数在一致性、收敛行为和效率上的权衡，实验表明理论分析与实际性能高度一致。

Conclusion: 为大规模机器学习中的损失函数选择提供了理论基础与实用指导。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [198] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 提出物理信息的Müntz-Szász网络（MSN-PINN），将尺度指数作为可训练参数，实现高精度物理指数恢复。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络无法显式提取物理系统中幂律标度的指数，而这些指数具有明确的物理意义。

Method: 构建基于幂律基的MSN-PINN，将标度指数作为可训练参数，并采用约束感知训练确保物理一致性。

Result: 在拉普拉斯方程和泊松问题中，指数恢复误差低至0.009%–0.05%，在40组楔形基准上成功率达100%，精度提升三个数量级。

Conclusion: MSN-PINN融合神经网络表达力与渐近分析可解释性，实现物理指数的可识别、高精度、可解释学习。

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [199] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP是一种轻量级客户端加密框架，通过在LLM潜在空间中注入模糊语义零空间扰动，在不降低模型效用的前提下实现隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法在LLM推理中面临隐私与效用难以平衡的问题，亟需一种无需后处理、轻量且高效的客户端加密方案。

Method: 定义并利用‘模糊语义零空间’，通过密钥依赖的随机映射向原始嵌入注入个性化的扰动，使其正交于原语义空间，从而实现隐私增强。

Result: 在12个生成与分类基准上，OSNIP显著降低攻击成功率，同时保持高水平模型效用，达到当前最优性能。

Conclusion: OSNIP提供了一种高效、可扩展的客户端隐私保护方案，为LLM推理中的隐私安全开辟了新路径。

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [200] [Understanding Generalization from Embedding Dimension and Distributional Convergence](https://arxiv.org/abs/2601.22756)
*Junjie Yu,Zhuoli Ouyang,Haotian Deng,Chen Wei,Wenxiao Ma,Jianyu Zhang,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 本文从表征角度分析深度神经网络的泛化性能，提出基于嵌入维度和Lipschitz敏感性的误差界，无需依赖参数量。


<details>
  <summary>Details</summary>
Motivation: 传统参数化分析无法解释过参数化神经网络的良好泛化，需从表征几何角度重新审视。

Method: 通过嵌入分布的内在维度和下游映射的Lipschitz常数构建泛化误差界，分析嵌入层的几何性质。

Result: 理论表明泛化误差主要由嵌入维度主导，与实验观察到的强相关性一致。

Conclusion: 嵌入层的几何特性是理解泛化的关键，为模型诊断提供了新工具。

Abstract: Deep neural networks often generalize well despite heavy over-parameterization, challenging classical parameter-based analyses. We study generalization from a representation-centric perspective and analyze how the geometry of learned embeddings controls predictive performance for a fixed trained model. We show that population risk can be bounded by two factors: (i) the intrinsic dimension of the embedding distribution, which determines the convergence rate of empirical embedding distribution to the population distribution in Wasserstein distance, and (ii) the sensitivity of the downstream mapping from embeddings to predictions, characterized by Lipschitz constants. Together, these yield an embedding-dependent error bound that does not rely on parameter counts or hypothesis class complexity. At the final embedding layer, architectural sensitivity vanishes and the bound is dominated by embedding dimension, explaining its strong empirical correlation with generalization performance. Experiments across architectures and datasets validate the theory and demonstrate the utility of embedding-based diagnostics.

</details>


### [201] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 系统研究分子语言模型的缩放规律，发现模型规模、数据量和分子表示对性能有显著影响，并发布最大规模的分子语言模型库。


<details>
  <summary>Details</summary>
Motivation: 当前分子生成模型是否遵循可预测的缩放规律尚不明确，亟需在固定计算预算下优化模型规模、数据量和分子表示之间的资源配置。

Method: 训练300个模型，进行超过10,000次实验，严格控制计算预算，独立调整模型大小、训练token数和分子表示方式。

Result: 发现分子模型在预训练和下游任务中均存在清晰的缩放规律，分子表示对性能影响显著，解释了此前观察到的缩放行为不一致现象。

Conclusion: 分子语言模型的缩放行为可预测，合理设计分子表示和资源分配可显著提升性能，发布的模型库将推动未来研究。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [202] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 本文建立了稀疏注意力机制与紧致核函数之间的理论联系，揭示了多种注意力机制可由核回归导出，并提出了一种基于核回归的Transformer变体Memory Mosaics，实验验证其性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究已知标准softmax注意力对应高斯核回归，但稀疏注意力的核理论解释尚缺失，亟需统一框架理解其原理。

Method: 通过核回归视角，证明ReLu与sparsemax注意力分别对应Epanechnikov核在固定和自适应归一化下的形式，并推广至α-entmax注意力与Epanechnikov、biweight、triweight等核的对应关系，极限情况还原softmax与高斯核关系。

Result: 理论揭示了稀疏性源于核函数的有界支撑；基于该理论构建的Memory Mosaics在语言建模、上下文学习和长度泛化任务上表现与主流方法相当。

Conclusion: 该工作为注意力机制设计提供了基于核理论的统一框架，替代了启发式top-k等方法，推动了可解释、结构化注意力的发展。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [203] [Float8@2bits: Entropy Coding Enables Data-Free Model Compression](https://arxiv.org/abs/2601.22787)
*Patrick Putzky,Martin Genzel,Mattes Mollenhauer,Sebastian Schulze,Thomas Wollmann,Stefan Dietzel*

Main category: cs.LG

TL;DR: EntQuant首次结合数据自由与数据依赖压缩方法的优势，在极低比特率下实现高效且高性能的模型压缩。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法要么在极低比特率下性能崩溃，要么计算开销大且鲁棒性差，亟需一种兼顾速度、通用性与精度的压缩框架。

Method: 通过熵编码解耦数值精度与存储成本，实现无需校准数据的快速压缩，支持70B模型在30分钟内完成压缩。

Result: 在标准评测和指令微调模型上达到SOTA性能，推理开销低，且在分布偏移下保持功能稳定性。

Conclusion: EntQuant为极端压缩场景提供了兼具高效性与鲁棒性的新范式，推动了实用化大模型压缩的发展。

Abstract: Post-training compression is currently divided into two contrasting regimes. On the one hand, fast, data-free, and model-agnostic methods (e.g., NF4 or HQQ) offer maximum accessibility but suffer from functional collapse at extreme bit-rates below 4 bits. On the other hand, techniques leveraging calibration data or extensive recovery training achieve superior fidelity but impose high computational constraints and face uncertain robustness under data distribution shifts. We introduce EntQuant, the first framework to unite the advantages of these distinct paradigms. By matching the performance of data-dependent methods with the speed and universality of data-free techniques, EntQuant enables practical utility in the extreme compression regime. Our method decouples numerical precision from storage cost via entropy coding, compressing a 70B parameter model in less than 30 minutes. We demonstrate that EntQuant does not only achieve state-of-the-art results on standard evaluation sets and models, but also retains functional performance on more complex benchmarks with instruction-tuned models, all at modest inference overhead.

</details>


### [204] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO提出了一种无裁剪的强化学习方法，替代传统裁剪机制，提升大语言模型微调的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于裁剪的强化学习算法在大规模应用中存在零梯度、奖励欺骗和训练不稳定等问题。

Method: 采用基于总变差散度约束的凸二次惩罚项，替代启发式裁剪，构建处处可微的优化目标。

Result: 在推理和对齐任务中，CFPO性能与裁剪方法相当，同时更稳定，缓解了冗长滥用和能力退化问题。

Conclusion: CFPO只需一行代码修改且无需额外超参，是裁剪方法的理想替代方案。

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [205] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: 提出Sombrero方法，通过边界丰富度指标B优化分段策略，提升长序列建模的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端分段方法难以量化和控制计算资源在预测难度上的分配。

Method: 引入边界丰富度B作为评价指标，设计置信度对齐损失与输入级置信度加权平滑技术。

Result: 在1B规模多语言和代码数据集上，Sombrero显著改善效率-精度权衡，使计算更集中于难预测位置。

Conclusion: 边界质量可量化引导，Sombrero有效提升分层序列模型的计算效率与预测性能。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [206] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS-EDEN量化方法和Quartet II方案，在NVFP4格式下实现更高效的全量化LLM训练，显著降低量化误差并提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有量化训练方法在NVFP4格式中因使用随机舍入（SR）牺牲了表示能力，导致精度低于FP16/FP8，亟需更优的量化策略。

Method: 引入MS-EDEN——一种针对微尺度格式的无偏量化方法，量化误差比SR降低2倍以上，并构建全NVFP4量化方案Quartet II，优化前后向矩阵乘法的梯度估计。

Result: 在1.9B参数LLM上训练，实现更高精度，且相比BF16获得最高4.2倍加速，代码已开源。

Conclusion: Quartet II显著提升NVFP4全量化训练性能，为大规模模型端到端量化训练提供新范式。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [207] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 提出一种级联扩散模型，通过低分辨率先生成类别特征和粗粒度数值表示，再引导高分辨率生成混合类型特征，提升生成真实性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型难以处理单特征中离散状态与连续分布的混合类型数据，尤其在缺失或膨胀值等复杂情形下。

Method: 采用级联扩散框架：先生成低分辨率的纯类别特征和粗粒度数值表示，再通过引导条件概率路径和数据依赖耦合进行高分辨率生成。

Result: 生成样本更真实，分布捕捉更准确，检测得分提升40%，并理论证明级联结构收紧了传输代价界。

Conclusion: 该方法显著改进了混合类型表格数据的生成质量，为复杂分布建模提供了新范式。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [208] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: 提出MetaDrug框架，通过多层次元学习和不确定性量化解决患者冷启动问题，显著提升新患者用药推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有用药推荐方法在新患者冷启动问题上表现不佳，虽有知识图谱但缺乏个性化适应，且元学习在EHR序列数据中应用不足。

Method: MetaDrug采用双层元适应机制：自适应（利用患者自身病历）和同伴适应（利用相似患者记录），并引入不确定性量化模块筛选有效支持样本。

Result: 在MIMIC-III和AKI数据集上，MetaDrug在冷启动患者上的推荐性能优于当前最优方法。

Conclusion: MetaDrug有效解决了EHR中患者冷启动问题，为个性化用药推荐提供了新范式。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [209] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出一种通过正则化预训练自编码器来增强时间序列潜在空间等变性的流匹配框架，显著提升生成质量并加速采样。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成模型在潜在空间中缺乏等变性设计，影响生成性能与几何一致性。

Method: 引入等变性损失函数，对预训练自编码器的潜在空间进行微调，使其对平移和幅度缩放等变换保持一致性。

Result: 在多个真实数据集上，该方法在生成质量上优于扩散模型，且采样速度快数个数量级。

Conclusion: 将几何归纳偏置引入潜在生成模型可有效提升时间序列生成的效率与性能。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [210] [Hierarchical Shift Mixing -- Beyond Dense Attention in Transformers](https://arxiv.org/abs/2601.22852)
*Robert Forchheimer*

Main category: cs.LG

TL;DR: 提出Hierarchical Shift Mixing (HSM)，以线性时间复杂度实现接近softmax注意力的性能，并可与softmax混合超越GPT基线。


<details>
  <summary>Details</summary>
Motivation: softmax注意力计算复杂度高，现有替代方法通常牺牲性能，亟需高效且高性能的替代方案。

Method: HSM框架将成对令牌交互分散到多层Transformer中，实现线性时间复杂度，且对混合函数无特定要求。

Result: 简单HSM变体性能接近softmax注意力，混合架构在降低计算成本的同时超越GPT基线。

Conclusion: HSM为高效Transformer设计提供了新范式，平衡了计算效率与模型性能。

Abstract: Since the introduction of the Transformer architecture for large language models, the softmax-based attention layer has faced increasing scrutinity due to its quadratic-time computational complexity. Attempts have been made to replace it with less complex methods, at the cost of reduced performance in most cases. We introduce Hierarchical Shift Mixing (HSM), a general framework for token mixing that distributes pairwise token interactions across Transformer layers rather than computing them densely within each layer. HSM enables linear-time complexity while remaining agnostic to the specific mixing function. We show that even simple HSM variants achieve performance close to softmax attention, and that hybrid architectures combining HSM with softmax attention can outperform a GPT-style Transformer baseline while reducing computational cost during both training and inference.

</details>


### [211] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出OptiMAG框架，利用非平衡最优传输解决多模态属性图中的模态结构冲突问题，显著提升多种任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定显式图结构上进行消息传递，导致不同模态语义不一致，引入模态噪声，阻碍节点表示学习。

Method: 采用Fused Gromov-Wasserstein距离引导局部邻域内跨模态结构一致性，结合KL散度惩罚自适应处理模态不一致，作为即插即用正则化器。

Result: 在节点分类、链路预测、graph2text、graph2image等任务上持续优于基线方法。

Conclusion: OptiMAG有效缓解多模态图中的结构-语义冲突，提升表示学习效果，具有广泛适用性。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [212] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn是一种新型脉冲Transformer，通过M-TTFS编码和memristive突触单元显著降低能耗，同时提升GLUE基准上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有SNN能耗评估仅关注累加操作，忽略了数据移动等实际硬件开销，而数据移动可占总能耗近80%。

Method: 提出M-TTFS编码方法，将沉默状态分配给最频繁的膜电位并引入死区策略以最大化稀疏性；结合memristive突触单元（MSU）利用存算一体技术消除权重访问开销。

Result: 在GLUE基准上，Matterhorn比现有SNN平均准确率提升1.42%，能效提高2.31倍。

Conclusion: Matterhorn通过算法-硬件协同设计，实现了SNN在能效和性能上的双重突破，为低功耗大模型推理提供了新路径。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [213] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: 使用量化图映射生成合成时间序列，与GAN方法相比具有竞争力且可解释


<details>
  <summary>Details</summary>
Motivation: 由于隐私、成本和标注问题，高质量时间序列数据难以获取，亟需有效的合成生成方法

Method: 将时间序列转换为量化图（QG），通过逆映射重建合成数据

Result: 所提方法在模拟和真实数据上表现良好，能保留原始数据的统计与结构特性，优于或媲美GAN方法

Conclusion: 基于量化图的生成框架是一种高效、可解释的合成时间序列生成新途径

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [214] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL通过将命题视为参数化谓词，实现对未见LTL命题和任务的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有LTL引导的多任务RL方法无法泛化到未见过的命题词汇，限制了其在新任务中的应用。

Method: 将命题建模为参数化谓词，设计新架构嵌入并组合谓词以表示LTL规范。

Result: 在复杂环境中实现了对新命题和任务的零样本泛化。

Conclusion: 参数化谓词方法显著提升了LTL引导RL的泛化能力。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [215] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出一种基于正则化的多变量校准方法，利用预排序函数提升校准性，引入PCA预排序揭示依赖结构错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多变量校准上效果有限，预排序函数仅用于事后评估，缺乏训练时的校准机制。

Method: 通过正则化在训练中强制多变量校准，提出基于PCA的预排序函数，投影至预测分布的主方向。

Result: 在模拟和18个真实数据集上，方法显著提升预排序校准性，且PCA预排序能发现现有方法未检出的依赖结构问题。

Conclusion: 所提方法在不牺牲预测精度的前提下，有效改善多变量校准，并提供更敏感的诊断工具。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [216] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出一种基于高斯过程的贝叶斯决策树模型，提升回归任务中的外推能力和不确定性校准


<details>
  <summary>Details</summary>
Motivation: 传统决策树在回归任务中无法可靠外推且不确定性校准差，叶子节点预测受限于训练目标范围并易过自信

Method: 在VSPYCT基础上，为每个叶子节点引入高斯过程预测器，采用贝叶斯斜切分实现不确定性感知的输入空间划分，并通过门控机制在输入超出训练范围时激活外推

Result: 在基准回归任务中优于标准变分斜树，在外推场景下性能显著提升

Conclusion: 该模型通过贝叶斯斜切分与高斯过程叶子的结合，实现了更可靠的外推与校准的不确定性估计

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [217] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA提出一种基于熵的动态低秩适配框架，突破传统LoRA固定秩限制，支持秩的剪枝与扩展，显著提升微调效率与性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA固定秩设计缺乏灵活性，现有动态秩方法依赖启发式全局排序，无法区分矩阵差异，且缺乏容量扩展机制。

Method: 提出FlexLoRA，通过谱能量熵评估矩阵重要性，支持全局预算下的秩剪枝与扩展，并采用零影响初始化保证稳定性。

Result: 在多个基准上 consistently 超越现有SOTA方法，验证了方法的有效性与优越性。

Conclusion: FlexLoRA为参数高效微调提供更系统、灵活且稳定的解决方案，兼顾细粒度、灵活性与稳定性。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [218] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 提出一种针对非光滑DC正则化分布的近端Langevin算法DC-LA，理论收敛且实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 处理数据保真项光滑但正则项为非光滑差分凸（DC）函数的采样问题，现有方法难以有效应对。

Method: 利用Moreau包络平滑DC正则项，将凹部分重分配至数据保真项，构造DC-LA算法并分析其在q-Wasserstein距离下的收敛性。

Result: 在distant dissipative条件下，证明DC-LA收敛至目标分布，且理论优于现有非对数凹采样工作；数值实验验证其在合成与CT重建任务中的准确性与不确定性量化能力。

Conclusion: DC-LA为非光滑DC正则化采样提供了通用且有效的框架，兼具理论保障与实际应用价值。

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [219] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: 提出STPGC方法，通过代数拓扑中的强收缩与边收缩，高效保留图拓扑结构并加速GNN训练。


<details>
  <summary>Details</summary>
Motivation: 现有图粗化方法仅保留谱或空间特性，而保留拓扑特征虽提升GNN性能，但时间复杂度指数级增长。

Method: 引入图强收缩与边收缩概念，设计GStrongCollapse、GEdgeCollapse和NeighborhoodConing三个算法，严格保留拓扑特征并压缩图结构。

Result: STPGC在保留GNN感受野的同时显著降低计算复杂度，实验表明其在节点分类任务上兼具高效性与有效性。

Conclusion: STPGC为拓扑感知图粗化提供了可扩展解决方案，兼顾性能与效率。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [220] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: 提出ECTR框架，通过环境条件的尾部重加权联合应对环境级和样本级分布偏移，提升OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有IRM方法忽视环境内样本异质性，导致在混合分布偏移下OOD性能不佳。

Method: ECTR框架结合总变差不变学习与环境条件尾部重加权，并可扩展至无环境标注场景。

Result: 在回归、表格、时序和图像分类基准上，ECTR在最差环境和平均OOD性能上均显著提升。

Conclusion: ECTR通过协同环境级不变性与环境内鲁棒性，有效应对混合分布偏移，提升模型泛化能力。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [221] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR:  perplexity 可能不适合作为模型选择指标，因其无法保证高准确率模型被优先选择


<details>
  <summary>Details</summary>
Motivation: 尽管困惑度被广泛用作模型质量评估指标，但其存在理论缺陷，可能误导模型选择

Method: 基于 Transformer 连续性理论，通过数学证明和等困惑度图分析，揭示困惑度与预测准确率之间的非一一对应关系

Result: 证明即使模型对某序列预测准确且自信，也必然存在另一序列具有低困惑度但预测错误；困惑度提升未必伴随准确率提升

Conclusion: 困惑度作为模型选择指标存在根本性缺陷，需谨慎使用，应结合准确率等直接指标评估模型性能

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [222] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文提出了一种新的公平性感知算法框架FairLinBandit，在线性多臂bandit中实现了最优的Nash遗憾和更通用的p-均值遗憾，理论与实验均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Nash遗憾在高维线性bandit中存在次优性，因依赖受限的集中不等式；亟需更优分析工具并扩展至更广的公平-效用权衡框架。

Method: 提出FairLinBandit元算法框架，结合Phased Elimination和UCB，引入新分析工具，首次实现p-均值 regret 的统一理论保证（涵盖所有p值）。

Result: 理论证明获得阶最优的Nash regret及子线性p-均值 regret，实验证明在真实数据集上优于现有基线。

Conclusion: 工作解决了线性bandit中Nash regret的开放问题，并建立了p-均值 regret 的新范式，为公平与效用平衡提供了通用可扩展框架。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [223] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 本文提出PAVE，通过优化批评值函数的几何性质来稳定策略，而非直接正则化策略输出，实现平滑且高性能的强化学习策略。


<details>
  <summary>Details</summary>
Motivation: 连续Actor-Critic方法学习的策略常有高频振荡，现有方法仅正则化策略输出，未解决根本原因。

Method: 理论证明策略非平滑性由批评函数的微分几何决定，提出PAVE通过最小化Q梯度波动来稳定价值场的梯度场。

Result: PAVE在不修改Actor的情况下，达到与策略正则化方法相当的平滑性和鲁棒性，同时保持任务性能。

Conclusion: 批评函数的几何特性是策略非平滑的根本原因，PAVE提供了一种批评中心的解决方案，优于传统策略侧正则化方法。

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [224] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出一种可端到端学习的排列框架，提升结构化剪枝性能


<details>
  <summary>Details</summary>
Motivation: Transformer规模增大导致排列搜索空间指数增长，传统启发式方法效果受限

Method: 引入可学习排列代价矩阵、可微二分匹配求解器和稀疏优化损失函数，联合优化排列操作

Result: 在视觉与语言Transformer上实现结构化剪枝的最先进排列效果

Conclusion: 该框架有效突破了大规模模型排列优化的瓶颈，为剪枝性能提升提供了新路径

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [225] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK是一种面向离散扩散语言模型的解码引导水印方法，利用解码顺序敏感性实现无概率重加权的水印嵌入。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（dLLMs）对掩码恢复顺序敏感，传统水印方法难以适用，需利用此特性构建新水印通道。

Method: dgMARK通过二进制哈希诱导的奇偶约束引导解码顺序，结合置信度、熵等解码策略，无需修改模型概率，支持一步前瞻增强。

Result: 水印通过奇偶匹配统计检测，滑动窗口检测器可抵抗插入、删除、替换和改写等后编辑操作。

Conclusion: dgMARK是一种高效、即插即用的dLLM水印方案，在保持生成质量的同时实现强鲁棒性。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [226] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: 提出VaR-CPO算法，直接优化VaR约束，实现训练中零违反安全约束


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练中无法保证安全约束不被违反，需一种更保守且样本高效的方法直接优化VaR

Method: 利用单侧切比雪夫不等式构建VaR约束的可微代理，扩展CPO的信赖域框架以提供策略改进与约束违反的最坏情况界

Result: VaR-CPO在可行环境中训练时实现零约束违反，且样本效率高，优于基线方法

Conclusion: VaR-CPO为强化学习中的安全约束优化提供了理论保障与实践优势

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [227] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 提出一种新的流形优化器Mano，在更低内存和计算成本下超越AdamW和Muon，显著提升大语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如AdamW忽略结构信息，Muon丢失曲率信息，传统流形优化在大规模模型中表现差，亟需更高效优化方法。

Method: 将动量投影到模型参数的切空间，并约束在旋转Oblique流形上，提出新型优化器Mano。

Result: 在LLaMA和Qwen3上实验表明，Mano在更低资源消耗下显著优于AdamW和Muon，拓展了时空效率的Pareto前沿。

Conclusion: Mano首次弥合了流形优化与现代优化器间的性能差距，为大模型训练提供了高效新范式。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [228] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出连续约束插值(CCI)框架，统一三种策略约束方法，并通过ACPO算法自动调整约束强度，在D4RL和NeoRL2上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法仅使用单一约束类型，缺乏对不同约束家族间联系与权衡的统一解释。

Method: 提出连续约束插值(CCI)框架，引入插值参数连接加权行为克隆、密度正则化和支持约束，并设计ACPO算法通过拉格朗日对偶更新自动优化该参数。

Result: 建立了最大熵性能差分引理，推导了最优策略及其参数投影的性能下界，在D4RL和NeoRL2上取得领先性能。

Conclusion: CCI为离线RL约束设计提供了统一理论框架，ACPO实现了自适应约束调整，显著提升泛化能力与稳定性。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [229] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出一种仅用两个sEMG通道的深度学习框架，实现高精度手势识别，并通过少样本迁移学习和增量学习提升泛化性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢受个体差异大和高密度传感器临床不实用的限制，亟需轻量、高效且泛化能力强的解决方案。

Method: 采用卷积稀疏自编码器（CSAE）从原始信号中自动提取时序特征，结合少样本迁移学习和增量学习策略实现跨个体适配与类别扩展。

Result: 在6类手势上达到94.3%±0.3%的F1分数，少样本迁移后未见个体性能提升至92.3%±0.9%，扩展至10类仍保持90.0%±0.2%的高精度。

Conclusion: 该框架以极低传感器和计算开销实现高精度、可扩展的肌电控制，为下一代经济型自适应假肢提供可行路径。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [230] [Mem-T: Densifying Rewards for Long-Horizon Memory Agents](https://arxiv.org/abs/2601.23014)
*Yanwei Yue,Guibin Zhang,Boci Peng,Xuanbo Fan,Jiaxin Guo,Qiankun Li,Yan Zhang*

Main category: cs.LG

TL;DR: 提出Mem-T内存代理和MoT-GRPO训练框架，实现端到端优化记忆管理，性能优于现有方法且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理训练依赖稀疏延迟奖励，难以优化长序列记忆操作策略。

Method: 设计Mem-T代理结合分层内存数据库，并提出MoT-GRPO树引导强化学习框架，通过记忆操作树反向传播和 hindsight 信用分配实现密集监督。

Result: Mem-T在性能上超越A-Mem和Mem0达14.92%，推理token减少24.45%，且保持准确率。

Conclusion: Mem-T与MoT-GRPO有效实现了自主记忆管理的端到端优化，在性能与效率上取得显著平衡。

Abstract: Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.

</details>


### [231] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 提出一种因果模型，区分测量误差与机制变化两类异常，实现异常类型分类与根源定位，性能达SOTA且对未知因果图鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法区分由测量误差和机制变化引起的异常，而二者处理方式不同，亟需建模区分。

Method: 构建因果模型，将异常视为对潜在（真实）和观测变量的潜在干预，采用最大似然估计进行推断。

Result: 在根因定位上达到SOTA性能，同时准确分类异常类型，并在因果DAG未知时仍保持鲁棒性。

Conclusion: 该方法能有效区分两类异常，提升根因分析的可解释性与实用性，适用于真实复杂场景。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [232] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出DC-CoT方法，通过并行子任务分解降低长链推理延迟，保持高准确率


<details>
  <summary>Details</summary>
Motivation: 长链推理导致高延迟，需在不牺牲准确率前提下加快推理速度

Method: 基于SFT初始化分任务能力，再用多阶段RL优化，降低最长路径长度

Result: 在AIME 2024和HMMT 2025上准确率持平，最长路径长度减少35-40%

Conclusion: DC-CoT有效实现低延迟高准确的并行推理，代码与数据已开源

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [233] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 提出高效PH-ASC算法解决熵正则化最优传输中因过早模式坍缩导致的不稳定性问题，将计算开销从O(N³)降至O(1)。


<details>
  <summary>Details</summary>
Motivation: 传统退火方法在ε→0时因模式坍缩导致推理不稳定，理论分析揭示了推理算子收缩速率与目标后验变化速率之间的热力学速度限制。

Method: 提出自适应调度算法Efficient PH-ASC，通过线性稳定性准则动态调整退火过程，避免昂贵的谱诊断，实现摊销O(1)复杂度。

Result: 有效避免了推理轨迹陷入虚假局部极小值，显著提升训练稳定性，计算效率大幅提升。

Conclusion: 该方法为可微匹配层提供了更稳定、高效的推理机制，且无需假设特征提取器极端快速收敛。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [234] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 提出一种基于WGAN的密度感知条件图生成框架，通过可学习的距离依赖边预测器替代随机采样，实现更真实的图结构生成。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法依赖固定概率的随机边采样，难以捕捉节点间的复杂结构依赖和类特定连接模式。

Method: 采用WGAN框架，将节点嵌入到潜在空间，利用可微分的边预测器基于节点距离预测边概率，并通过密度感知机制自适应控制边密度以匹配真实图的类特定稀疏分布。

Result: 在基准数据集上生成的图在结构连贯性和类一致性上优于基线方法，边预测器能捕捉复杂关系，生成图的密度与拓扑更接近真实分布，且训练更稳定。

Conclusion: 该框架有效实现了可控、稳定的图生成，适用于真实图数据合成与数据增强。

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [235] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出RLRR框架，用相对排名替代绝对奖励，提升大语言模型在推理和开放生成任务中的强化学习效果


<details>
  <summary>Details</summary>
Motivation: 现有组内强化学习方法依赖绝对数值奖励，在可验证任务中监督稀疏，在开放任务中奖励模型不稳定，导致优势估计不准

Method: 提出RLRR框架，结合排序奖励模型，将原始评估转化为相对排名信号，实现组内基于排序的策略优化

Result: 在推理基准和开放生成任务中，RLRR相比基线方法持续提升性能

Conclusion: 相对奖励机制能有效缓解稀疏监督与奖励不稳问题，是组内强化学习的更优范式

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [236] [ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations](https://arxiv.org/abs/2601.23068)
*Joao Fonseca,Julia Stoyanovich*

Main category: cs.LG

TL;DR: 提出ExplainerPFN，一种无需访问模型即可零样本估计Shapley值的表格基础模型，性能媲美需少量示例的代理解释方法。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值计算需访问模型或大量样本，实际部署中常不可行，亟需无需模型访问的零样本解释方法。

Method: 基于TabPFN构建ExplainerPFN，用随机因果模型生成的合成数据预训练，监督学习精确/近似Shapley值，实现零样本特征重要性预测。

Result: ExplainerPFN在真实与合成数据上表现优异，仅用2个参考样本即可逼近SHAP精度，无需模型访问、梯度或示例解释。

Conclusion: ExplainerPFN首次实现零样本Shapley值估计，为模型解释提供高效、实用的新范式，开源代码促进可复现性。

Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations.
  Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.

</details>


### [237] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow 使用 B 样条插值改进流匹配，更好建模动态系统的多边际约束


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法使用线性插值，难以捕捉高阶动态和不规则采样下的状态演化，高阶多项式不稳定且振荡

Method: 提出 SplineFlow，利用 B 样条基函数的平滑性和稳定性，联合建模观测间的条件路径，满足多边际约束

Result: 在多种确定性和随机动态系统及细胞轨迹推断任务中，SplineFlow 性能显著优于基线方法

Conclusion: B 样条插值为流匹配提供了更稳定、理论严谨的动态建模框架，适用于复杂系统学习

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [238] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 提出CATTO方法，在不牺牲准确率的前提下显著提升大语言模型置信度校准效果，并引入Confidence@k提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在偏好对齐后出现置信度与预测正确性脱钩，高置信度预测常错误，低置信度预测却可能正确。

Method: 提出CATTO校准感知训练目标，联合偏好优化目标，使预测置信度与实际正确性对齐；并设计Confidence@k测试时推理机制。

Result: CATTO在in-distribution和out-of-distribution下分别降低ECE 2.22%-7.61%和1.46%-10.44%，且保持或小幅提升多选题准确率。

Conclusion: CATTO有效解决LLM置信度校准问题，实现校准与准确性的双赢，Confidence@k进一步提升推理性能。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [239] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 提出进化预测（EF）范式，突破直接预测的局限，短周期训练模型可超越长周期训练，实现更优性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 直接预测（DF）范式因输出与评估 horizon 强耦合，导致需为每个目标 horizon 重新训练，效率低下且优化困难。

Method: 提出进化预测（EF）范式，通过渐进式推理替代单次预测，证明 DF 是 EF 的退化特例，利用短 horizon 模型演化预测长 horizon。

Result: 单一 EF 模型超越多个 DF 集成模型，在标准基准上表现更优，且在极端外推中具备稳健的渐近稳定性。

Conclusion: LTSF 应从静态映射转向自主进化推理，EF 为未来长时序预测提供新范式。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [240] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出分布感知的共形排序方法DCR，通过精确计算非一致性分数分布，显著缩小预测集大小，同时保证覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有共形排序方法因使用非一致性分数的上界而过于保守，导致预测集过大，影响实用性。

Method: DCR利用校准项的绝对秩服从负超几何分布的特性，推导非一致性分数的精确分布，从而确定共形阈值。

Result: DCR在保证覆盖率的前提下，平均预测集大小减少高达36%。

Conclusion: DCR在效率与理论保障上均优于基线方法，为实际排序系统提供了更实用的不确定性量化方案。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [241] [Regularisation in neural networks: a survey and empirical analysis of approaches](https://arxiv.org/abs/2601.23131)
*Christiaan P. Opperman,Anna S. Bosman,Katherine M. Malan*

Main category: cs.LG

TL;DR: 研究发现正则化技术的效果依赖于数据集类型，并非总是提升性能，需根据数据特性选择合适方法。


<details>
  <summary>Details</summary>
Motivation: 传统假设认为任何正则化都能提升泛化能力，但该假设缺乏实证支持，亟需系统评估不同正则化方法的实际效果。

Method: 提出四类正则化方法分类（数据、架构、训练、损失函数），并基于十组数值与图像数据集对MLP和CNN进行实证比较。

Result: 正则化效果具有数据依赖性：数值数据中仅正则项有效，图像数据中仅批归一化有效，且各类方法间存在矛盾与关联。

Conclusion: 正则化并非万能，应根据数据类型和模型架构有针对性地选择方法，才能真正提升泛化能力。

Abstract: Despite huge successes on a wide range of tasks, neural networks are known to sometimes struggle to generalise to unseen data. Many approaches have been proposed over the years to promote the generalisation ability of neural networks, collectively known as regularisation techniques. These are used as common practice under the assumption that any regularisation added to the pipeline would result in a performance improvement. In this study, we investigate whether this assumption holds in practice. First, we provide a broad review of regularisation techniques, including modern theories such as double descent. We propose a taxonomy of methods under four broad categories, namely: (1) data-based strategies, (2) architecture strategies, (3) training strategies, and (4) loss function strategies. Notably, we highlight the contradictions and correspondences between the approaches in these broad classes. Further, we perform an empirical comparison of the various regularisation techniques on classification tasks for ten numerical and image datasets applied to the multi-layer perceptron and convolutional neural network architectures. Results show that the efficacy of regularisation is dataset-dependent. For example, the use of a regularisation term only improved performance on numeric datasets, whereas batch normalisation improved performance on image datasets only. Generalisation is crucial to machine learning; thus, understanding the effects of applying regularisation techniques, and considering the connections between them is essential to the appropriate use of these methods in practice.

</details>


### [242] [Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients](https://arxiv.org/abs/2601.23135)
*Cheng Ge,Caitlyn Heqi Yin,Hao Liang,Jiawei Zhang*

Main category: cs.LG

TL;DR: GRPO通过标准差归一化实现自适应梯度，理论上提升收敛速度，实证揭示其在三个训练阶段的作用机制。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO作为无批评者RL算法的主流方法被广泛应用，但其标准差归一化为何及何时有效仍不明确。

Method: 从序列级策略梯度的局部曲率角度分析，理论推导GRPO的收敛性优势，并在GSM8K和MATH数据集上实证验证训练相位。

Result: GRPO在标准差较高和特征正交时收敛更快，实证发现三个训练阶段：早期加速、过渡稳定、晚期正交性丧失限制增益。

Conclusion: 标准差归一化通过自适应梯度提升GRPO性能，为无批评者RL算法设计提供理论依据和实践指导。

Abstract: Reinforcement learning (RL) has become a key driver of language model reasoning. Among RL algorithms, Group Relative Policy Optimization (GRPO) is the de facto standard, avoiding the need for a critic by using per-prompt baselines and variance normalization. Yet why and when this normalization helps remains unclear. In this work, we provide an explanation through the lens of local curvature of the sequence-level policy gradient: standard deviation normalization implements an adaptive gradient. Theoretically, under mild conditions, GRPO enjoys a strictly improved convergence rate over unnormalized REINFORCE, with gains characterized by the average within-prompt reward standard deviation across prompts and iterations. Empirically, our analysis on GSM8K and MATH benchmarks reveals three distinct training phases governed by the interplay between feature orthogonality and reward variance: (I) an early acceleration phase where high variance and orthogonality favor adaptive scaling; (II) a relatively stable transition phase; and (III) a late-stage regime where the loss of orthogonality limits further gains. Together, these results provide a principled account of when std normalization helps in GRPO, and offer broader insights into the design of critic-free RL algorithms.

</details>


### [243] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: STGAT是一种新型时空图注意力网络，用于检测物联网设备中的时间异常，准确率达95.7%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的时间同步易受时钟漂移、同步操纵和时间戳不连续（如Y2K38）干扰，传统异常检测模型因依赖可靠时间戳而失效。

Method: STGAT结合漂移感知的时间嵌入、时间自注意力和图注意力机制，通过曲率正则化的潜在表示区分正常与异常时间演化。

Result: 在受控时间扰动的能源物联网数据上，STGAT达到95.7%准确率，检测延迟降低26%，显著优于循环、变换器和图基基线模型。

Conclusion: STGAT有效建模时空时间异常，提升能源物联网系统在复杂时间干扰下的可靠性与检测效率。

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [244] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种计算廉价且数学严谨的分布修正方法，解决等式约束生成模型中的采样问题


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理等式约束的科学领域数据时存在数学局限，难以正确建模约束流形上的分布

Method: 通过约束感知的方式扰动数据分布，使新分布支持空间与环境维数一致，同时保留潜在流形几何结构

Result: 理论分析与实验表明，该方法在扩散模型和归一化流中均能稳定采样并有效恢复数据分布

Conclusion: 该方法为等式约束场景下的生成建模提供了通用、高效且理论可靠的解决方案

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [245] [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)
*Eugenia Iofinova,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出Behemoth框架，通过合成数据研究大语言模型编辑的机制与效果。


<details>
  <summary>Details</summary>
Motivation: 现实模型编辑方法效果不稳定，且因真实数据复杂难以分析训练数据与模型存储机制的关系。

Method: 构建Behemoth合成数据生成框架，基于简单表格数据实验模型编辑行为。

Result: 发现某些情况下限制更新秩反而提升编辑效果，部分结论与现实现象一致。

Conclusion: 合成数据有助于揭示模型编辑的本质机制，为可靠编辑提供理论基础。

Abstract: As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.

</details>


### [246] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 使用深度强化学习根据部分可观测数据优化ICU镇痛用药策略，发现同时考虑疼痛缓解和死亡率的目标可显著降低死亡风险。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在ICU镇痛管理中忽视患者生存率，且不适用于不完全观测环境，可能导致治疗风险。

Method: 基于MIMIC-IV数据库中47,144例ICU住院数据，构建深度强化学习框架，分别以降低疼痛和联合降低疼痛与死亡率为目标训练用药策略。

Result: 仅优化疼痛缓解的策略与死亡率正相关，而联合优化死亡率的策略与死亡率负相关，表明长期目标对安全用药至关重要。

Conclusion: 在ICU镇痛管理中，即使以短期目标为主，也需纳入长期生存指标，以避免有害治疗决策。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [247] [SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training](https://arxiv.org/abs/2601.23155)
*Powei Chang,Jinpeng Zhang,Bowen Chen,Chenyu Wang,Chenlu Guo,Yixing Zhang,Yukang Gao,JianXiang Xiang,Yue Gao,Chaoqun Sun,Yiyi Chen,Dongying Kong*

Main category: cs.LG

TL;DR: SPICE通过减少梯度冲突，在仅用10%数据的情况下，显著提升指令调优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Fisher信息的选样方法因梯度冲突导致信息增益衰减缓慢，影响效率与效果。

Method: 提出SPICE，通过ε分解量化冲突，设计冲突感知的选样策略，结合早期停止与代理模型提升效率。

Result: 在8个基准上使用LLaMA2-7B和Qwen2-7B，仅用10%数据即超越全数据训练及其他6种方法。

Conclusion: 梯度冲突是信息选样的关键瓶颈，SPICE通过显式惩罚冲突实现高效高性能的指令调优。

Abstract: Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.

</details>


### [248] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种无需标签的语法方法，从无标注轨迹中自动分割技能并构建层次结构，提升强化学习中的技能复用与学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖动作标签、奖励或人工标注，限制了在无标注场景下的应用，亟需无需监督的技能分割与层次发现方法。

Method: 采用基于语法的方法，从无标签轨迹中自动分割技能并构建层次化结构，捕捉低层行为与高层技能的组合关系。

Result: 在高维像素环境（如Craftax和未修改的Minecraft）中，该方法在技能分割、复用和层次质量指标上优于基线，且能加速下游强化学习任务的学习。

Conclusion: 该语法方法可有效发现无监督技能层次结构，提升强化学习的可解释性与效率，具有实际应用潜力。

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [249] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 通过截断推理轨迹并重新输入模型，发现准确率和决策信心随推理token增加而提升，且由内容而非长度或风格驱动。


<details>
  <summary>Details</summary>
Motivation: 理解LLM推理过程中准确率与决策信心如何随推理轨迹演变，以及中间片段是否包含超越长度和风格的相关信息。

Method: 生成推理轨迹，按固定token百分比截断，并将部分轨迹注入模型，通过下一个token概率测量答案分布。

Result: 准确率和决策信心随推理token增加而提升；提升主要来自内容而非长度或风格；强模型能修正错误轨迹，但初始回答仍受弱模型影响。

Conclusion: 轨迹探测为高效安全部署推理模型提供诊断工具，可指导实际的轨迹处理与监控策略，无需假设中间token是忠实解释。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [250] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 研究带参数噪声的随机线性bandits，提出紧致的后悔上界与下界，并证明简单探索-利用算法可达最优 regret。


<details>
  <summary>Details</summary>
Motivation: 传统加性噪声模型下线性bandits的后悔界为$d\sqrt{T}$，但参数噪声模型下可能实现更优性能，亟需理论分析。

Method: 分析参数噪声模型下的奖励结构，推导一般动作集的后悔上界与下界，并针对$\ell_p$单位球形动作集给出精确的minimax regret表达式。

Result: 得到上界$\widetilde{O}(\sqrt{d T \log (K/δ) σ^2_{\max}})$，下界$\widetilde{Ω}(d \sqrt{T σ^2_{\max}})$，对$\ell_p$球形集得$\widetilde{Θ}(\sqrt{dT σ^2_q})$，且$σ^2_q \leq 4$。

Conclusion: 参数噪声模型显著优于经典加性噪声模型，且简单探索-利用算法即可达到理论最优，颠覆传统认知。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [251] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: 提出一种对可互换标记重命名不变的Transformer机制，提升模型在新符号上的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有神经架构无法有效处理语义等价但可区分的标记（如绑定变量），导致在未见过的符号上泛化能力差

Method: 采用并行嵌入流隔离每个可互换标记的贡献，结合聚合注意力机制实现跨流结构化信息共享

Result: 理论上有证明的不变性，实验表明在开放词汇任务中显著提升性能

Conclusion: 该方法有效解决可互换标记的泛化问题，为符号敏感任务提供新思路

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [252] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: DyCAST是一种动态字符对齐的语音分词器，通过可变帧率和持续时间建模减少令牌数量，同时保持高质量语音重建。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器采用固定帧率，导致令牌序列过长且效率低。

Method: 引入DyCAST，通过软字符级对齐和显式持续时间建模实现可变帧率分词，并结合检索增强解码提升低帧率下的重建质量。

Result: DyCAST在显著减少令牌数量的同时，保持了与固定帧率编解码器相当的语音重建质量和下游任务性能。

Conclusion: DyCAST为语音到LLM的输入提供了更高效、更灵活的编码方案，是未来对话语音技术的重要改进。

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [253] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MGN-T通过结合Transformer与MeshGraphNet，高效处理工业级高分辨率网格的长程物理交互，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准MeshGraphNet在高分辨率网格上因迭代消息传递导致长程信息传播效率低下，难以处理工业级复杂几何与边界条件。

Method: 引入物理注意力Transformer作为全局处理器，直接捕捉长程物理交互，同时保留节点与边属性，无需深层消息传递或层次化网格。

Result: MGN-T在冲击动力学等工业场景中成功建模自接触、塑性及多变量输出，精度超越SOTA，且参数量大幅减少。

Conclusion: MGN-T为高分辨率网格的物理仿真提供高效、准确且可扩展的解决方案，具备工业应用潜力。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [254] [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)
*Haoyun Jiang,Junqi He,Feng Hong,Xinlong Yang,Jianwei Zhang,Zheng Li,Zhengyang Zhuge,Zhiyong Chen,Bo Han,Junyang Lin,Jiangchao Yao*

Main category: cs.LG

TL;DR: 提出TriSpec，一种三元草稿验证框架，显著降低验证成本，实现最高35%的加速和50%的模型调用减少。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在草稿质量上已接近极限，但验证成本仍高，限制了推理效率的进一步提升。

Method: 引入轻量级代理模型，对易验证的草稿序列直接通过，仅在不确定时调用完整目标模型，构建三元验证机制。

Result: 在Qwen3和DeepSeek-R1-Distill-Qwen/LLaMA上，TriSpec相比标准推测解码实现最高35%加速，目标模型调用减少50%，精度相当。

Conclusion: 通过降低验证成本而非提升草稿质量，TriSpec为推测解码提供了新方向，可与现有方法兼容并进一步提升效率。

Abstract: Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\% speedup over standard SD, with up to 50\% fewer target model invocations while maintaining comparable accuracy.

</details>


### [255] [Ensuring Semantics in Weights of Implicit Neural Representations through the Implicit Function Theorem](https://arxiv.org/abs/2601.23181)
*Tianming Qiu,Christos Sonis,Hao Shen*

Main category: cs.LG

TL;DR: 利用隐函数定理建立数据空间与网络权重空间的映射，提出基于超网络的INR权重编码框架，在分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对数据语义如何编码到神经网络权重中的理论解释。

Method: 采用隐函数定理（IFT）构建数据空间与权重空间的精确映射，并通过共享超网络将实例嵌入映射到INR权重。

Result: 在2D和3D数据集的下游分类任务中，方法性能与现有基线相当。

Conclusion: 该工作为理解网络权重的语义编码提供了理论基础，推动未来对权重空间学习的研究。

Abstract: Weight Space Learning (WSL), which frames neural network weights as a data modality, is an emerging field with potential for tasks like meta-learning or transfer learning. Particularly, Implicit Neural Representations (INRs) provide a convenient testbed, where each set of weights determines the corresponding individual data sample as a mapping from coordinates to contextual values. So far, a precise theoretical explanation for the mechanism of encoding semantics of data into network weights is still missing. In this work, we deploy the Implicit Function Theorem (IFT) to establish a rigorous mapping between the data space and its latent weight representation space. We analyze a framework that maps instance-specific embeddings to INR weights via a shared hypernetwork, achieving performance competitive with existing baselines on downstream classification tasks across 2D and 3D datasets. These findings offer a theoretical lens for future investigations into network weights.

</details>


### [256] [Learning to Execute Graph Algorithms Exactly with Graph Neural Networks](https://arxiv.org/abs/2601.23207)
*Muhammad Fetrat Qharabagh,Artur Back de Luca,George Giapitzakis,Kimon Fountoulakis*

Main category: cs.LG

TL;DR: 通过训练MLP执行局部节点指令，证明图神经网络可在有限精度下精确学习图算法，如BFS、DFS和Bellman-Ford。


<details>
  <summary>Details</summary>
Motivation: 图神经网络学习图算法的能力缺乏理论保障，尤其在分布式计算模型中的可学习性尚不明确。

Method: 使用MLPensemble学习局部节点指令，结合NTK理论证明其可从少量样本中学习，并嵌入GNN中实现全图算法精确执行。

Result: 在有界度和有限精度条件下，成功证明了包括LOCAL模型、消息扩散、BFS、DFS和Bellman-Ford等算法的精确可学习性。

Conclusion: 图神经网络具备理论上的精确学习图算法的能力，为GNN的算法推理提供了坚实的理论基础。

Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.

</details>


### [257] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 利用交通流量数据构建高精度空气污染预测模型，通过环形交通表示法和偏最小二乘回归实现城市级动态预报


<details>
  <summary>Details</summary>
Motivation: 城市空气污染严重，现有监测数据时空分辨率低，而实时交通数据丰富且精细，亟需建立交通与污染的关联模型以提升预报精度

Method: 将彩色交通地图转化为同心环结构描述交通强度，采用偏最小二乘回归（PLSR）建模污染与交通关系，通过多组训练样本优化模型

Result: 模型显著提升污染预测的时空分辨率，揭示交通强度与污染物的非线性关系，方法具备跨城市迁移能力

Conclusion: 该方法简单高效，可推广至其他城市，为基于公开交通数据的高分辨率空气质量预警提供新范式

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [258] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 本文提出了一种在众包标注聚合中保障公平性的方法，理论分析了多数投票和最优贝叶斯聚合的公平性，并提出离散场景下的后处理算法以满足严格的人口统计平等。


<details>
  <summary>Details</summary>
Motivation: 众包标注易放大个体偏见，而现有方法缺乏对公平性的理论保障与收敛分析，尤其在人口统计平等方面研究不足。

Method: 在ε-公平性框架下分析 Majority Vote 和 Optimal Bayesian 聚合的公平性间隙，推导小规模群体下公平性上界，并证明其收敛性；同时将连续型公平后处理算法推广至离散设置。

Result: 理论证明聚合结果可指数收敛至真实标签的公平性，并在合成与真实数据上验证了所提后处理方法能有效满足严格人口统计平等。

Conclusion: 首次系统性建模众包聚合的公平性，提出可证明收敛的公平保障机制，为实际应用提供了理论支撑与实用算法。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [259] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: SPAN是一种基于样条的自适应网络，显著提升强化学习在资源受限环境中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统MLP在近似值函数时参数效率低，且模型压缩技术无法提升学习效率；现有样条网络如KAN计算开销大。

Method: SPAN结合可学习预处理层与可分离张量积B样条基，基于低秩KHRONOS框架构建。

Result: 在PPO、SAC和D4RL等任务中，SPAN比MLP样本效率提升30-50%，成功率提高1.3-9倍，且具备更强的 anytime 性能与超参数鲁棒性。

Conclusion: SPAN是资源受限环境中高效策略学习的高性能替代方案。

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


### [260] [Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph](https://arxiv.org/abs/2601.23233)
*Nguyen Minh Duc,Viet Cuong Ta*

Main category: cs.LG

TL;DR: SDG提出了一种序列级扩散框架，通过去噪生成统一动态图学习与生成建模，显著提升时序链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序图神经网络仅进行判别式学习，无法捕捉未来时序交互的不确定性与序列结构。

Method: SDG在历史交互序列上注入噪声，通过条件去噪过程联合重建所有交互嵌入，并采用交叉注意力解码器引导目标序列重建，端到端优化。

Result: 在多个时序图基准上，SDG consistently达到state-of-the-art性能。

Conclusion: SDG通过生成式去噪框架有效建模时序交互的分布特性，为时序链接预测提供了新范式。

Abstract: Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task.

</details>


### [261] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 将Transformer层解释为优化算法的迭代，提出Nesterov加速版本并超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构缺乏明确的优化理论基础，难以进行系统性改进。

Method: 将自注意力和MLP分别视为交互能和势能的梯度更新，基于Lie-Trotter分裂构建优化框架，并引入Nesterov加速。

Result: 加速版本在TinyStories和OpenWebText上一致优于nanoGPT基线。

Conclusion: 优化理论视角可为Transformer架构设计提供新思路，并带来实际性能提升。

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [262] [How well do generative models solve inverse problems? A benchmark study](https://arxiv.org/abs/2601.23238)
*Patrick Krüger,Patrick Materne,Werner Krebs,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 条件流匹配在燃气轮机燃烧室逆向设计中表现最优，超越了传统贝叶斯方法和其它生成模型。


<details>
  <summary>Details</summary>
Motivation: 生成学习可解决贝叶斯逆问题，但需系统比较不同生成模型与传统方法的性能。

Method: 对比传统贝叶斯马尔可夫链蒙特卡洛方法与三种生成模型（cGAN、INN、CFM），应用于6个设计参数到3个性能标签的燃气轮机燃烧室设计问题，并提出评估指标衡量精度与多样性。

Result: 条件流匹配（CFM）在所有评估指标和不同训练数据规模下均显著优于其他方法。

Conclusion: 条件流匹配是解决此类逆设计问题的最优生成学习方法。

Abstract: Generative learning generates high dimensional data based on low dimensional conditions, also called prompts. Therefore, generative learning algorithms are eligible for solving (Bayesian) inverse problems. In this article we compare a traditional Bayesian inverse approach based on a forward regression model and a prior sampled with the Markov Chain Monte Carlo method with three state of the art generative learning models, namely conditional Generative Adversarial Networks, Invertible Neural Networks and Conditional Flow Matching. We apply them to a problem of gas turbine combustor design where we map six independent design parameters to three performance labels. We propose several metrics for the evaluation of this inverse design approaches and measure the accuracy of the labels of the generated designs along with the diversity. We also study the performance as a function of the training dataset size. Our benchmark has a clear winner, as Conditional Flow Matching consistently outperforms all competing approaches.

</details>


### [263] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 在非实现实性假设下，提出语言识别与生成的全新目标，获得近似紧致的速率刻画。


<details>
  <summary>Details</summary>
Motivation: 现有工作依赖强实现实性假设，限制了实际应用，本研究旨在打破这一假设，探索更通用的agnostically设置。

Method: 提出适用于非实现实性环境的语言识别与生成目标函数，分析其统计速率。

Result: 在无分布限制条件下，得到了语言识别与生成的全新且近似紧致的统计速率。

Conclusion: 该工作拓展了语言任务的理论边界，为真实世界中不完美数据提供了更鲁棒的分析框架。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [264] [TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training](https://arxiv.org/abs/2601.23261)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Dongyang Li,Yupeng Su,Sijia Liu,Zheng Zhang*

Main category: cs.LG

TL;DR: TEON是Muon的推广，通过张量级梯度正交化提升大语言模型训练性能


<details>
  <summary>Details</summary>
Motivation: Muon仅在单层进行梯度正交化，缺乏全局优化视角，导致收敛性受限

Method: 将神经网络梯度建模为高阶张量，提出TEON方法并给出理论收敛保证，结合近似SVD实现高效计算

Result: 在GPT和LLaMA系列模型上，TEON在130M至1B参数范围内显著提升训练与验证困惑度，对SVD近似方案具有强鲁棒性

Conclusion: TEON通过全局张量正交化超越层间独立优化，为大模型预训练提供更高效稳定的优化框架

Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.

</details>


### [265] [Particle-Guided Diffusion Models for Partial Differential Equations](https://arxiv.org/abs/2601.23262)
*Andrew Millard,Fredrik Lindsten,Zheng Zhao*

Main category: cs.LG

TL;DR: 提出一种基于物理约束的引导随机采样方法，提升扩散模型生成解的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式方法在求解偏微分方程（PDE）时生成的解常违反物理规律，需引入物理约束以提升解的可接受性。

Method: 结合PDE残差与观测约束构建物理引导信号，并嵌入顺序蒙特卡洛（SMC）框架，实现可扩展的生成式PDE求解器。

Result: 在多个基准PDE及多物理场耦合系统中，该方法比现有生成方法具有更低的数值误差。

Conclusion: 该方法为生成式模型求解PDE提供了物理一致且高精度的新范式。

Abstract: We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) framework, yielding a scalable generative PDE solver. Across multiple benchmark PDE systems as well as multiphysics and interacting PDE systems, our method produces solution fields with lower numerical error than existing state-of-the-art generative methods.

</details>


### [266] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS通过动态聚焦可解码token，提升扩散大语言模型的推理效率，最高提升3.52倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）因解码过程中大量计算浪费在不可解码token上，导致推理成本过高。

Method: FOCUS利用注意力机制评估token重要性，动态聚焦可解码token并实时剔除不可解码token，提升有效批大小。

Result: 相比LMDeploy，FOCUS在保持或提升生成质量的前提下，吞吐量提升最高达3.52倍。

Conclusion: FOCUS有效缓解DLLMs的解码效率瓶颈，为大规模部署提供可行方案。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [267] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出一种解耦扩散逆求解器（DDIS），在函数空间中实现数据高效、物理感知的逆PDE求解，显著提升稀疏数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散后验采样方法依赖联合建模系数与解，需大量配对数据，数据效率低且在稀疏数据下易出现引导衰减问题。

Method: DDIS采用解耦设计：无条件扩散学习系数先验，神经算子显式建模前向PDE作为引导，并提出解耦退火后验采样（DAPS）避免过平滑。

Result: 在稀疏观测下，DDIS平均降低11% L2误差和54%频谱误差；数据仅1%时，L2误差仍比联合模型低40%。

Conclusion: DDIS通过解耦架构实现物理信息的有效注入，在数据稀缺条件下显著优于现有方法，理论与实验均验证其优越性。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [268] [Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey](https://arxiv.org/abs/2601.22198)
*Judith Vilella-Cantos,Mónica Ballesta,David Valiente,María Flores,Luis Payá*

Main category: cs.RO

TL;DR: 本文首次系统综述了激光雷达（LiDAR）在农业环境中的定位技术，分析了挑战、方法、数据集与未来方向。


<details>
  <summary>Details</summary>
Motivation: 农业环境结构复杂、特征稀疏，传统LiDAR定位方法效果不佳，亟需针对农业场景的深度学习定位方案。

Method: 对当前深度学习在农业LiDAR定位（LPR）中的应用进行系统性综述，分析现有方法、数据集与评估指标。

Result: 明确了农业环境下LiDAR定位的主要挑战与技术瓶颈，整理了现有研究进展。

Conclusion: 本综述是首个聚焦农业LiDAR定位的研究，旨在推动该领域未来研究发展。

Abstract: An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings. However, the lack of distinctive features and the unstructured nature of agricultural environments make place recognition challenging. This work presents a comprehensive review of state-of-the-art the latest deep learning applications for agricultural environments and LPR techniques. We focus on the challenges that arise in these environments. We analyze the existing approaches, datasets, and metrics used to evaluate LPR system performance and discuss the limitations and future directions of research in this eld. This is the rst survey that focuses on LiDAR based localization in agricultural settings, with the aim of providing a thorough understanding and fostering further research in this specialized domain.

</details>


### [269] [Game-Based and Gamified Robotics Education: A Comparative Systematic Review and Design Guidelines](https://arxiv.org/abs/2601.22199)
*Syed T. Mubarrat,Byung-Cheol Min,Tianyu Shao,E. Cho Smith,Bedrich Benes,Alejandra J. Magana,Christos Mousas,Dominic Kao*

Main category: cs.RO

TL;DR: 首次系统综述游戏化与游戏式学习在机器人教育中的比较影响，发现两者在场景与教学法上有显著差异，并指出技术应用与研究时长的局限性。


<details>
  <summary>Details</summary>
Motivation: 机器人教育虽能培养计算思维与解决问题能力，但因技术复杂而难以推广；游戏化与游戏式学习虽具吸引力，但其效果对比尚不清晰。

Method: 基于PRISMA标准的系统性综述，分析来自四大数据库的95项研究（2014-2025），编码其方法、情境、技能水平、教学法与成果，编码一致性k=0.918。

Result: 发现三类模式：（1）GBL多用于非正式场景，游戏化主导正式课堂并偏好项目式学习；（2）教学集中于入门编程与模块化套件，高级软硬件与沉浸技术采用率低；（3）研究周期短，依赖自我报告数据。

Conclusion: 提出八项研究方向与设计框架，为机器人教育提供实践指南，呼吁提升技术深度与研究严谨性。

Abstract: Robotics education fosters computational thinking, creativity, and problem-solving, but remains challenging due to technical complexity. Game-based learning (GBL) and gamification offer engagement benefits, yet their comparative impact remains unclear. We present the first PRISMA-aligned systematic review and comparative synthesis of GBL and gamification in robotics education, analyzing 95 studies from 12,485 records across four databases (2014-2025). We coded each study's approach, learning context, skill level, modality, pedagogy, and outcomes (k = .918). Three patterns emerged: (1) approach-context-pedagogy coupling (GBL more prevalent in informal settings, while gamification dominated formal classrooms [p < .001] and favored project-based learning [p = .009]); (2) emphasis on introductory programming and modular kits, with limited adoption of advanced software (~17%), advanced hardware (~5%), or immersive technologies (~22%); and (3) short study horizons, relying on self-report. We propose eight research directions and a design space outlining best practices and pitfalls, offering actionable guidance for robotics education.

</details>


### [270] [ReloPush-BOSS: Optimization-guided Nonmonotone Rearrangement Planning for a Car-like Robot Pusher](https://arxiv.org/abs/2601.22289)
*Jeeho Ahn,Christoforos Mavrogiannis*

Main category: cs.RO

TL;DR: 提出ReloPush-BOSS框架，利用Dubins路径分类优化预移动位置，高效解决密 clutter 环境下多物体重排问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预移动位置选择上易陷入局部最优，导致路径不可行或成本过高。

Method: 通过Dubins路径分类指导预移动优化，构建融合运动学、几何与推动约束的物体可通行图，并采用深度优先搜索生成重排序列。

Result: 在最多13个物体的密集场景中，ReloPush-BOSS成功率和推送路径长度均优于现有方法，硬件实验验证了鲁棒性。

Conclusion: 该方法有效避免局部最优，显著提升复杂环境中多物体重排的效率与可行性。

Abstract: We focus on multi-object rearrangement planning in densely cluttered environments using a car-like robot pusher. The combination of kinematic, geometric and physics constraints underlying this domain results in challenging nonmonotone problem instances which demand breaking each manipulation action into multiple parts to achieve a desired object rearrangement. Prior work tackles such instances by planning prerelocations, temporary object displacements that enable constraint satisfaction, but deciding where to prerelocate remains difficult due to local minima leading to infeasible or high-cost paths. Our key insight is that these minima can be avoided by steering a prerelocation optimization toward low-cost regions informed by Dubins path classification. These optimized prerelocations are integrated into an object traversability graph that encodes kinematic, geometric, and pushing constraints. Searching this graph in a depth-first fashion results in efficient, feasible rearrangement sequences. Across a series of densely cluttered scenarios with up to 13 objects, our framework, ReloPush-BOSS, exhibits consistently highest success rates and shortest pushing paths compared to state-of-the-art baselines. Hardware experiments on a 1/10 car-like pusher demonstrate the robustness of our approach. Code and footage from our experiments can be found at: https://fluentrobotics.com/relopushboss.

</details>


### [271] [Lantern: A Minimalist Robotic Object Platform](https://arxiv.org/abs/2601.22381)
*Victor Nikhil Antony,Zhili Gong,Guanchen Li,Clara Jeon,Chien-Ming Huang*

Main category: cs.RO

TL;DR: Lantern是一个低成本、开源的简约机器人平台，旨在降低人机交互研究的门槛并激发社会性互动。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统复杂昂贵，难以普及，亟需一个低成本、易用的平台来促进人机交互研究的多样化探索。

Method: 设计并迭代开发了Lantern机器人平台（成本约40美元），并通过五种场景评估其潜力：联合设计工作坊、感官房间案例、外部实验室分发、研究生课程整合和公众展览。

Result: Lantern能有效激发用户参与，支持情绪调节、专注工作等多种应用，并被广泛接受为可行的HRI研究平台。

Conclusion: Lantern作为简约机器人平台，成功降低了HRI研究的准入门槛，验证了简单形态在激发社会性互动中的潜力。

Abstract: Robotic objects are simple actuated systems that subtly blend into human environments. We design and introduce Lantern, a minimalist robotic object platform to enable building simple robotic artifacts. We conducted in-depth design and engineering iterations of Lantern's mechatronic architecture to meet specific design goals while maintaining a low build cost (~40 USD). As an extendable, open-source platform, Lantern aims to enable exploration of a range of HRI scenarios by leveraging human tendency to assign social meaning to simple forms. To evaluate Lantern's potential for HRI, we conducted a series of explorations: 1) a co-design workshop, 2) a sensory room case study, 3) distribution to external HRI labs, 4) integration into a graduate-level HRI course, and 5) public exhibitions with older adults and children. Our findings show that Lantern effectively evokes engagement, can support versatile applications ranging from emotion regulation to focused work, and serves as a viable platform for lowering barriers to HRI as a field.

</details>


### [272] [Plant-Inspired Robot Design Metaphors for Ambient HRI](https://arxiv.org/abs/2601.22387)
*Victor Nikhil Antony,Adithya R N,Sarah Derrick,Zhili Gong,Peter M. Donley,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 通过植物隐喻重新思考人机交互，设计并测试了一系列开源植物启发型机器人原型，探索其在存在性、时间性与姿态表达上的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统人机交互依赖拟人/拟物范式，交互需求高；植物作为低需求但影响氛围的隐喻，提供全新交互设计思路。

Method: 采用研究通过设计（RtD）方法，通过迭代设计、原型构建与原型中心工作坊，收集用户对植物机器人感知与想象的反馈。

Result: 提出了一组植物启发的机器人原型，提炼出关于感知、设计原则的关键洞见，并形成指导植物隐喻应用于HRI的设计考量。

Conclusion: 植物作为交互隐喻可推动人机交互向更柔和、持久、环境融合的方向转变，具有广阔设计潜力。

Abstract: Plants offer a paradoxical model for interaction: they are ambient, low-demand presences that nonetheless shape atmosphere, routines, and relationships through temporal rhythms and subtle expressions. In contrast, most human-robot interaction (HRI) has been grounded in anthropomorphic and zoomorphic paradigms, producing overt, high-demand forms of engagement. Using a Research through Design (RtD) methodology, we explore plants as metaphoric inspiration for HRI; we conducted iterative cycles of ideation, prototyping, and reflection to investigate what design primitives emerge from plant metaphors and morphologies, and how these primitives can be combined into expressive robotic forms. We present a suite of speculative, open-source prototypes that help probe plant-inspired presence, temporality, form, and gestures. We deepened our learnings from design and prototyping through prototype-centered workshops that explored people's perceptions and imaginaries of plant-inspired robots. This work contributes: (1) Set of plant-inspired robotic artifacts; (2) Designerly insights on how people perceive plant-inspired robots; and (3) Design consideration to inform how to use plant metaphors to reshape HRI.

</details>


### [273] [Accurate Pedestrian Tracking in Urban Canyons: A Multi-Modal Fusion Approach](https://arxiv.org/abs/2601.22406)
*Shahar Dubiner,Peng Ren,Roberto Manduchi*

Main category: cs.RO

TL;DR: 提出一种融合GNSS、惯性数据和地图先验的粒子滤波方法，提升城市环境中盲人导航的定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS在城市环境中性能下降，对依赖精准导航的视障用户构成挑战，而相机定位不实用。

Method: 采用RoNIN进行惯性定位，通过粒子滤波融合GNSS与惯性数据，并引入地图先验（如建筑、不可行走区域）作为概率性地图匹配。

Result: 在旧金山六条复杂路线上的实验表明，GNSS+RoNIN+PF融合方法在多数指标上显著优于纯GNSS，惯性定位+粒子滤波亦优于GNSS。

Conclusion: 融合惯性数据与地图先验的粒子滤波方法能有效提升城市环境下的定位精度，尤其对视障用户的关键导航任务（如 sidewalk 正确性）有显著改善。

Abstract: The contribution describes a pedestrian navigation approach designed to improve localization accuracy in urban environments where GNSS performance is degraded, a problem that is especially critical for blind or low-vision users who depend on precise guidance such as identifying the correct side of a street. To address GNSS limitations and the impracticality of camera-based visual positioning, the work proposes a particle filter based fusion of GNSS and inertial data that incorporates spatial priors from maps, such as impassable buildings and unlikely walking areas, functioning as a probabilistic form of map matching. Inertial localization is provided by the RoNIN machine learning method, and fusion with GNSS is achieved by weighting particles based on their consistency with GNSS estimates and uncertainty. The system was evaluated on six challenging walking routes in downtown San Francisco using three metrics related to sidewalk correctness and localization error. Results show that the fused approach (GNSS+RoNIN+PF) significantly outperforms GNSS only localization on most metrics, while inertial-only localization with particle filtering also surpasses GNSS alone for critical measures such as sidewalk assignment and across street error.

</details>


### [274] [High-Definition 5MP Stereo Vision Sensing for Robotics](https://arxiv.org/abs/2601.22445)
*Leaf Jiang,Matthew Holzel,Bernhard Kaplan,Hsiou-Yuan Liu,Sabyasachi Paul,Karen Rankin,Piotr Swierczynski*

Main category: cs.RO

TL;DR: 高分辨率立体视觉系统需高精度校准与快速处理才能发挥潜力，本文提出新方法实现高精度与高速度的实时立体匹配。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法满足高分辨率传感器对校准精度和处理速度的要求，限制了高像素摄像头在机器人中的应用。

Method: 提出一种新的帧间校准与立体匹配方法，结合实时视差图与高精度地面真值视差图的对比评估实时性能。

Result: 实验表明，仅当采用高精度校准时，高像素摄像头才能生成高质量的点云。

Conclusion: 高分辨率立体视觉系统的性能高度依赖于校准精度，本文方法有效实现了精度与速度的平衡。

Abstract: High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.

</details>


### [275] [CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control](https://arxiv.org/abs/2601.22467)
*Jiaqi Shi,Xulong Zhang,Xiaoyang Qu,Jianzong Wang*

Main category: cs.RO

TL;DR: CARE是一种无需动作标注、仅用视频-文本对训练视觉-语言-动作模型的弱监督框架，显著提升机器人控制的可扩展性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖动作标注，限制了其扩展性和泛化能力。

Method: CARE通过多任务预训练目标，仅使用视频-文本对学习连续潜在动作表示，微调时仅需少量带标签数据训练动作头。

Result: 在多个仿真任务中，CARE在成功率、语义可解释性和避免捷径学习方面优于现有方法。

Conclusion: CARE实现了无需显式动作标注的鲁棒机器人控制，具有高可扩展性、可解释性和有效性。

Abstract: Recent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE's superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE's scalability, interpretability, and effectiveness in robotic control with weak supervision.

</details>


### [276] [RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing](https://arxiv.org/abs/2601.22517)
*Kangning Yin,Zhe Cao,Wentao Dong,Weishuai Zeng,Tianyi Zhang,Qiang Zhang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Weinan Zhang*

Main category: cs.RO

TL;DR: 提出RoboStriker框架，通过分层设计实现人形机器人自主拳击，结合动作先验与隐空间博弈训练，显著提升动态任务性能与迁移能力。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在接触密集、高动态任务（如拳击）中难以实现人类水平的智能与敏捷性，传统MARL因高维接触动力学和缺乏物理先验而受限。

Method: 采用三级分层框架：1）基于人类动作捕捉数据训练单代理运动跟踪器；2）将技能蒸馏至正则化隐空间（单位超球面约束）；3）提出LS-NFSP，在隐空间中进行多代理博弈训练。

Result: 在仿真中实现优越的拳击竞争力，并成功实现仿真到现实的迁移。

Conclusion: RoboStriker通过解耦策略与执行、约束物理可行动作空间，有效解决了人形机器人高动态任务中的MARL训练难题。

Abstract: Achieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at RoboStriker.

</details>


### [277] [Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios](https://arxiv.org/abs/2601.22545)
*Feng Tao,Luca Paparusso,Chenyi Gu,Robin Koehler,Chenxu Wu,Xinyu Huang,Christian Juette,David Paz,Ren Liu*

Main category: cs.RO

TL;DR: 提出一种基于深度强化学习的实时路径规划框架，在停车场景中显著超越传统规划器的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 传统规划器依赖理想感知和高计算成本的在线搜索，难以在复杂约束环境中实现实时部署。

Method: 采用深度强化学习框架，基于自行车模型动力学直接学习导航策略，无需完美感知或额外模块，测试时仅需单次前向传播。

Result: 在新构建的挑战性停车基准上，成功率提升96%，效率提升52%，并开源基准与工具。

Conclusion: 该方法实现了更简单、高效、实用的实时路径规划，为自主系统研究提供了新范式和公共资源。

Abstract: Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.

</details>


### [278] [Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation](https://arxiv.org/abs/2601.22550)
*Geonho Leem,Jaedong Lee,Jehee Lee,Seungmoon Song,Jungdam Won*

Main category: cs.RO

TL;DR: Exo-plore是一种无需真人实验的仿真框架，用于优化外骨骼辅助策略。


<details>
  <summary>Details</summary>
Motivation: 现有外骨骼控制器优化依赖长时间真人实验，而行动障碍者难以参与，形成应用悖论。

Method: 结合神经力学仿真与深度强化学习，构建Exo-plore仿真框架，模拟人体对辅助力的适应。

Result: Exo-plore能生成真实步态数据、稳定优化结果，并可泛化至病理步态，病理严重度与最优辅助呈强线性关系。

Conclusion: Exo-plore突破了真人实验限制，为外骨骼个性化辅助提供了高效、可推广的优化方法。

Abstract: Exoskeletons show great promise for enhancing mobility, but providing appropriate assistance remains challenging due to the complexity of human adaptation to external forces. Current state-of-the-art approaches for optimizing exoskeleton controllers require extensive human experiments in which participants must walk for hours, creating a paradox: those who could benefit most from exoskeleton assistance, such as individuals with mobility impairments, are rarely able to participate in such demanding procedures. We present Exo-plore, a simulation framework that combines neuromechanical simulation with deep reinforcement learning to optimize hip exoskeleton assistance without requiring real human experiments. Exo-plore can (1) generate realistic gait data that captures human adaptation to assistive forces, (2) produce reliable optimization results despite the stochastic nature of human gait, and (3) generalize to pathological gaits, showing strong linear relationships between pathology severity and optimal assistance.

</details>


### [279] [Postural Virtual Fixtures for Ergonomic Physical Interactions with Supernumerary Robotic Bodies](https://arxiv.org/abs/2601.22672)
*Theodora Kastritsi,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 提出一种基于动能反馈的SRB控制框架，通过检测非人体工学姿态并提供阻力，促使用户养成正确姿势。


<details>
  <summary>Details</summary>
Motivation: 现有SRB在物理交互中用户仍可能采取不舒适姿势，长期使用易导致损伤，需引导 ergonomic 行为。

Method: 结合虚拟固定装置与在线人体工学评估，动态调整浮动基座位置，提供实时阻力反馈。

Result: 通过14名被试的实验验证，相比基线方法，所提框架显著提升姿势正确性与协调性。

Conclusion: 该框架能有效促进用户长期形成人体工学习惯，提升人机协作安全性与效率。

Abstract: Conjoined collaborative robots, functioning as supernumerary robotic bodies (SRBs), can enhance human load tolerance abilities. However, in tasks involving physical interaction with humans, users may still adopt awkward, non-ergonomic postures, which can lead to discomfort or injury over time. In this paper, we propose a novel control framework that provides kinesthetic feedback to SRB users when a non-ergonomic posture is detected, offering resistance to discourage such behaviors. This approach aims to foster long-term learning of ergonomic habits and promote proper posture during physical interactions. To achieve this, a virtual fixture method is developed, integrated with a continuous, online ergonomic posture assessment framework. Additionally, to improve coordination between the operator and the SRB, which consists of a robotic arm mounted on a floating base, the position of the floating base is adjusted as needed. Experimental results demonstrate the functionality and efficacy of the ergonomics-driven control framework, including two user studies involving practical loco-manipulation tasks with 14 subjects, comparing the proposed framework with a baseline control framework that does not account for human ergonomics.

</details>


### [280] [FlyAware: Inertia-Aware Aerial Manipulation via Vision-Based Estimation and Post-Grasp Adaptation](https://arxiv.org/abs/2601.22686)
*Biyu Ye,Na Fan,Zhengping Fan,Weiliang Deng,Hongming Chen,Qifeng Chen,Ximin Lyu*

Main category: cs.RO

TL;DR: 提出一种基于视觉的在线惯性估计与自适应控制框架，提升空中机械臂在负载变化下的鲁棒操作能力。


<details>
  <summary>Details</summary>
Motivation: 空中机械臂因负载和构型变化导致惯性参数时变，传统方法难以应对，亟需更鲁棒的控制策略。

Method: 融合视觉预抓取惯性估计与抓取后自适应机制，采用基于增益调度的惯性感知自适应控制，并通过频域系统辨识验证鲁棒性。

Result: 实验证明该框架能实时准确估计惯性参数并实现稳定控制，显著提升操作鲁棒性与可行性。

Conclusion: 该框架为空中机械臂的抓取后控制提供了新思路，具备实际部署潜力。

Abstract: Aerial manipulators (AMs) are gaining increasing attention in automated transportation and emergency services due to their superior dexterity compared to conventional multirotor drones. However, their practical deployment is challenged by the complexity of time-varying inertial parameters, which are highly sensitive to payload variations and manipulator configurations. Inspired by human strategies for interacting with unknown objects, this letter presents a novel onboard framework for robust aerial manipulation. The proposed system integrates a vision-based pre-grasp inertia estimation module with a post-grasp adaptation mechanism, enabling real-time estimation and adaptation of inertial dynamics. For control, we develop an inertia-aware adaptive control strategy based on gain scheduling, and assess its robustness via frequency-domain system identification. Our study provides new insights into post-grasp control for AMs, and real-world experiments validate the effectiveness and feasibility of the proposed framework.

</details>


### [281] [Robust Rigid Body Assembly via Contact-Implicit Optimal Control with Exact Second-Order Derivatives](https://arxiv.org/abs/2601.22849)
*Christian Dietz,Sebastian Albrecht,Gianluca Frison,Moritz Diehl,Armin Nurkanović*

Main category: cs.RO

TL;DR: 提出一种基于可微物理仿真和二阶导数的高效装配运动规划方法，显著减少仿真次数并实现超过99%的真实成功率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习和采样方法依赖大量物理仿真，效率低且难以泛化到真实场景。

Method: 构建可微物理仿真系统，结合内点法平滑接触检测与解析，改进基于线性规划的碰撞检测模型，并推导多场景鲁棒轨迹优化框架。

Result: 真实实验成功率达99%以上，仿真中证明精确Hessian优于常用近似，平滑接触建模与鲁棒性设计显著提升性能。

Conclusion: 该方法通过可微分与二阶优化实现样本高效、鲁棒的装配规划，为仿真到现实的迁移提供新范式。

Abstract: Efficient planning of assembly motions is a long standing challenge in the field of robotics that has been primarily tackled with reinforcement learning and sampling-based methods by using extensive physics simulations. This paper proposes a sample-efficient robust optimal control approach for the determination of assembly motions, which requires significantly less physics simulation steps during planning through the efficient use of derivative information. To this end, a differentiable physics simulation is constructed that provides second-order analytic derivatives to the numerical solver and allows one to traverse seamlessly from informative derivatives to accurate contact simulation. The solution of the physics simulation problem is made differentiable by using smoothing inspired by interior-point methods applied to both the collision detection as well as the contact resolution problem. We propose a modified variant of an optimization-based formulation of collision detection formulated as a linear program and present an efficient implementation for the nominal evaluation and corresponding first- and second-order derivatives. Moreover, a multi-scenario-based trajectory optimization problem that ensures robustness with respect to sim-to-real mismatches is derived. The capability of the considered formulation is illustrated by results where over 99\% successful executions are achieved in real-world experiments. Thereby, we carefully investigate the effect of smooth approximations of the contact dynamics and robust modeling on the success rates. Furthermore, the method's capability is tested on different peg-in-hole problems in simulation to show the benefit of using exact Hessians over commonly used Hessian approximations.

</details>


### [282] [Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs](https://arxiv.org/abs/2601.22927)
*Lars Ullrich,Michael Buchholz,Klaus Dietmayer,Knut Graichen*

Main category: cs.RO

TL;DR: 本文重新审视人工智能在完全自动驾驶中的作用，分析其挑战与机遇，并提出未来研究需求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临开放世界中的复杂性与安全问题，AI虽表现优异但其可转移性与安全性存疑，亟需系统性评估。

Method: 分析当前自动驾驶技术现状，梳理AI带来的局限性与潜在技术可能性，结合前瞻性发展探讨相关挑战。

Result: 识别出AI在自动驾驶功能实现中的关键挑战与机遇，明确未来研究需关注的安全、泛化与技术落地需求。

Conclusion: AI推动自动驾驶迈向更高 autonomy，但必须解决其安全性和可转移性问题，需持续开展针对性研究。

Abstract: Automated driving (AD) is promising, but the transition to fully autonomous driving is, among other things, subject to the real, ever-changing open world and the resulting challenges. However, research in the field of AD demonstrates the ability of artificial intelligence (AI) to outperform classical approaches, handle higher complexities, and reach a new level of autonomy. At the same time, the use of AI raises further questions of safety and transferability. To identify the challenges and opportunities arising from AI concerning autonomous driving functionalities, we have analyzed the current state of AD, outlined limitations, and identified foreseeable technological possibilities. Thereby, various further challenges are examined in the context of prospective developments. In this way, this article reconsiders fully autonomous driving with respect to advancements in the field of AI and carves out the respective needs and resulting research questions.

</details>


### [283] [MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2601.22930)
*Xidong Li,Mingyu Guo,Chenchao Xu,Bailin Li,Wenjing Zhu,Yangang Zou,Rui Chen,Zehuan Wang*

Main category: cs.RO

TL;DR: 提出MTDrive框架，通过多轮推理和mtGRPO算法提升自动驾驶轨迹规划在复杂场景中的性能，并实现2.5倍训练吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于单轮推理，难以处理需迭代优化的复杂驾驶场景。

Method: 引入MTDrive多轮框架，结合mtGRPO相对策略优化算法，并构建基于闭环仿真的人机交互轨迹数据集。

Result: 在NAVSIM基准上表现优于现有方法，训练吞吐量提升2.5倍。

Conclusion: 多轮推理范式显著提升轨迹规划能力，开源数据与模型将推动该领域发展。

Abstract: Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing "long-tail" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.

</details>


### [284] [Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation](https://arxiv.org/abs/2601.22965)
*Runhua Zhang,Junyi Hou,Changxu Cheng,Qiyi Chen,Tao Wang,Wuyue Zhao*

Main category: cs.RO

TL;DR: 提出自模仿扩散策略SIDP，通过奖励引导的自模仿和课程学习提升视觉导航效率，推理速度比基线快2.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习导致扩散策略依赖低效的生成-过滤流程，存在次优性和冗余问题。

Method: SIDP引入奖励引导的自模仿机制，结合课程学习与目标无关探索，直接学习高质量轨迹，减少采样与后过滤。

Result: 在仿真和真实机器人上显著优于现有方法，Jetson Orin Nano上推理速度达110ms，比NavDP快2.5倍。

Conclusion: SIDP通过自模仿有效提升规划效率与鲁棒性，实现高效实时部署。

Abstract: Diffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions. However, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive "generate-then-filter" pipeline that relies on auxiliary selectors during inference. To address these challenges, we propose Self-Imitated Diffusion Policy (SIDP), a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself. Specifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering. During training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility, and goal-agnostic exploration for trajectory augmentation to improve planning robustness. Extensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms. On Jetson Orin Nano, SIDP delivers a 2.5$\times$ faster inference than the baseline NavDP, i.e., 110ms VS 273ms, enabling efficient real-time deployment.

</details>


### [285] [Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.22988)
*Di Zhang,Weicheng Duan,Dasen Gu,Hongye Lu,Hai Zhang,Hang Yu,Junqiao Zhao,Guang Chen*

Main category: cs.RO

TL;DR: 提出一种单视角3D预训练与策略蒸馏框架，显著提升机器人操作在多视角变化下的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉表征依赖多视角输入、几何建模不完整、策略训练无法有效利用3D知识，限制了真实场景中的泛化性能

Method: 采用单视角3D预训练（点云重建+前馈高斯溅射）学习完整几何表征，并通过多步知识蒸馏将3D理解迁移至操控策略

Result: 在12个RLBench任务上平均成功率超越SOTA 12.7%，在中等和大视角偏移下成功率下降仅22.0%和29.7%，远优于SOTA的41.6%和51.5%

Conclusion: MethodName实现了单视角下的高效3D表征学习与策略迁移，为真实场景中视点不变的机器人操作提供了新范式

Abstract: Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge. To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation. MethodName introduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations. During policy learning, MethodName performs multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills. We conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art method by 12.7% in average success rate. Further evaluation on six representative tasks demonstrates strong zero-shot view generalization, with success rate drops of only 22.0% and 29.7% under moderate and large viewpoint shifts respectively, whereas the state-of-the-art method suffers larger decreases of 41.6% and 51.5%.

</details>


### [286] [MOSAIC: Modular Scalable Autonomy for Intelligent Coordination of Heterogeneous Robotic Teams](https://arxiv.org/abs/2601.23038)
*David Oberacker,Julia Richer,Philip Arm,Marvin Grosse Besselmann,Lennart Puck,William Talbot,Maximilian Schik,Sabine Bellmann,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Marco Hutter,Arne Roennau*

Main category: cs.RO

TL;DR: MOSAIC框架通过点兴趣（POI）统一任务抽象和多层次自主性，实现单操作员监督下的多机器人科学探索，显著降低人工干预需求。


<details>
  <summary>Details</summary>
Motivation: 传统移动机器人依赖人工遥操作，限制了部署规模并要求持续低延迟通信，亟需更高自主性的解决方案。

Method: 提出MOSAIC框架，基于点兴趣（POI）统一任务抽象，结合多层自主性与机器人能力动态分配任务，利用团队冗余与专业化实现连续作业。

Result: 在模拟月球探测实验中，5台异构机器人在1台操作员监督下完成82.3%任务，自主比达86%，操作员负载仅78.2%，即使1台机器人失效仍能完成任务。

Conclusion: MOSAIC框架证明了高鲁棒性、可扩展的多机器人探索可行，为未来任务提供了机器人互操作性、网络架构、团队构成与操作员负载管理的实用经验。

Abstract: Mobile robots have become indispensable for exploring hostile environments, such as in space or disaster relief scenarios, but often remain limited to teleoperation by a human operator. This restricts the deployment scale and requires near-continuous low-latency communication between the operator and the robot. We present MOSAIC: a scalable autonomy framework for multi-robot scientific exploration using a unified mission abstraction based on Points of Interest (POIs) and multiple layers of autonomy, enabling supervision by a single operator. The framework dynamically allocates exploration and measurement tasks based on each robot's capabilities, leveraging team-level redundancy and specialization to enable continuous operation. We validated the framework in a space-analog field experiment emulating a lunar prospecting scenario, involving a heterogeneous team of five robots and a single operator. Despite the complete failure of one robot during the mission, the team completed 82.3% of assigned tasks at an Autonomy Ratio of 86%, while the operator workload remained at only 78.2%. These results demonstrate that the proposed framework enables robust, scalable multi-robot scientific exploration with limited operator intervention. We further derive practical lessons learned in robot interoperability, networking architecture, team composition, and operator workload management to inform future multi-robot exploration missions.

</details>


### [287] [Robust and Generalized Humanoid Motion Tracking](https://arxiv.org/abs/2601.23080)
*Yubiao Ma,Han Yu,Jiayin Xie,Changtai Lv,Qiang Luo,Chi Zhang,Yunpeng Yin,Boyang Xing,Xuemei Ren,Dongdong Zheng*

Main category: cs.RO

TL;DR: 提出一种动力学条件命令聚合框架，仅用3.5小时运动数据实现端到端训练，显著提升人形机器人在复杂动态行为中的鲁棒性和零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 实际参考动作在转移到机器人域时存在噪声和不一致，闭环执行会放大局部缺陷，导致动态和接触丰富行为中的漂移或失败。

Method: 采用因果时间编码器汇总近期本体感觉，结合多头交叉注意力命令编码器根据当前动力学选择性聚合上下文窗口，并引入跌倒恢复课程、随机不稳定初始化和渐减向上辅助力提升鲁棒性。

Result: 仅需3.5小时数据即可单阶段端到端训练，无需蒸馏，在多种参考输入和挑战性动作中表现优异，实现零样本迁移和真实机器人上的鲁棒仿真到现实转移。

Conclusion: 该方法有效解决了人形机器人控制中的噪声敏感与泛化难题，为高效、鲁棒的全身体控制提供了新范式。

Abstract: Learning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies after being transferred to the robot domain, and local defects may be amplified by closed-loop execution, causing drift or failure in highly dynamic and contact-rich behaviors. We propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception and a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics. We further integrate a fall recovery curriculum with random unstable initialization and an annealed upward assistance force to improve robustness and disturbance rejection. The resulting policy requires only about 3.5 hours of motion data and supports single-stage end-to-end training without distillation. The proposed method is evaluated under diverse reference inputs and challenging motion regimes, demonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.

</details>


### [288] [Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation](https://arxiv.org/abs/2601.23087)
*Wu Songwei,Jiang Zhiduo,Xie Guanghu,Liu Yang,Liu Hong*

Main category: cs.RO

TL;DR: LG-Flow Policy通过在潜在动作空间中进行流匹配，实现高效、稳定的长时程机器人操作，优于扩散和原始动作空间流模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成策略在长时程机器人操作中难以兼顾表达能力、实时推理与执行稳定性：扩散模型推理慢，流匹配在原始动作空间易不稳定。

Method: 提出LG-Flow Policy，在时间正则化的连续潜在动作空间中进行流匹配，结合几何感知点云条件与多模态调制，解耦全局运动结构与低层噪声。

Result: 在仿真与实物机器人上实现近单步推理，显著提升轨迹平滑性与任务成功率，效率远超扩散模型。

Conclusion: 潜在空间流匹配是实现高效、稳定长时程机器人操作的有效途径，优于原始空间流与扩散方法。

Abstract: Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference latency, while flow matching enables fast one-step generation yet often leads to unstable execution when applied directly in the raw action space.
  We propose LG-Flow Policy, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space. By encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow, the proposed approach decouples global motion structure from low-level control noise, resulting in smooth and reliable long-horizon execution.
  LG-Flow Policy further incorporates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues evaluated as a representative modality in real-world settings. Experimental results in simulation and on physical robot platforms demonstrate that LG-Flow Policy achieves near single-step inference, substantially improves trajectory smoothness and task success over flow-based baselines operating in the raw action space, and remains significantly more efficient than diffusion-based policies.

</details>


### [289] [IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models](https://arxiv.org/abs/2601.23266)
*Seyed Ahmad Hosseini Miangoleh,Amin Jalal Aghdasian,Farzaneh Abdollahi*

Main category: cs.RO

TL;DR: 提出一种基于扩散的自适应前瞻规划器的逆强化学习框架（IRL-DAL），在Webots仿真中实现96%成功率和极低碰撞率，显著提升自动驾驶安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶方法在复杂环境中安全性和专家级决策能力不足，需结合模仿学习与强化学习以提升策略鲁棒性。

Method: 采用FSM模仿初始化，结合扩散模型作为安全监督者规划路径，引入可学习自适应掩码（LAM）优化感知，使用混合奖励（环境+IRL）与PPO进行微调，训练采用两阶段课程学习。

Result: 在Webots仿真中达到96%成功率，碰撞率低至0.05/1k步，超越现有基准，实现安全、平稳、专家级驾驶。

Conclusion: IRL-DAL框架有效融合模仿学习、扩散规划与自适应感知，显著提升自动驾驶系统在复杂场景中的安全性与泛化能力，代码已开源。

Abstract: This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.

</details>


### [290] [End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms](https://arxiv.org/abs/2601.23285)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Andrew Fisher,Reza Abiri*

Main category: cs.RO

TL;DR: BRACE框架通过端到端联合优化意图推断与辅助决策，显著提升共享自主系统在复杂目标模糊场景下的成功率与路径效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖静态混合比例或分离意图推断与辅助决策，难以适应非结构化环境，导致性能次优。

Method: 提出BRACE框架，结合贝叶斯意图推断与上下文自适应辅助，通过端到端梯度流整合信念信息到策略学习中。

Result: 在三项评估中，BRACE相较SOTA方法（IDA、DQN）提升6.3%成功率与41%路径效率，相较无辅助控制提升36.3%成功率与87%路径效率。

Conclusion: 集成优化在复杂目标模糊场景中优势显著，且具有跨机器人领域的泛化能力，推动自适应共享自主的SOTA发展。

Abstract: Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.

</details>
